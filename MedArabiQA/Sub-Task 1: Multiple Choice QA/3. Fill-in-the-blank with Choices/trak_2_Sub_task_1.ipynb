{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install groq nltk rouge-score bert-score transformers sentencepiece -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrRYh8HGWXFg",
        "outputId": "283e9263-a478-4a1a-ca36-d9b4911572b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Track 2: MedArabiQ 2025 (General Health)\n",
        "\n",
        "\n",
        "# Sub-Task 1: Fill-in-the-blank with Choices\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# llama-3.3-70b-versatile -> 69%"
      ],
      "metadata": {
        "id": "F4o554nXZhaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import groq\n",
        "import time\n",
        "from getpass import getpass\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- Groq API Configuration ---\n",
        "# Your Groq API key will be accessed securely from Colab's secrets\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "except (ImportError, KeyError):\n",
        "    print(\"Secret 'GROQ_API_KEY' not found. Please add it to the Colab secrets manager.\")\n",
        "    GROQ_API_KEY = getpass('Please enter your Groq API key: ')\n",
        "\n",
        "# Initialize the Groq client\n",
        "client = groq.Client(api_key=GROQ_API_KEY)\n",
        "\n",
        "\n",
        "# --- File paths and column names for the MCQ task ---\n",
        "# NOTE: The input CSV name suggests a \"fill-in-the-blank\" task, but the code logic\n",
        "# is designed for Multiple Choice. Ensure this is the correct file.\n",
        "INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/fill-in-the-blank-choices.csv'\n",
        "OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices.csv'\n",
        "\n",
        "# --- UPDATED: Correct column names as per your specification ---\n",
        "QUESTION_COLUMN = 'Question - Arabic'\n",
        "ANSWER_COLUMN = 'Answer - Arabic'\n",
        "\n",
        "\n",
        "# --- Function to Generate Answers for MCQ Task ---\n",
        "def generate_answer(question):\n",
        "    \"\"\"\n",
        "    Sends an MCQ question to the Groq API, prompting the model\n",
        "    to return only the single correct letter.\n",
        "    \"\"\"\n",
        "    system_prompt = \"\"\"You are a medical exam answering machine. Your only task is to answer the following multiple-choice medical question. Read the question and the provided options (Ø£, Ø¨, Ø¬, Ø¯, Ù‡). Your response must be ONLY the single Arabic letter corresponding to the correct answer. Do not provide any explanation, reasoning, or any other text. For example, if option 'Ø¨' is correct, your entire response must be 'Ø¨'.\"\"\"\n",
        "\n",
        "    max_retries = 3\n",
        "    retry_delay = 5\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            chat_completion = client.chat.completions.create(\n",
        "              messages=[\n",
        "                  {\"role\":\"system\", \"content\": system_prompt},\n",
        "                  {\"role\":\"user\",\"content\":question}\n",
        "              ],\n",
        "              model=\"llama3-70b-8192\", # Using a standard available model\n",
        "              temperature=0.0,\n",
        "              max_tokens=5,\n",
        "            )\n",
        "            response_text = chat_completion.choices[0].message.content.strip()\n",
        "\n",
        "            # Clean the response to ensure it's just a single Arabic letter\n",
        "            arabic_letters = re.findall(r'[\\u0621-\\u064A]', response_text)\n",
        "            if arabic_letters:\n",
        "                return arabic_letters[0]\n",
        "            else:\n",
        "                print(f\"  -> Warning: Model returned an unexpected response: '{response_text}'. Recording as empty.\")\n",
        "                return \"\" # Return empty if no letter is found\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  -> An error occurred: {e}. Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  -> API Error after multiple retries: {e}\")\n",
        "                return \"API_ERROR\"\n",
        "    return \"FAILED_ATTEMPTS\"\n",
        "\n",
        "\n",
        "# --- Function to Evaluate MCQ Accuracy ---\n",
        "def evaluate_mcq_accuracy(predictions, ground_truths):\n",
        "    \"\"\"\n",
        "    Calculates and prints the accuracy of the MCQ predictions.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸš€ Starting Evaluation...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if len(predictions) != len(ground_truths):\n",
        "        print(\"Warning: Prediction and ground truth lists have different lengths. Evaluation might be inaccurate.\")\n",
        "        min_len = min(len(predictions), len(ground_truths))\n",
        "        predictions = predictions[:min_len]\n",
        "        ground_truths = ground_truths[:min_len]\n",
        "\n",
        "    # Filter out API errors before calculating accuracy\n",
        "    valid_indices = [i for i, p in enumerate(predictions) if p not in [\"API_ERROR\", \"FAILED_ATTEMPTS\"]]\n",
        "    valid_predictions = [predictions[i] for i in valid_indices]\n",
        "    valid_ground_truths = [ground_truths[i] for i in valid_indices]\n",
        "\n",
        "    if not valid_predictions:\n",
        "        print(\"No valid predictions to evaluate. Check for widespread API errors.\")\n",
        "        return\n",
        "\n",
        "    accuracy = accuracy_score(valid_ground_truths, valid_predictions)\n",
        "    correct_predictions = sum(p == g for p, g in zip(valid_predictions, valid_ground_truths))\n",
        "    total_valid_predictions = len(valid_predictions)\n",
        "    total_questions = len(ground_truths)\n",
        "    api_errors = total_questions - total_valid_predictions\n",
        "\n",
        "\n",
        "    print(f\"Total Questions Attempted: {total_questions}\")\n",
        "    print(f\"API Errors/Failed Attempts: {api_errors}\")\n",
        "    print(f\"Valid Predictions to Evaluate: {total_valid_predictions}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Correct Predictions: {correct_predictions} / {total_valid_predictions}\")\n",
        "    print(f\"ðŸ“Š Accuracy (on valid responses): {accuracy * 100:.2f}%\")\n",
        "    print(\"=\"*50 + \"\\nâœ… Evaluation Complete.\\n\" + \"=\"*50)\n",
        "\n",
        "\n",
        "# --- Main Execution for MCQ Task ---\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function for the Multiple Choice Question Answering task.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use encoding='utf-8' to handle Arabic characters properly\n",
        "        df = pd.read_csv(INPUT_CSV, encoding='utf-8')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{INPUT_CSV}' was not found. Please check the path.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    # Check if the corrected column names exist in the DataFrame\n",
        "    if QUESTION_COLUMN not in df.columns or ANSWER_COLUMN not in df.columns:\n",
        "        print(f\"Error: Required columns ('{QUESTION_COLUMN}', '{ANSWER_COLUMN}') not found in the CSV.\")\n",
        "        print(f\"Available columns are: {df.columns.tolist()}\")\n",
        "        return\n",
        "\n",
        "    # Drop rows where the question or answer is missing\n",
        "    df.dropna(subset=[QUESTION_COLUMN, ANSWER_COLUMN], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "    if os.path.exists(OUTPUT_CSV):\n",
        "        print(f\"âœ… Found existing prediction file: '{OUTPUT_CSV}'.\")\n",
        "        print(\"Skipping generation and loading predictions from file for evaluation.\")\n",
        "        predictions_df = pd.read_csv(OUTPUT_CSV, header=None, encoding='utf-8')\n",
        "        predictions = predictions_df[0].astype(str).tolist() # Ensure predictions are read as strings\n",
        "    else:\n",
        "        print(f\"'{OUTPUT_CSV}' not found. Starting prediction generation process...\")\n",
        "        predictions = []\n",
        "        total_questions = len(df)\n",
        "        for index, row in df.iterrows():\n",
        "            question = row[QUESTION_COLUMN]\n",
        "            print(f\"Processing question {index + 1}/{total_questions}...\")\n",
        "            answer_letter = generate_answer(question)\n",
        "            predictions.append(answer_letter)\n",
        "            print(f\"  -> Ground Truth: {str(row[ANSWER_COLUMN]).strip()[0]} | Model's Prediction: {answer_letter}\")\n",
        "\n",
        "            # Delay for Groq API rate limit (30 RPM limit)\n",
        "            if index < total_questions - 1:\n",
        "                time.sleep(2.1) # Sleep for slightly over 2 seconds\n",
        "\n",
        "        predictions_df = pd.DataFrame(predictions)\n",
        "        predictions_df.to_csv(OUTPUT_CSV, header=False, index=False, encoding='utf-8')\n",
        "        print(f\"\\nSuccessfully generated predictions and saved them to '{OUTPUT_CSV}'.\")\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(OUTPUT_CSV)\n",
        "        except ImportError:\n",
        "            print(f\"To download '{OUTPUT_CSV}', see the file browser on the left.\")\n",
        "\n",
        "    # Extracting the first character from the Answer column as the ground truth\n",
        "    ground_truths = [str(ans).strip()[0] for ans in df[ANSWER_COLUMN].tolist()]\n",
        "\n",
        "    evaluate_mcq_accuracy(predictions, ground_truths)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qp4olnfAV5hP",
        "outputId": "e890cf4c-78bd-4d03-f4c2-52450a5c8875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices.csv' not found. Starting prediction generation process...\n",
            "Processing question 1/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
            "Processing question 2/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 3/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 4/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 5/100...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 6/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 7/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 8/100...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¬\n",
            "Processing question 9/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 10/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 11/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 12/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 13/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 14/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 15/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 16/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 17/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 18/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 19/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 20/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 21/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¨\n",
            "Processing question 22/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 23/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 24/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 25/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 26/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 27/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
            "Processing question 28/100...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¨\n",
            "Processing question 29/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 30/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 31/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 32/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¬\n",
            "Processing question 33/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 34/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 35/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 36/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 37/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 38/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 39/100...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¨\n",
            "Processing question 40/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 41/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 42/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 43/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 44/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 45/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
            "Processing question 46/100...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 47/100...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 48/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 49/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
            "Processing question 50/100...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¬\n",
            "Processing question 51/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 52/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 53/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 54/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 55/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 56/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 57/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
            "Processing question 58/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
            "Processing question 59/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 60/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 61/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 62/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 63/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 64/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 65/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 66/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 67/100...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¨\n",
            "Processing question 68/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¬\n",
            "Processing question 69/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 70/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 71/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 72/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 73/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 74/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 75/100...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 76/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 77/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 78/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 79/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¬\n",
            "Processing question 80/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 81/100...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 82/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 83/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 84/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 85/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 86/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 87/100...\n",
            "  -> Ground Truth: Ø¥ | Model's Prediction: Ø¯\n",
            "Processing question 88/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¨\n",
            "Processing question 89/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 90/100...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 91/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 92/100...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 93/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 94/100...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¨\n",
            "Processing question 95/100...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¬\n",
            "Processing question 96/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 97/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 98/100...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
            "Processing question 99/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 100/100...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "\n",
            "Successfully generated predictions and saved them to '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices.csv'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_82b9ff1d-9565-4b16-99d4-e10e07fe4e73\", \"predictions_fitb_choices.csv\", 300)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸš€ Starting Evaluation...\n",
            "==================================================\n",
            "Total Questions Attempted: 100\n",
            "API Errors/Failed Attempts: 0\n",
            "Valid Predictions to Evaluate: 100\n",
            "--------------------\n",
            "Correct Predictions: 69 / 100\n",
            "ðŸ“Š Accuracy (on valid responses): 69.00%\n",
            "==================================================\n",
            "âœ… Evaluation Complete.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2JwGSfAcaW2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRcBmcc5aW9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Track 2: MedArabiQ 2025 (General Health)\n",
        "\n",
        "\n",
        "# Sub-Task 1: Fill-in-the-blank with Choices\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# deepseek -> 17%"
      ],
      "metadata": {
        "id": "6v4YX7HBaXGe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kdvDDWDfaZ0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import groq\n",
        "import time\n",
        "from getpass import getpass\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- Groq API Configuration ---\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "except (ImportError, KeyError):\n",
        "    print(\"Secret 'GROQ_API_KEY' not found. Please add it to the Colab secrets manager.\")\n",
        "    GROQ_API_KEY = getpass('Please enter your Groq API key: ')\n",
        "\n",
        "# Initialize the Groq client\n",
        "client = groq.Client(api_key=GROQ_API_KEY)\n",
        "\n",
        "\n",
        "# --- File paths and column names for the MCQ task ---\n",
        "INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/fill-in-the-blank-choices.csv'\n",
        "OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek.csv'\n",
        "\n",
        "# Column names from your CSV\n",
        "QUESTION_COLUMN = 'Question - Arabic'\n",
        "ANSWER_COLUMN = 'Answer - Arabic'\n",
        "\n",
        "\n",
        "# --- Function to Generate Answers for MCQ Task using DeepSeek ---\n",
        "def generate_answer(question):\n",
        "    \"\"\"\n",
        "    Sends an MCQ question to the Groq API, prompting the DeepSeek model\n",
        "    to return only the single correct letter. Includes cleaning for <think> blocks.\n",
        "    \"\"\"\n",
        "    # --- UPDATED: Added an explicit instruction to not use <think> tags ---\n",
        "    system_prompt = \"\"\"You are a medical exam answering machine. Your only task is to answer the following multiple-choice medical question. Read the question and the provided options (Ø£, Ø¨, Ø¬, Ø¯, Ù‡). Your response must be ONLY the single Arabic letter corresponding to the correct answer. Do not provide any explanation, reasoning, or any other text. Do not use <think> tags. For example, if option 'Ø¨' is correct, your entire response must be 'Ø¨'.\"\"\"\n",
        "\n",
        "    max_retries = 3\n",
        "    retry_delay = 5\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            chat_completion = client.chat.completions.create(\n",
        "              messages=[\n",
        "                  {\"role\":\"system\", \"content\": system_prompt},\n",
        "                  {\"role\":\"user\",\"content\":question}\n",
        "              ],\n",
        "              model=\"deepseek-r1-distill-llama-70b\",\n",
        "              temperature=0.0,\n",
        "              # --- UPDATED: Increased max_tokens significantly to prevent cutoff ---\n",
        "              max_tokens=150,\n",
        "            )\n",
        "            raw_response_text = chat_completion.choices[0].message.content.strip()\n",
        "\n",
        "            # The cleaning logic is now a robust failsafe.\n",
        "            # It will work correctly now that the model has enough tokens to provide a complete response.\n",
        "            cleaned_text = re.sub(r'<think>.*?</think>', '', raw_response_text, flags=re.DOTALL).strip()\n",
        "            arabic_letters = re.findall(r'[\\u0621-\\u064A]', cleaned_text)\n",
        "\n",
        "            if arabic_letters:\n",
        "                return arabic_letters[0]\n",
        "            else:\n",
        "                print(f\"  -> Warning: No Arabic letter found after cleaning. Cleaned response: '{cleaned_text}'. Recording as empty.\")\n",
        "                return \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  -> An error occurred: {e}. Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  -> API Error after multiple retries: {e}\")\n",
        "                return \"API_ERROR\"\n",
        "    return \"FAILED_ATTEMPTS\"\n",
        "\n",
        "\n",
        "# --- Function to Evaluate MCQ Accuracy (Unchanged) ---\n",
        "def evaluate_mcq_accuracy(predictions, ground_truths):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸš€ Starting Evaluation...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if len(predictions) != len(ground_truths):\n",
        "        print(\"Warning: Prediction and ground truth lists have different lengths. Evaluation might be inaccurate.\")\n",
        "        min_len = min(len(predictions), len(ground_truths))\n",
        "        predictions = predictions[:min_len]\n",
        "        ground_truths = ground_truths[:min_len]\n",
        "\n",
        "    valid_indices = [i for i, p in enumerate(predictions) if p not in [\"API_ERROR\", \"FAILED_ATTEMPTS\"]]\n",
        "    valid_predictions = [predictions[i] for i in valid_indices]\n",
        "    valid_ground_truths = [ground_truths[i] for i in valid_indices]\n",
        "\n",
        "    if not valid_predictions:\n",
        "        print(\"No valid predictions to evaluate. Check for widespread API errors.\")\n",
        "        return\n",
        "\n",
        "    accuracy = accuracy_score(valid_ground_truths, valid_predictions)\n",
        "    correct_predictions = sum(p == g for p, g in zip(valid_predictions, valid_ground_truths))\n",
        "    total_valid_predictions = len(valid_predictions)\n",
        "    total_questions = len(ground_truths)\n",
        "    api_errors = total_questions - total_valid_predictions\n",
        "\n",
        "    print(f\"Total Questions Attempted: {total_questions}\")\n",
        "    print(f\"API Errors/Failed Attempts: {api_errors}\")\n",
        "    print(f\"Valid Predictions to Evaluate: {total_valid_predictions}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Correct Predictions: {correct_predictions} / {total_valid_predictions}\")\n",
        "    print(f\"ðŸ“Š Accuracy (on valid responses): {accuracy * 100:.2f}%\")\n",
        "    print(\"=\"*50 + \"\\nâœ… Evaluation Complete.\\n\" + \"=\"*50)\n",
        "\n",
        "\n",
        "# --- Main Execution for MCQ Task (Unchanged) ---\n",
        "def main():\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_CSV, encoding='utf-8')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{INPUT_CSV}' was not found. Please check the path.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    if QUESTION_COLUMN not in df.columns or ANSWER_COLUMN not in df.columns:\n",
        "        print(f\"Error: Required columns ('{QUESTION_COLUMN}', '{ANSWER_COLUMN}') not found in the CSV.\")\n",
        "        print(f\"Available columns are: {df.columns.tolist()}\")\n",
        "        return\n",
        "\n",
        "    df.dropna(subset=[QUESTION_COLUMN, ANSWER_COLUMN], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    if os.path.exists(OUTPUT_CSV):\n",
        "        print(f\"âœ… Found existing prediction file: '{OUTPUT_CSV}'.\")\n",
        "        print(\"Skipping generation and loading predictions from file for evaluation.\")\n",
        "        predictions_df = pd.read_csv(OUTPUT_CSV, header=None, encoding='utf-8')\n",
        "        predictions = predictions_df[0].astype(str).tolist()\n",
        "    else:\n",
        "        print(f\"'{OUTPUT_CSV}' not found. Starting prediction generation process with DeepSeek...\")\n",
        "        predictions = []\n",
        "        total_questions = len(df)\n",
        "        for index, row in df.iterrows():\n",
        "            question = row[QUESTION_COLUMN]\n",
        "            print(f\"Processing question {index + 1}/{total_questions} with DeepSeek (MCQ Mode)...\")\n",
        "            answer_letter = generate_answer(question)\n",
        "            predictions.append(answer_letter)\n",
        "            print(f\"  -> Ground Truth: {str(row[ANSWER_COLUMN]).strip()[0]} | Model's Prediction: {answer_letter}\")\n",
        "\n",
        "            if index < total_questions - 1:\n",
        "                time.sleep(2.1)\n",
        "\n",
        "        predictions_df = pd.DataFrame(predictions)\n",
        "        predictions_df.to_csv(OUTPUT_CSV, header=False, index=False, encoding='utf-8')\n",
        "        print(f\"\\nSuccessfully generated predictions and saved them to '{OUTPUT_CSV}'.\")\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(OUTPUT_CSV)\n",
        "        except ImportError:\n",
        "            print(f\"To download '{OUTPUT_CSV}', see the file browser on the left.\")\n",
        "\n",
        "    ground_truths = [str(ans).strip()[0] for ans in df[ANSWER_COLUMN].tolist()]\n",
        "    evaluate_mcq_accuracy(predictions, ground_truths)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ePB6zbLHaZ94",
        "outputId": "498c13ef-0df1-4032-e1a1-fb1d3a0b41a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek.csv' not found. Starting prediction generation process with DeepSeek...\n",
            "Processing question 1/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 2/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø§\n",
            "Processing question 3/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Okay, so I've got this medical question here about pleural effusion. I'm a bit rusty on my respiratory system, but let me try to think this through.\n",
            "\n",
            "The question is about filling in the blanks in a sentence. It says that in cases of pleural effusion, the decrease or absence of tactile fremitus on the affected side indicates something, which is caused by something else.\n",
            "\n",
            "First, I need to remember what pleural effusion is. From what I recall, it's when there's an abnormal accumulation of fluid in the pleural space, which is the area between the lungs and the chest wall. This fluid buildup can be due to various reasons like infections, heart failure, or even cancer.\n",
            "\n",
            "Now,'. Recording as empty.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: \n",
            "Processing question 4/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Okay, so I've got this medical question here, and I need to figure it out step by step. Let me read it again carefully.\n",
            "\n",
            "The question is about iron transport and storage in the body. It says there are three main categories of proteins involved. The first two are Transferrins and Transferrin receptors (TFR1). The third one is missing, and I need to choose the correct option from the given choices. Also, the second part of the question asks how many iron ions each transferrin molecule can bind.\n",
            "\n",
            "Alright, I remember that iron metabolism involves several proteins. Transferrin is the main protein that transports iron in the blood. The receptors, like TFR1, help cells take'. Recording as empty.\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: \n",
            "Processing question 5/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
            "Processing question 6/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: ÙŠ\n",
            "Processing question 7/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 8/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
            "Processing question 9/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: ÙŠ\n",
            "Processing question 10/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: ÙŠ\n",
            "Processing question 11/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø­\n",
            "Processing question 12/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Øª\n",
            "Processing question 13/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Okay, so I've got this question about Guillain-BarrÃ© Syndrome (GBS) and cerebrospinal fluid (CSF) findings. I'm a bit rusty on my neurology, but let me think this through.\n",
            "\n",
            "First, I remember that GBS is an autoimmune disorder where the body's immune system attacks the peripheral nerves. This leads to muscle weakness, sometimes paralysis, and other neurological symptoms. Now, the question is about what's found in the CSF in GBS patients.\n",
            "\n",
            "I recall that in GBS, the CSF analysis typically shows an elevated protein level without a significant increase in white blood cell count. This is often referred to as \"albuminocytological dissociation,\" where the protein is'. Recording as empty.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: \n",
            "Processing question 14/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 15/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Øª\n",
            "Processing question 16/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Okay, so I've got this medical question here about ECG findings in myocarditis. Hmm, I'm a bit rusty on this, but let me think through it.\n",
            "\n",
            "The question is asking about the classic ECG results in myocarditis and what commonly accompanies them. The sentence has two blanks, so I need to figure out both parts.\n",
            "\n",
            "First, I remember that myocarditis is inflammation of the heart muscle, often due to viral infections. ECG changes can vary, but there are some classic signs. One common finding is the presence of ST-segment elevation. That's because the inflammation can cause the heart's cells to become irritated, leading to changes in the electrical activity, which shows up as ST elevation.\n",
            "\n",
            "Another thing I recall'. Recording as empty.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: \n",
            "Processing question 17/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 18/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 19/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Okay, so I've got this medical question here about Systemic Lupus Erythematosus (SLE), and I need to fill in the blanks. Let me think through this step by step.\n",
            "\n",
            "First, the question is in Arabic, but I can translate it to understand better. It says: \"In systemic lupus erythematosus, the immune system produces antibodies known as _____, which target nuclear components. The disease can be diagnosed using tests to detect antibodies against _____ using immunological methods.\"\n",
            "\n",
            "So, the first blank is about the type of antibodies produced, and the second is about what those antibodies target.\n",
            "\n",
            "I remember that in SLE, the immune system mistakenly attacks the body's own tissues, particularly the nucleus of'. Recording as empty.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: \n",
            "Processing question 20/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Øª\n",
            "Processing question 21/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 22/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 23/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Okay, so I've got this medical question here about peripheral neuropathy caused by vitamin B6 toxicity. Hmm, I need to figure out the correct answer. Let me think through this step by step.\n",
            "\n",
            "First, I remember that vitamin B6, or pyridoxine, is a B complex vitamin. It's important for brain function and helping the body make the hormones serotonin and norepinephrine. But too much of it can be a problem. I've heard that excessive intake can lead to toxicity, which affects the nervous system.\n",
            "\n",
            "Peripheral neuropathy is a common side effect of vitamin B6 toxicity. So, what does that neuropathy present like? I think peripheral neuropathy can cause various symptoms depending on which nerves are'. Recording as empty.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: \n",
            "Processing question 24/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 25/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 26/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 27/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 28/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
            "Processing question 29/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 30/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Okay, so I've got this question about thalassemia syndromes. I'm a bit rusty on my hematology, but let me think through this.\n",
            "\n",
            "The question is asking about the types of globin chain deficiencies that cause thalassemia. The sentence says it's a group of genetic disorders resulting from a deficiency in the synthesis of globin chains, and the blanks are for the types of chains involved.\n",
            "\n",
            "I remember that globin chains are a key part of hemoglobin. There are different types, like alpha, beta, gamma, and delta. Thalassemia is classified based on which chains are affected.\n",
            "\n",
            "The main types I recall are alpha-thalassemia and beta-thalas'. Recording as empty.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: \n",
            "Processing question 31/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 32/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 33/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Øª\n",
            "Processing question 34/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 35/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Øª\n",
            "Processing question 36/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ù…\n",
            "Processing question 37/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: ÙŠ\n",
            "Processing question 38/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 39/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
            "Processing question 40/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 41/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: ÙŠ\n",
            "Processing question 42/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 43/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 44/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 45/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 46/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ù…\n",
            "Processing question 47/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
            "Processing question 48/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ù\n",
            "Processing question 49/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Øª\n",
            "Processing question 50/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Øª\n",
            "Processing question 51/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 52/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 53/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 54/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 55/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: ÙŠ\n",
            "Processing question 56/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 57/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 58/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 59/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 60/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Okay, so I've got this medical question here about the D-Dimer test. I'm a bit rusty on this, but I'll try to think it through. The question is about filling in the blank in a sentence. The sentence says that a positive D-Dimer test indicates a possible blood clot, and a negative test means what?\n",
            "\n",
            "First, I remember that D-Dimer is a blood test used to help diagnose or rule out thrombotic disorders. Thrombotic disorders include things like deep vein thrombosis (DVT) or pulmonary embolism (PE). The D-Dimer test measures the levels of a protein fragment called D-Dimer, which is produced when a blood clot dissolves. So, if'. Recording as empty.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: \n",
            "Processing question 61/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 62/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 63/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Okay, so I've got this medical question here about the corneal layer dissolution. Hmm, I remember that the cornea is the clear part at the front of the eye, right? And when it gets infected, it can lead to something called corneal ulcers or maybe keratitis. \n",
            "\n",
            "The question is asking about the pathogens responsible for this condition. The options are pairs of bacteria. Let me think about the common causes of corneal infections. I remember that Staphylococcus and Streptococcus are pretty common in eye infections. They're both Gram-positive cocci. \n",
            "\n",
            "Wait, but I also recall that Pseudomonas is a big player in corneal ulcers, especially in contact lens'. Recording as empty.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: \n",
            "Processing question 64/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Okay, so I have this medical question to answer, and I need to figure it out step by step. The question is about Erb-Duchenne palsy, which I remember is a type of brachial plexus injury. The question is asking which nerve roots are involved in this condition.\n",
            "\n",
            "First, I should recall what Erb-Duchenne palsy entails. From what I remember, it's the most common type of obstetric brachial plexus injury. It usually happens during childbirth when the baby's shoulder gets stuck during delivery, causing the nerves in the brachial plexus to stretch or tear.\n",
            "\n",
            "The brachial plexus is a network of nerves that originates in the spinal cord'. Recording as empty.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: \n",
            "Processing question 65/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¥\n",
            "Processing question 66/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Okay, so I've got this medical question here about ketoacidosis in diabetes. Hmm, I remember that ketoacidosis is a serious complication, especially in type 1 diabetes. It happens when the body doesn't have enough insulin, so it starts breaking down fat for energy instead of glucose. \n",
            "\n",
            "Wait, when the body breaks down fat, it produces ketones. Oh right, ketones are acidic, so if they build up, the blood becomes too acidic, leading to ketoacidosis. The question is asking which substance accumulates to cause this.\n",
            "\n",
            "Looking at the options: \n",
            "\n",
            "A is lactic acid. I know lactic acid can build up during intense exercise or in certain conditions, causing lactic acidosis, but'. Recording as empty.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: \n",
            "Processing question 67/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Øº\n",
            "Processing question 68/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 69/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 70/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 71/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¯\n",
            "Processing question 72/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 73/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 74/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: ÙŠ\n",
            "Processing question 75/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
            "Processing question 76/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ù\n",
            "Processing question 77/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 78/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 79/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 80/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 81/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø§\n",
            "Processing question 82/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 83/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø®\n",
            "Processing question 84/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Okay, so I have this medical question to answer, and I need to figure it out step by step. The question is about confirming the diagnosis of an insulinoma, which I remember is a type of tumor that produces insulin. The sentence to fill in is about the blood glucose and insulin levels during a fasting episode.\n",
            "\n",
            "First, I need to recall the normal and abnormal ranges for blood glucose and insulin. I know that normal fasting blood glucose is usually between 70 to 100 mg/dL. When someone is diagnosed with an insulinoma, they have episodes of hypoglycemia because the tumor secretes insulin inappropriately, even when blood glucose is low.\n",
            "\n",
            "So during a hypoglycemic episode caused by insulinoma'. Recording as empty.\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: \n",
            "Processing question 85/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 86/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ù\n",
            "Processing question 87/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Alright, so I've got this medical question here about the treatment for corneal ulcers. The question is in Arabic, and I need to figure out the correct answer. Let me break it down step by step.\n",
            "\n",
            "The question is asking about the topical treatment for corneal ulcers, specifically what medication is used. The options are erythromycin, penicillin, ciprofloxacin, and tetracycline.\n",
            "\n",
            "First, I remember that corneal ulcers can be caused by different types of infectionsâ€”bacterial, viral, or fungal. The treatment depends on the cause. Since the question is about topical treatment, I'm focusing on eye drops or ointments.\n",
            "\n",
            "Erythromycin'. Recording as empty.\n",
            "  -> Ground Truth: Ø¥ | Model's Prediction: \n",
            "Processing question 88/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 89/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 90/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ù\n",
            "Processing question 91/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ù\n",
            "Processing question 92/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
            "Processing question 93/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ù…\n",
            "Processing question 94/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: ÙŠ\n",
            "Processing question 95/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
            "Okay, so I have this medical question to answer, and I need to figure it out step by step. The question is about the diagnosis of exudative pleural effusion using Light's criteria, specifically focusing on the LDH levels in pleural fluid compared to serum.\n",
            "\n",
            "First, I remember that Light's criteria are used to differentiate between exudative and transudative pleural effusions. Exudative effusions are typically caused by inflammatory or malignant processes, while transudative are due to fluid overload or other non-inflammatory conditions.\n",
            "\n",
            "The criteria mention that for a fluid to be considered exudative, it must meet at least one of the following: the pleural fluid protein to serum protein ratio is greater than 0'. Recording as empty.\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: \n",
            "Processing question 96/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 97/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ù\n",
            "Processing question 98/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø§\n",
            "Processing question 99/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 100/100 with DeepSeek (MCQ Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: ÙŠ\n",
            "\n",
            "Successfully generated predictions and saved them to '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek.csv'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_00578c14-5cc4-46e0-a42e-609ee44ec442\", \"predictions_fitb_choices_deepseek.csv\", 300)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸš€ Starting Evaluation...\n",
            "==================================================\n",
            "Total Questions Attempted: 100\n",
            "API Errors/Failed Attempts: 0\n",
            "Valid Predictions to Evaluate: 100\n",
            "--------------------\n",
            "Correct Predictions: 17 / 100\n",
            "ðŸ“Š Accuracy (on valid responses): 17.00%\n",
            "==================================================\n",
            "âœ… Evaluation Complete.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYoKe6uAVmx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# deep seek cahin of thoughts"
      ],
      "metadata": {
        "id": "6sFt6pCCVnFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import groq\n",
        "import time\n",
        "from getpass import getpass\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- Groq API Configuration ---\n",
        "# It's recommended to use environment variables or a secret manager for API keys.\n",
        "try:\n",
        "    # Used when running in Google Colab\n",
        "    from google.colab import userdata\n",
        "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "    print(\"Successfully loaded GROQ_API_KEY from Colab secrets.\")\n",
        "except (ImportError, KeyError):\n",
        "    # Fallback for local execution\n",
        "    if 'GROQ_API_KEY' in os.environ:\n",
        "        GROQ_API_KEY = os.environ.get('GROQ_API_KEY')\n",
        "        print(\"Successfully loaded GROQ_API_KEY from environment variables.\")\n",
        "    else:\n",
        "        print(\"GROQ_API_KEY not found in Colab secrets or environment variables.\")\n",
        "        GROQ_API_KEY = getpass('Please enter your Groq API key: ')\n",
        "\n",
        "# Initialize the Groq client\n",
        "client = groq.Client(api_key=GROQ_API_KEY)\n",
        "\n",
        "\n",
        "# --- File paths and column names for the MCQ task ---\n",
        "# Ensure this path points to your actual file location\n",
        "INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/fill-in-the-blank-choices.csv'\n",
        "OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek_cot.csv' # Changed output file name\n",
        "\n",
        "# Column names from your CSV\n",
        "QUESTION_COLUMN = 'Question - Arabic'\n",
        "ANSWER_COLUMN = 'Answer - Arabic'\n",
        "\n",
        "\n",
        "# --- MODIFIED Function to Generate Answers using Chain of Thought ---\n",
        "def generate_answer(question):\n",
        "    \"\"\"\n",
        "    Sends an MCQ question to the Groq API, prompting the DeepSeek model\n",
        "    to use a Chain of Thought process and return only the single correct letter\n",
        "    in a structured format.\n",
        "    \"\"\"\n",
        "    # --- NEW: Chain of Thought System Prompt ---\n",
        "    # This prompt guides the model to first \"think\" and then provide a final answer.\n",
        "    system_prompt = \"\"\"You are an expert medical professional and a meticulous test-taker. Your task is to answer the following multiple-choice medical question.\n",
        "\n",
        "Follow these steps precisely:\n",
        "1.  First, carefully analyze the question and all the provided options (Ø£, Ø¨, Ø¬, Ø¯, Ù‡).\n",
        "2.  Engage in a step-by-step reasoning process to determine the correct answer. Explain the medical context, evaluate each option, and state why you are choosing one and eliminating the others.\n",
        "3.  Enclose this entire thought process within `<thinking>` and `</thinking>` tags.\n",
        "4.  After the closing `</thinking>` tag, on a new line, state the final answer clearly and concisely. The final answer line must be in the format: `Final Answer: [letter]`, where `[letter]` is the single Arabic letter corresponding to the correct option.\n",
        "\n",
        "Example Response Format:\n",
        "<thinking>\n",
        "[Your detailed, step-by-step analysis of the question and options goes here...]\n",
        "</thinking>\n",
        "Final Answer: Ø¨\n",
        "\"\"\"\n",
        "\n",
        "    max_retries = 3\n",
        "    retry_delay = 5 # seconds\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            chat_completion = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": question}\n",
        "                ],\n",
        "                model=\"deepseek-r1-distill-llama-70b\",\n",
        "                temperature=0.0,\n",
        "                # --- UPDATED: Increased max_tokens for Chain of Thought ---\n",
        "                max_tokens=1024,\n",
        "            )\n",
        "            raw_response_text = chat_completion.choices[0].message.content.strip()\n",
        "\n",
        "            # --- NEW: Robust parsing for \"Final Answer: [letter]\" format ---\n",
        "            # This regex looks for \"Final Answer:\", optional whitespace, and captures the Arabic letter.\n",
        "            match = re.search(r\"Final Answer:\\s*([\\u0621-\\u064A])\", raw_response_text)\n",
        "\n",
        "            if match:\n",
        "                # The first captured group is the letter we need.\n",
        "                return match.group(1)\n",
        "            else:\n",
        "                # Fallback in case the model doesn't follow the format.\n",
        "                # Try to find the last Arabic letter in the response as a last resort.\n",
        "                arabic_letters = re.findall(r'[\\u0621-\\u064A]', raw_response_text)\n",
        "                if arabic_letters:\n",
        "                    print(f\"  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\")\n",
        "                    return arabic_letters[-1] # Return the last letter, which is likely the intended answer\n",
        "                else:\n",
        "                    print(f\"  -> Warning: No final answer or Arabic letter found. Full response: '{raw_response_text}'. Recording as empty.\")\n",
        "                    return \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  -> An error occurred: {e}. Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  -> API Error after multiple retries: {e}\")\n",
        "                return \"API_ERROR\"\n",
        "    return \"FAILED_ATTEMPTS\"\n",
        "\n",
        "\n",
        "# --- Function to Evaluate MCQ Accuracy (Unchanged) ---\n",
        "def evaluate_mcq_accuracy(predictions, ground_truths):\n",
        "    \"\"\"Calculates and prints the accuracy of the model's predictions.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸš€ Starting Evaluation...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if len(predictions) != len(ground_truths):\n",
        "        print(\"Warning: Prediction and ground truth lists have different lengths. Evaluation might be inaccurate.\")\n",
        "        min_len = min(len(predictions), len(ground_truths))\n",
        "        predictions = predictions[:min_len]\n",
        "        ground_truths = ground_truths[:min_len]\n",
        "\n",
        "    valid_indices = [i for i, p in enumerate(predictions) if p not in [\"API_ERROR\", \"FAILED_ATTEMPTS\", \"\"]]\n",
        "    valid_predictions = [predictions[i] for i in valid_indices]\n",
        "    valid_ground_truths = [ground_truths[i] for i in valid_indices]\n",
        "\n",
        "    if not valid_predictions:\n",
        "        print(\"No valid predictions to evaluate. Check for widespread API or formatting errors.\")\n",
        "        return\n",
        "\n",
        "    accuracy = accuracy_score(valid_ground_truths, valid_predictions)\n",
        "    correct_predictions = sum(p == g for p, g in zip(valid_predictions, valid_ground_truths))\n",
        "    total_valid_predictions = len(valid_predictions)\n",
        "    total_questions = len(ground_truths)\n",
        "    failed_or_empty = total_questions - total_valid_predictions\n",
        "\n",
        "    print(f\"Total Questions Attempted: {total_questions}\")\n",
        "    print(f\"API Errors/Failed/Empty: {failed_or_empty}\")\n",
        "    print(f\"Valid Predictions to Evaluate: {total_valid_predictions}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Correct Predictions: {correct_predictions} / {total_valid_predictions}\")\n",
        "    print(f\"ðŸ“Š Accuracy (on valid responses): {accuracy * 100:.2f}%\")\n",
        "    print(\"=\"*50 + \"\\nâœ… Evaluation Complete.\\n\" + \"=\"*50)\n",
        "\n",
        "\n",
        "# --- Main Execution for MCQ Task (Unchanged) ---\n",
        "def main():\n",
        "    \"\"\"Main function to run the MCQ prediction and evaluation process.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_CSV, encoding='utf-8')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{INPUT_CSV}' was not found. Please check the path.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    if QUESTION_COLUMN not in df.columns or ANSWER_COLUMN not in df.columns:\n",
        "        print(f\"Error: Required columns ('{QUESTION_COLUMN}', '{ANSWER_COLUMN}') not found.\")\n",
        "        print(f\"Available columns are: {df.columns.tolist()}\")\n",
        "        return\n",
        "\n",
        "    df.dropna(subset=[QUESTION_COLUMN, ANSWER_COLUMN], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    if os.path.exists(OUTPUT_CSV):\n",
        "        print(f\"âœ… Found existing prediction file: '{OUTPUT_CSV}'.\")\n",
        "        print(\"Skipping generation and loading predictions from file for evaluation.\")\n",
        "        predictions_df = pd.read_csv(OUTPUT_CSV, header=None, encoding='utf-8')\n",
        "        predictions = predictions_df[0].astype(str).tolist()\n",
        "    else:\n",
        "        print(f\"'{OUTPUT_CSV}' not found. Starting prediction generation with DeepSeek (CoT Mode)...\")\n",
        "        predictions = []\n",
        "        total_questions = len(df)\n",
        "        for index, row in df.iterrows():\n",
        "            question = row[QUESTION_COLUMN]\n",
        "            print(f\"Processing question {index + 1}/{total_questions} with DeepSeek (CoT Mode)...\")\n",
        "            answer_letter = generate_answer(question)\n",
        "            predictions.append(answer_letter)\n",
        "            # Normalize ground truth for accurate comparison\n",
        "            ground_truth_letter = str(row[ANSWER_COLUMN]).strip()[0]\n",
        "            print(f\"  -> Ground Truth: {ground_truth_letter} | Model's Prediction: {answer_letter}\")\n",
        "\n",
        "            # Optional: A delay between API calls to avoid rate limiting\n",
        "            if index < total_questions - 1:\n",
        "                time.sleep(1) # A small delay can be helpful\n",
        "\n",
        "        predictions_df = pd.DataFrame(predictions)\n",
        "        predictions_df.to_csv(OUTPUT_CSV, header=False, index=False, encoding='utf-8')\n",
        "        print(f\"\\nSuccessfully generated predictions and saved them to '{OUTPUT_CSV}'.\")\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(OUTPUT_CSV)\n",
        "        except ImportError:\n",
        "            print(f\"To download '{OUTPUT_CSV}', please use the file browser.\")\n",
        "\n",
        "    # Normalize the ground truth answers to be a single letter\n",
        "    ground_truths = [str(ans).strip()[0] for ans in df[ANSWER_COLUMN].tolist()]\n",
        "    evaluate_mcq_accuracy(predictions, ground_truths)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZxDKonVYVqLd",
        "outputId": "951d266a-fc47-4acf-a6ff-c16e0ddb4ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded GROQ_API_KEY from Colab secrets.\n",
            "'/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek_cot.csv' not found. Starting prediction generation with DeepSeek (CoT Mode)...\n",
            "Processing question 1/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 2/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 3/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 4/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 5/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 6/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 7/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 8/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 9/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¨\n",
            "Processing question 10/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 11/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 12/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 13/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 14/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¯\n",
            "Processing question 15/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 16/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø©\n",
            "Processing question 17/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 18/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 19/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¶\n",
            "Processing question 20/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ù…\n",
            "Processing question 21/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: ÙŠ\n",
            "Processing question 22/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 23/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 24/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 25/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 26/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 27/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 28/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¨\n",
            "Processing question 29/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 30/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 31/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 32/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¬\n",
            "Processing question 33/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 34/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 35/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 36/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 37/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø©\n",
            "Processing question 38/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 39/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 40/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 41/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 42/100 with DeepSeek (CoT Mode)...\n",
            "  -> An error occurred: no healthy upstream. Retrying in 5 seconds...\n",
            "  -> An error occurred: no healthy upstream. Retrying in 5 seconds...\n",
            "  -> API Error after multiple retries: no healthy upstream\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: API_ERROR\n",
            "Processing question 43/100 with DeepSeek (CoT Mode)...\n",
            "  -> An error occurred: no healthy upstream. Retrying in 5 seconds...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 44/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 45/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 46/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 47/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 48/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 49/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 50/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
            "Processing question 51/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 52/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 53/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 54/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 55/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 56/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 57/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 58/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 59/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 60/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 61/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 62/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 63/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 64/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 65/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 66/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 67/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¨\n",
            "Processing question 68/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¯\n",
            "Processing question 69/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 70/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 71/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¯\n",
            "Processing question 72/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 73/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 74/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 75/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 76/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 77/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 78/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 79/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 80/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 81/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 82/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 83/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 84/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 85/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 86/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 87/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¥ | Model's Prediction: Ø¬\n",
            "Processing question 88/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 89/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 90/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 91/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 92/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 93/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 94/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¨\n",
            "Processing question 95/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¨\n",
            "Processing question 96/100 with DeepSeek (CoT Mode)...\n",
            "  -> Warning: Model did not use the 'Final Answer:' format. Using last found Arabic letter.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¶\n",
            "Processing question 97/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 98/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 99/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 100/100 with DeepSeek (CoT Mode)...\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "\n",
            "Successfully generated predictions and saved them to '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek_cot.csv'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_86868cef-3741-464c-a3b0-6aacd67fd0d3\", \"predictions_fitb_choices_deepseek_cot.csv\", 307)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸš€ Starting Evaluation...\n",
            "==================================================\n",
            "Total Questions Attempted: 100\n",
            "API Errors/Failed/Empty: 1\n",
            "Valid Predictions to Evaluate: 99\n",
            "--------------------\n",
            "Correct Predictions: 70 / 99\n",
            "ðŸ“Š Accuracy (on valid responses): 70.71%\n",
            "==================================================\n",
            "âœ… Evaluation Complete.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_y0MyZ87Xndr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z1Jp7yBnXngw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# deep seek no halusination"
      ],
      "metadata": {
        "id": "mKoYDW71Xqg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import groq\n",
        "import time\n",
        "from getpass import getpass\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- Groq API Configuration ---\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "    print(\"âœ… Successfully loaded GROQ_API_KEY from Colab secrets.\")\n",
        "except (ImportError, KeyError):\n",
        "    if 'GROQ_API_KEY' in os.environ:\n",
        "        GROQ_API_KEY = os.environ.get('GROQ_API_KEY')\n",
        "        print(\"âœ… Successfully loaded GROQ_API_KEY from environment variables.\")\n",
        "    else:\n",
        "        print(\"âš ï¸ GROQ_API_KEY not found in Colab secrets or environment variables.\")\n",
        "        GROQ_API_KEY = getpass('Please enter your Groq API key: ')\n",
        "\n",
        "# Initialize the Groq client\n",
        "client = groq.Client(api_key=GROQ_API_KEY)\n",
        "\n",
        "\n",
        "# --- File paths and column names for the MCQ task ---\n",
        "INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/fill-in-the-blank-choices.csv'\n",
        "OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek_cot_validated.csv' # Changed output file name\n",
        "\n",
        "QUESTION_COLUMN = 'Question - Arabic'\n",
        "ANSWER_COLUMN = 'Answer - Arabic'\n",
        "\n",
        "\n",
        "# --- NEW: Helper function to find valid choice letters in the question ---\n",
        "def extract_choice_letters(question_text):\n",
        "    \"\"\"\n",
        "    Extracts the Arabic letters used as choices from the question text.\n",
        "    Looks for patterns like 'Ø£)', 'Ø¨-', 'Ø¬.' etc.\n",
        "    \"\"\"\n",
        "    # This regex finds Arabic letters followed by a parenthesis, dot, or hyphen.\n",
        "    # It returns a list of unique letters found.\n",
        "    letters = re.findall(r'([\\u0621-\\u064A])[.)-]\\s', question_text)\n",
        "    # Return unique letters while preserving order\n",
        "    return sorted(list(set(letters)), key=letters.index)\n",
        "\n",
        "\n",
        "# --- MODIFIED Function to Generate Answers using a Constrained Chain of Thought ---\n",
        "def generate_answer(question, valid_letters):\n",
        "    \"\"\"\n",
        "    Sends a question to the Groq API, instructing it to use Chain of Thought\n",
        "    and respond with a letter from the provided list of valid choices.\n",
        "    \"\"\"\n",
        "    # --- NEW: Dynamic System Prompt ---\n",
        "    # It now dynamically includes the list of valid letters to constrain the model.\n",
        "    system_prompt = f\"\"\"You are an expert medical professional and a meticulous test-taker. Your task is to answer the following multiple-choice medical question.\n",
        "\n",
        "Follow these steps precisely:\n",
        "1.  First, carefully analyze the question and all the provided options.\n",
        "2.  Engage in a step-by-step reasoning process to determine the correct answer. Enclose this entire thought process within `<thinking>` and `</thinking>` tags.\n",
        "3.  After the closing `</thinking>` tag, on a new line, state the final answer clearly.\n",
        "4.  The final answer must be in the format: `Final Answer: [letter]`.\n",
        "5.  **Crucially, the `[letter]` you choose MUST be one of these valid options ONLY: {', '.join(valid_letters)}.** Do not invent any other letters.\n",
        "\n",
        "Example Response Format:\n",
        "<thinking>\n",
        "[Your detailed, step-by-step analysis of the question and options goes here...]\n",
        "</thinking>\n",
        "Final Answer: Ø¨\n",
        "\"\"\"\n",
        "\n",
        "    max_retries = 3\n",
        "    retry_delay = 5 # seconds\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            chat_completion = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": question}\n",
        "                ],\n",
        "                model=\"deepseek-r1-distill-llama-70b\",\n",
        "                temperature=0.0,\n",
        "                max_tokens=1024,\n",
        "            )\n",
        "            raw_response_text = chat_completion.choices[0].message.content.strip()\n",
        "\n",
        "            # --- NEW: Stricter parsing and validation ---\n",
        "            predicted_letter = None\n",
        "            match = re.search(r\"Final Answer:\\s*([\\u0621-\\u064A])\", raw_response_text)\n",
        "\n",
        "            if match:\n",
        "                letter = match.group(1)\n",
        "                # Validate if the extracted letter is one of the allowed choices\n",
        "                if letter in valid_letters:\n",
        "                    predicted_letter = letter\n",
        "                else:\n",
        "                    print(f\"  -> Warning: Model returned a letter '{letter}' not in the valid list {valid_letters}.\")\n",
        "            else:\n",
        "                 # Fallback: find the last valid letter in the whole response\n",
        "                all_letters_in_response = re.findall(r'[\\u0621-\\u064A]', raw_response_text)\n",
        "                for letter in reversed(all_letters_in_response):\n",
        "                    if letter in valid_letters:\n",
        "                        print(f\"  -> Info: Using fallback. Found valid letter '{letter}' in response.\")\n",
        "                        predicted_letter = letter\n",
        "                        break # Stop after finding the last valid one\n",
        "\n",
        "            if predicted_letter:\n",
        "                return predicted_letter\n",
        "            else:\n",
        "                print(f\"  -> Warning: No valid answer letter found in response. Full response: '{raw_response_text}'.\")\n",
        "                return \"INVALID_LETTER\"\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  -> An error occurred: {e}. Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  -> API Error after multiple retries: {e}\")\n",
        "                return \"API_ERROR\"\n",
        "    return \"FAILED_ATTEMPTS\"\n",
        "\n",
        "\n",
        "# --- Function to Evaluate MCQ Accuracy (Slightly modified for new error code) ---\n",
        "def evaluate_mcq_accuracy(predictions, ground_truths):\n",
        "    \"\"\"Calculates and prints the accuracy of the model's predictions.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸš€ Starting Evaluation...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Filter out any kind of error or invalid response for accuracy calculation\n",
        "    valid_indices = [i for i, p in enumerate(predictions) if p not in [\"API_ERROR\", \"FAILED_ATTEMPTS\", \"INVALID_LETTER\", \"\"]]\n",
        "    valid_predictions = [predictions[i] for i in valid_indices]\n",
        "    valid_ground_truths = [ground_truths[i] for i in valid_indices]\n",
        "\n",
        "    if not valid_predictions:\n",
        "        print(\"No valid predictions to evaluate. Check for widespread API or formatting errors.\")\n",
        "        return\n",
        "\n",
        "    accuracy = accuracy_score(valid_ground_truths, valid_predictions)\n",
        "    correct_predictions = sum(p == g for p, g in zip(valid_predictions, valid_ground_truths))\n",
        "    total_valid_predictions = len(valid_predictions)\n",
        "    total_questions = len(ground_truths)\n",
        "    failed_or_invalid = total_questions - total_valid_predictions\n",
        "\n",
        "    print(f\"Total Questions Attempted: {total_questions}\")\n",
        "    print(f\"Errors or Invalid Letters: {failed_or_invalid}\")\n",
        "    print(f\"Valid Predictions to Evaluate: {total_valid_predictions}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Correct Predictions: {correct_predictions} / {total_valid_predictions}\")\n",
        "    print(f\"ðŸ“Š Accuracy (on valid responses): {accuracy * 100:.2f}%\")\n",
        "    print(\"=\"*50 + \"\\nâœ… Evaluation Complete.\\n\" + \"=\"*50)\n",
        "\n",
        "\n",
        "# --- MODIFIED Main Execution Logic ---\n",
        "def main():\n",
        "    \"\"\"Main function to run the constrained MCQ prediction and evaluation.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_CSV, encoding='utf-8')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Error: The file '{INPUT_CSV}' was not found. Please check the path.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ An error occurred while reading the CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    # Basic data validation\n",
        "    df.dropna(subset=[QUESTION_COLUMN, ANSWER_COLUMN], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    ground_truths = [str(ans).strip()[0] for ans in df[ANSWER_COLUMN].tolist()]\n",
        "\n",
        "    # --- NEW: Print all unique ground truth letters found in the answer key ---\n",
        "    unique_ground_truth_letters = sorted(list(set(ground_truths)))\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Found {len(unique_ground_truth_letters)} unique ground truth letters in the answer key:\")\n",
        "    print(f\"âž¡ï¸  {', '.join(unique_ground_truth_letters)}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "\n",
        "    if os.path.exists(OUTPUT_CSV):\n",
        "        print(f\"âœ… Found existing prediction file: '{OUTPUT_CSV}'. Loading for evaluation.\")\n",
        "        predictions_df = pd.read_csv(OUTPUT_CSV, header=None, encoding='utf-8')\n",
        "        # Ensure loaded data is treated as string\n",
        "        predictions = predictions_df[0].astype(str).apply(lambda x: 'nan' if pd.isna(x) else x).tolist()\n",
        "    else:\n",
        "        print(f\"â„¹ï¸ '{OUTPUT_CSV}' not found. Starting prediction generation...\")\n",
        "        predictions = []\n",
        "        total_questions = len(df)\n",
        "        for index, row in df.iterrows():\n",
        "            question = row[QUESTION_COLUMN]\n",
        "            ground_truth_letter = ground_truths[index]\n",
        "\n",
        "            # 1. Extract valid letters from the question\n",
        "            valid_letters = extract_choice_letters(question)\n",
        "            if not valid_letters:\n",
        "                print(f\"Processing question {index + 1}/{total_questions}...\")\n",
        "                print(f\"  -> âš ï¸ Warning: Could not extract any choice letters from the question. Skipping.\")\n",
        "                predictions.append(\"INVALID_QUESTION\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing question {index + 1}/{total_questions}... (Valid choices: {', '.join(valid_letters)})\")\n",
        "\n",
        "            # 2. Generate answer with the context of valid letters\n",
        "            answer_letter = generate_answer(question, valid_letters)\n",
        "            predictions.append(answer_letter)\n",
        "            print(f\"  -> Ground Truth: {ground_truth_letter} | Model's Prediction: {answer_letter}\")\n",
        "\n",
        "            time.sleep(1) # Small delay between API calls\n",
        "\n",
        "        predictions_df = pd.DataFrame(predictions)\n",
        "        predictions_df.to_csv(OUTPUT_CSV, header=False, index=False, encoding='utf-8')\n",
        "        print(f\"\\nâœ… Successfully generated predictions and saved them to '{OUTPUT_CSV}'.\")\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(OUTPUT_CSV)\n",
        "        except (ImportError, NameError):\n",
        "            print(f\"â„¹ï¸ To download '{OUTPUT_CSV}', please use the file browser.\")\n",
        "\n",
        "    evaluate_mcq_accuracy(predictions, ground_truths)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i1mspE7JXnjb",
        "outputId": "cb856153-711f-40e7-f1d4-b55d0dce82c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Successfully loaded GROQ_API_KEY from Colab secrets.\n",
            "==================================================\n",
            "Found 5 unique ground truth letters in the answer key:\n",
            "âž¡ï¸  Ø£, Ø¥, Ø¨, Ø¬, Ø¯\n",
            "==================================================\n",
            "â„¹ï¸ '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek_cot_validated.csv' not found. Starting prediction generation...\n",
            "Processing question 1/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 2/100... (Valid choices: Ù‡, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¨\n",
            "Processing question 3/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 4/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 5/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 6/100... (Valid choices: Ù„, Ø¶, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 7/100... (Valid choices: Ù…, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 8/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 9/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Info: Using fallback. Found valid letter 'Ø¨' in response.\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¨\n",
            "Processing question 10/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Info: Using fallback. Found valid letter 'Ø£' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 11/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 12/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 13/100... (Valid choices: Ø¡, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 14/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Info: Using fallback. Found valid letter 'Ø¨' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 15/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 16/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 17/100... (Valid choices: Ø¹, Ø£, Ø¨, Ø¬, Ù€)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 18/100... (Valid choices: Ùƒ, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 19/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Info: Using fallback. Found valid letter 'Ø¨' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 20/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Info: Using fallback. Found valid letter 'Ø©' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø©\n",
            "Processing question 21/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¨\n",
            "Processing question 22/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯, Ù€)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 23/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 24/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 25/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 26/100... (Valid choices: Ù‰, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 27/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 28/100... (Valid choices: Ø¨, Ø£, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¨\n",
            "Processing question 29/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 30/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 31/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 32/100... (Valid choices: Ø¯, Ø£, Ø¨, Ø¬)\n",
            "  -> Warning: No valid answer letter found in response. Full response: '<think>\n",
            "Okay, so I've got this medical question here, and I need to figure it out step by step. Let me read it again to make sure I understand what's being asked.\n",
            "\n",
            "The question is about a hormone whose secretion increases after birth, and this hormone is essential for maintaining the newborn's body temperature. The options are cortisol, TSH, T4, and T3.\n",
            "\n",
            "Hmm, I remember that after birth, newborns have to adapt to the outside environment, which is quite different from the womb. One of the main challenges is regulating their body temperature. I think the thyroid hormones play a role here because they're involved in metabolism, which affects heat production.\n",
            "\n",
            "Wait, so the thyroid gland produces T3 and T4. TSH is the hormone that stimulates the thyroid to release these hormones. But which one is more active right after birth?\n",
            "\n",
            "I recall that T3 is the more active form of thyroid hormone. It's responsible for increasing metabolic rate, which in turn generates body heat. So after birth, the baby's thyroid starts working more to produce T3 to help maintain their body temperature.\n",
            "\n",
            "But wait, isn't T4 the main hormone released by the thyroid, and then it's converted to T3 in the tissues? So maybe the question is referring to the increase in T4. Or does it specifically refer to T3?\n",
            "\n",
            "I think the question is about the hormone that's crucial for heat production. T3 is more potent, so maybe the increase is in T3. Alternatively, maybe the question is pointing towards TSH, but TSH is the stimulating hormone, not the one directly causing the effect.\n",
            "\n",
            "Wait, no, the question says the hormone itself is increased. So it's either T3 or T4. I remember that in newborns, especially premature ones, the levels of T4 might be lower, but T3 might be higher because of the conversion from T4 to T3. Or maybe the surge is in TSH right after birth, which then stimulates the thyroid to release more T4 and T3.\n",
            "\n",
            "Wait, I'm getting a bit confused. Let me think again. After birth, the baby's thyroid function increases to help with thermogenesis. The main hormones involved are T3 and T4. TSH is released by the pituitary gland to stimulate the thyroid.\n",
            "\n",
            "So the question is asking which hormone's secretion increases. If TSH increases, that would cause the thyroid to produce more T3 and T4. But the question is about the hormone that's directly involved in maintaining body temperature.\n",
            "\n",
            "Alternatively, maybe the correct answer is TSH because it's the hormone that's increased, which then leads to more thyroid hormones. But I'm not sure. I think the direct effect on metabolism and heat is from T3 and T4.\n",
            "\n",
            "Wait, another angle: the question says \"after birth, there's an increase in the secretion of hormone X, which is necessary for maintaining the newborn's body temperature.\" So the hormone X is the one that's increased, not the one causing the increase.\n",
            "\n",
            "So if the baby's TSH increases, that would cause the thyroid to produce more T3 and T4. But the hormone that's directly involved in heat production is T3 and T4.\n",
            "\n",
            "I think the correct answer is TSH because it's the hormone that's secreted in higher amounts after birth to stimulate the thyroid. But wait, no, the question is about the hormone that's necessary for maintaining body temperature, which would be the thyroid hormones themselves.\n",
            "\n",
            "Wait, I'm getting stuck. Let me try to remember. I think that after birth, the levels of TSH surge, which then causes the thyroid to release more T4 and T3. But the hormone that's directly responsible for the metabolic rate and heat is T3.\n",
            "\n",
            "Alternatively, maybe the question is referring to T4 because it's the main hormone produced by the thyroid. But I'm not entirely sure.\n",
            "\n",
            "Wait, another thought: in adults, T3 is the active form, but in newborns, the conversion from T4 to T3 is crucial. So maybe the increase is in T4, which is then converted to T3.\n",
            "\n",
            "But I'm not certain. I think the correct answer is TSH because it's the hormone that increases to stimulate the thyroid. But no, the question is about the hormone that's necessary for body temperature, which would be the thyroid hormones.\n",
            "\n",
            "Wait, I'm overcomplicating this. Let me think about the options again. The options are cortisol, TSH, T4, and T3.\n",
            "\n",
            "Cortisol is a stress hormone, but it's not primarily involved in thermogenesis. TSH stimulates the thyroid. T4 and T3 are the thyroid hormones.\n",
            "\n",
            "So the correct answer is either T4 or T3. Now, which one is it?\n",
            "\n",
            "I remember that T3 is the more active form, so maybe the increase is in T3. But I also recall that in the first few days of life, T4 levels might be lower, but T3'.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: INVALID_LETTER\n",
            "Processing question 33/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 34/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 35/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 36/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Info: Using fallback. Found valid letter 'Ø¬' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
            "Processing question 37/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 38/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 39/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 40/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 41/100... (Valid choices: Ù…, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Info: Using fallback. Found valid letter 'Ø£' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 42/100... (Valid choices: ÙŠ, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 43/100... (Valid choices: Ø³, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 44/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 45/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 46/100... (Valid choices: Ø¡, Ù…, Ø£, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 47/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 48/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 49/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 50/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 51/100... (Valid choices: Ø², Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 52/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 53/100... (Valid choices: Ø¡, ÙŠ, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 54/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 55/100... (Valid choices: Ù…, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 56/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 57/100... (Valid choices: ÙŠ, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 58/100... (Valid choices: Ù„, Ù†, ÙŠ, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Info: Using fallback. Found valid letter 'Ù„' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ù„\n",
            "Processing question 59/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 60/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 61/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 62/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 63/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 64/100... (Valid choices: Ø², Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 65/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 66/100... (Valid choices: Ù†, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 67/100... (Valid choices: Ù†, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¨\n",
            "Processing question 68/100... (Valid choices: Ù, Ø¯, Ø£, Ø¨, Ø¬)\n",
            "  -> Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¯\n",
            "Processing question 69/100... (Valid choices: ÙŠ, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 70/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 71/100... (Valid choices: Ù‚, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¯\n",
            "Processing question 72/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 73/100... (Valid choices: Ù…, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 74/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 75/100... (Valid choices: Ù…, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 76/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 77/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 78/100... (Valid choices: ÙŠ, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 79/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 80/100... (Valid choices: ÙŠ, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 81/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 82/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 83/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 84/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 85/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 86/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 87/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¥ | Model's Prediction: Ø¬\n",
            "Processing question 88/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 89/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 90/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 91/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 92/100... (Valid choices: Ù…, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 93/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 94/100... (Valid choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 95/100... (Valid choices: Ù„, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Info: Using fallback. Found valid letter 'Ù„' in response.\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ù„\n",
            "Processing question 96/100... (Valid choices: Øª, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 97/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 98/100... (Valid choices: ÙŠ, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 99/100... (Valid choices: Ø©, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 100/100... (Valid choices: Ø§, Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "\n",
            "âœ… Successfully generated predictions and saved them to '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek_cot_validated.csv'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4d370509-7d17-4e57-8f31-6ca2b166a319\", \"predictions_fitb_choices_deepseek_cot_validated.csv\", 312)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸš€ Starting Evaluation...\n",
            "==================================================\n",
            "Total Questions Attempted: 100\n",
            "Errors or Invalid Letters: 1\n",
            "Valid Predictions to Evaluate: 99\n",
            "--------------------\n",
            "Correct Predictions: 76 / 99\n",
            "ðŸ“Š Accuracy (on valid responses): 76.77%\n",
            "==================================================\n",
            "âœ… Evaluation Complete.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1NAmD171drDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# normlized letters"
      ],
      "metadata": {
        "id": "hg6qLSfPdrF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import groq\n",
        "import time\n",
        "from getpass import getpass\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- Groq API Configuration ---\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "    print(\"âœ… Successfully loaded GROQ_API_KEY from Colab secrets.\")\n",
        "except (ImportError, KeyError):\n",
        "    if 'GROQ_API_KEY' in os.environ:\n",
        "        GROQ_API_KEY = os.environ.get('GROQ_API_KEY')\n",
        "        print(\"âœ… Successfully loaded GROQ_API_KEY from environment variables.\")\n",
        "    else:\n",
        "        print(\"âš ï¸ GROQ_API_KEY not found in Colab secrets or environment variables.\")\n",
        "        GROQ_API_KEY = getpass('Please enter your Groq API key: ')\n",
        "\n",
        "# Initialize the Groq client\n",
        "client = groq.Client(api_key=GROQ_API_KEY)\n",
        "\n",
        "\n",
        "# --- File paths and configuration ---\n",
        "INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/fill-in-the-blank-choices.csv'\n",
        "OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek_cot_normalized.csv'\n",
        "\n",
        "QUESTION_COLUMN = 'Question - Arabic'\n",
        "ANSWER_COLUMN = 'Answer - Arabic'\n",
        "\n",
        "# --- UPDATED: Whitelist including all letter variants before normalization ---\n",
        "VALID_MCQ_LETTERS = {'Ø£', 'Ø¥', 'Ø§', 'Ø¨', 'Ø¬', 'Ø¯', 'Ù‡', 'Ø©', 'Ùˆ', 'Ø²'}\n",
        "\n",
        "# --- NEW: Function to normalize Arabic letters ---\n",
        "def normalize_arabic_letter(char):\n",
        "    \"\"\"\n",
        "    Normalizes different forms of Arabic letters to a canonical form.\n",
        "    - All forms of Alef (Ø£, Ø¥, Ø§) are converted to 'Ø£'.\n",
        "    - Taa Marbutah (Ø©) is converted to Haa (Ù‡).\n",
        "    \"\"\"\n",
        "    if char in ['Ø¥', 'Ø§']:\n",
        "        return 'Ø£'\n",
        "    if char == 'Ø©':\n",
        "        return 'Ù‡'\n",
        "    return char\n",
        "\n",
        "# --- Helper function to find and normalize valid choice letters ---\n",
        "def extract_choice_letters(question_text):\n",
        "    \"\"\"\n",
        "    Extracts, normalizes, and de-duplicates Arabic choice letters from the question.\n",
        "    \"\"\"\n",
        "    pattern = r'(?:^|\\n)\\s*([\\u0621-\\u064A])[.)-]\\s*'\n",
        "    letters = re.findall(pattern, question_text)\n",
        "\n",
        "    # First, filter against the whitelist\n",
        "    validated_letters = [char for char in letters if char in VALID_MCQ_LETTERS]\n",
        "\n",
        "    # Second, normalize each validated letter\n",
        "    normalized_letters = [normalize_arabic_letter(char) for char in validated_letters]\n",
        "\n",
        "    # Return a unique, sorted list of the final, normalized letters\n",
        "    return sorted(list(set(normalized_letters)))\n",
        "\n",
        "\n",
        "# --- Function to Generate Answers ---\n",
        "def generate_answer(question, valid_letters):\n",
        "    \"\"\"\n",
        "    Sends a question to the Groq API, instructing it to use Chain of Thought\n",
        "    and respond with a letter from the provided list of valid (and normalized) choices.\n",
        "    \"\"\"\n",
        "    system_prompt = f\"\"\"You are an expert medical professional and a meticulous test-taker. Your task is to answer the following multiple-choice medical question.\n",
        "\n",
        "Follow these steps precisely:\n",
        "1.  First, carefully analyze the question and all the provided options.\n",
        "2.  Engage in a step-by-step reasoning process to determine the correct answer. Your entire detailed analysis MUST be enclosed within `<thinking>` and `</thinking>` tags.\n",
        "3.  After the closing `</thinking>` tag, you MUST provide the final answer on a new line.\n",
        "4.  This final line must ONLY contain the text in the format: `Final Answer: [letter]`.\n",
        "5.  **CRITICAL:** The `[letter]` you choose **MUST** be one of these valid options and nothing else: **{', '.join(valid_letters)}**.\n",
        "\n",
        "There should be absolutely no other text outside the `<thinking>` block except for the 'Final Answer:' line.\n",
        "\"\"\"\n",
        "    max_retries = 3\n",
        "    retry_delay = 5\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            chat_completion = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": question}\n",
        "                ],\n",
        "                model=\"deepseek-r1-distill-llama-70b\",\n",
        "                temperature=0.0,\n",
        "                max_tokens=1024,\n",
        "            )\n",
        "            raw_response_text = chat_completion.choices[0].message.content.strip()\n",
        "            predicted_letter = None\n",
        "            match = re.search(r\"Final Answer:\\s*([\\u0621-\\u064A])\", raw_response_text)\n",
        "\n",
        "            if match:\n",
        "                # Normalize the predicted letter before validation\n",
        "                letter = normalize_arabic_letter(match.group(1))\n",
        "                if letter in valid_letters:\n",
        "                    predicted_letter = letter\n",
        "                else:\n",
        "                    print(f\"  -> âš ï¸ Warning: Model returned letter '{match.group(1)}' which normalizes to '{letter}' and is not in the valid list {valid_letters}.\")\n",
        "            else:\n",
        "                all_letters_in_response = re.findall(r'[\\u0621-\\u064A]', raw_response_text)\n",
        "                for letter_char in reversed(all_letters_in_response):\n",
        "                    # Normalize the letter from fallback before validation\n",
        "                    letter = normalize_arabic_letter(letter_char)\n",
        "                    if letter in valid_letters:\n",
        "                        print(f\"  -> â„¹ï¸ Info: Using fallback. Found valid letter '{letter}' in response.\")\n",
        "                        predicted_letter = letter\n",
        "                        break\n",
        "\n",
        "            return predicted_letter if predicted_letter else \"INVALID_LETTER\"\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  -> â€¼ï¸ An error occurred: {e}. Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"  -> â€¼ï¸ API Error after multiple retries: {e}\")\n",
        "                return \"API_ERROR\"\n",
        "    return \"FAILED_ATTEMPTS\"\n",
        "\n",
        "# --- Evaluation Function (Unchanged) ---\n",
        "def evaluate_mcq_accuracy(predictions, ground_truths):\n",
        "    \"\"\"Calculates and prints the accuracy of the model's predictions.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸš€ Starting Evaluation...\")\n",
        "    print(\"=\"*50)\n",
        "    valid_indices = [i for i, p in enumerate(predictions) if p not in [\"API_ERROR\", \"FAILED_ATTEMPTS\", \"INVALID_LETTER\", \"\", \"INVALID_QUESTION\"]]\n",
        "    valid_predictions = [predictions[i] for i in valid_indices]\n",
        "    valid_ground_truths = [ground_truths[i] for i in valid_indices]\n",
        "\n",
        "    if not valid_predictions:\n",
        "        print(\"No valid predictions to evaluate. Check for widespread API or formatting errors.\")\n",
        "        return\n",
        "\n",
        "    accuracy = accuracy_score(valid_ground_truths, valid_predictions)\n",
        "    correct_predictions = sum(p == g for p, g in zip(valid_predictions, valid_ground_truths))\n",
        "    total_valid_predictions = len(valid_predictions)\n",
        "    total_questions = len(ground_truths)\n",
        "    failed_or_invalid = total_questions - total_valid_predictions\n",
        "\n",
        "    print(f\"Total Questions Attempted: {total_questions}\")\n",
        "    print(f\"Errors or Invalid Responses: {failed_or_invalid}\")\n",
        "    print(f\"Valid Predictions to Evaluate: {total_valid_predictions}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Correct Predictions: {correct_predictions} / {total_valid_predictions}\")\n",
        "    print(f\"ðŸ“Š Accuracy (on valid responses): {accuracy * 100:.2f}%\")\n",
        "    print(\"=\"*50 + \"\\nâœ… Evaluation Complete.\\n\" + \"=\"*50)\n",
        "\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "def main():\n",
        "    \"\"\"Main function to run the constrained and normalized MCQ prediction and evaluation.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_CSV, encoding='utf-8')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Error: The file '{INPUT_CSV}' was not found. Please check the path.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ An error occurred while reading the CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    df.dropna(subset=[QUESTION_COLUMN, ANSWER_COLUMN], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # --- UPDATED: Normalize ground truths at the start ---\n",
        "    ground_truths = [normalize_arabic_letter(str(ans).strip()[0]) for ans in df[ANSWER_COLUMN].tolist()]\n",
        "\n",
        "    unique_ground_truth_letters = sorted(list(set(ground_truths)))\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Found {len(unique_ground_truth_letters)} unique ground truth letters after normalization:\")\n",
        "    print(f\"âž¡ï¸  {', '.join(unique_ground_truth_letters)}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if os.path.exists(OUTPUT_CSV):\n",
        "        print(f\"âœ… Found existing prediction file: '{OUTPUT_CSV}'. Loading for evaluation.\")\n",
        "        predictions_df = pd.read_csv(OUTPUT_CSV, header=None, encoding='utf-8')\n",
        "        predictions = predictions_df[0].astype(str).apply(lambda x: 'nan' if pd.isna(x) else x).tolist()\n",
        "    else:\n",
        "        print(f\"â„¹ï¸ '{OUTPUT_CSV}' not found. Starting prediction generation...\")\n",
        "        predictions = []\n",
        "        total_questions = len(df)\n",
        "        for index, row in df.iterrows():\n",
        "            question = row[QUESTION_COLUMN]\n",
        "            ground_truth_letter = ground_truths[index]\n",
        "\n",
        "            valid_letters = extract_choice_letters(question)\n",
        "            if not valid_letters:\n",
        "                print(f\"Processing question {index + 1}/{total_questions}...\")\n",
        "                print(f\"  -> âš ï¸ Warning: Could not extract any valid choice letters from the question. Skipping.\")\n",
        "                predictions.append(\"INVALID_QUESTION\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing question {index + 1}/{total_questions}... (Normalized choices: {', '.join(valid_letters)})\")\n",
        "\n",
        "            answer_letter = generate_answer(question, valid_letters)\n",
        "            predictions.append(answer_letter)\n",
        "            print(f\"  -> Ground Truth: {ground_truth_letter} | Model's Prediction: {answer_letter}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "        predictions_df = pd.DataFrame(predictions)\n",
        "        predictions_df.to_csv(OUTPUT_CSV, header=False, index=False, encoding='utf-8')\n",
        "        print(f\"\\nâœ… Successfully generated predictions and saved them to '{OUTPUT_CSV}'.\")\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(OUTPUT_CSV)\n",
        "        except (ImportError, NameError):\n",
        "            print(f\"â„¹ï¸ To download '{OUTPUT_CSV}', please use the file browser.\")\n",
        "\n",
        "    evaluate_mcq_accuracy(predictions, ground_truths)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7jS5EgAMds5N",
        "outputId": "0af714d5-f9e6-4afd-c00e-86ddc283248f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Successfully loaded GROQ_API_KEY from Colab secrets.\n",
            "==================================================\n",
            "Found 4 unique ground truth letters after normalization:\n",
            "âž¡ï¸  Ø£, Ø¨, Ø¬, Ø¯\n",
            "==================================================\n",
            "â„¹ï¸ '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek_cot_normalized.csv' not found. Starting prediction generation...\n",
            "Processing question 1/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 2/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 3/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 4/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 5/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 6/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 7/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø£' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 8/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 9/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¯\n",
            "Processing question 10/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 11/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 12/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 13/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 14/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¯\n",
            "Processing question 15/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 16/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¨' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 17/100... (Normalized choices: Ø£, Ø¨, Ø¬)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 18/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¬' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¬\n",
            "Processing question 19/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø£' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 20/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¨' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 21/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
            "Processing question 22/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 23/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 24/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 25/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 26/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø£' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 27/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 28/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¨\n",
            "Processing question 29/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 30/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 31/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 32/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: INVALID_LETTER\n",
            "Processing question 33/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø£' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 34/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 35/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 36/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¬' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
            "Processing question 37/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø£' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 38/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 39/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 40/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 41/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¯\n",
            "Processing question 42/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 43/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 44/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 45/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø£' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 46/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 47/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 48/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 49/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 50/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¨\n",
            "Processing question 51/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 52/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 53/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 54/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 55/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 56/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 57/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 58/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¨' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
            "Processing question 59/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¨' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 60/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø£' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 61/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 62/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 63/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø£' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 64/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 65/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 66/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 67/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 68/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¯\n",
            "Processing question 69/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 70/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 71/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¯\n",
            "Processing question 72/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¯\n",
            "Processing question 73/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 74/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 75/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 76/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 77/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 78/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 79/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 80/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 81/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 82/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 83/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 84/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 85/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 86/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¨' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 87/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
            "Processing question 88/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 89/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 90/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 91/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
            "Processing question 92/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 93/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø£' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 94/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¯\n",
            "Processing question 95/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
            "Processing question 96/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø¯' in response.\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
            "Processing question 97/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "Processing question 98/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
            "Processing question 99/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> â„¹ï¸ Info: Using fallback. Found valid letter 'Ø£' in response.\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
            "Processing question 100/100... (Normalized choices: Ø£, Ø¨, Ø¬, Ø¯)\n",
            "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
            "\n",
            "âœ… Successfully generated predictions and saved them to '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek_cot_normalized.csv'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2da2db5c-7b27-4f62-a1df-f09a20bed135\", \"predictions_fitb_choices_deepseek_cot_normalized.csv\", 312)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸš€ Starting Evaluation...\n",
            "==================================================\n",
            "Total Questions Attempted: 100\n",
            "Errors or Invalid Responses: 1\n",
            "Valid Predictions to Evaluate: 99\n",
            "--------------------\n",
            "Correct Predictions: 73 / 99\n",
            "ðŸ“Š Accuracy (on valid responses): 73.74%\n",
            "==================================================\n",
            "âœ… Evaluation Complete.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6eMjlEqsds7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QPgKwPOAXnmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoNaAO9tV_zB",
        "outputId": "c46417e6-ec76-4a40-ff75-1020d2093082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pWKfKpOfUbYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "asBIJnsEUbcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MMXd_qZiUbgd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}