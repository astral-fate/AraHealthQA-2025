import os
import re
import pandas as pd
import time
# Import the userdata module for Google Colab secrets
from google.colab import userdata
from sklearn.metrics import accuracy_score
# Import libraries for local Hugging Face model inference
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login

# --- File paths and column names (Updated for your data) ---
INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/t2t1/data/fill-in-the-blank-choices.csv'
OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/t2t1/data/predictions_fill-in-the-blank_OpenBioLLM_8B.csv'

# --- Column names (Set for your data) ---
QUESTION_COLUMN = 'Question - Arabic'
ANSWER_COLUMN = 'Answer - Arabic'
# --- NEW: Added a column name for our predictions to manage state ---
PREDICTED_COLUMN = 'Predicted_Answer'


# --- Chain of Thought & Few-Shot Prompting Configuration ---
# This system prompt is now in Arabic to guide the model more effectively.
SYSTEM_PROMPT = """Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø·Ø¨ÙŠ ÙˆÙ…Ø³Ø§Ø¹Ø¯ Ø§Ù…ØªØ­Ø§Ù†Ø§Øª Ø¯Ù‚ÙŠÙ‚. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø­Ù„ Ø³Ø¤Ø§Ù„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø³ØªÙ‚ÙˆÙ… Ø¨Ø¹Ù…Ù„ÙŠØ© ØªÙÙƒÙŠØ± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©. Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø·Ø¨ÙŠØŒ ÙˆØªÙ‚ÙŠÙŠÙ… ÙƒÙ„ Ø®ÙŠØ§Ø± (Ø£, Ø¨, Ø¬, Ø¯, Ù‡Ù€)ØŒ ÙˆØ´Ø±Ø­ Ø£Ø³Ø¨Ø§Ø¨ Ø§Ø®ØªÙŠØ§Ø±Ùƒ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©.
Ø«Ø§Ù†ÙŠØ§Ù‹ØŒ Ø¨Ø¹Ø¯ Ø´Ø±Ø­ Ø£Ø³Ø¨Ø§Ø¨ÙƒØŒ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙÙŠ Ø³Ø·Ø± Ø¬Ø¯ÙŠØ¯ Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ:
Final Answer: [Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„Ø®ÙŠØ§Ø± Ø§Ù„ØµØ­ÙŠØ­]

Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù…ÙƒÙˆÙ†Ø© Ù…Ù† Ø®Ø·ÙˆØªÙŠÙ† Ø¥Ù„Ø²Ø§Ù…ÙŠØ©. ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
"""

FEW_SHOT_EXAMPLES = [
    {
        "role": "user",
        "content": """Ø§Ù…Ù„Ø£ Ø§Ù„ÙØ±Ø§ØºØ§Øª ÙÙŠ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©:
ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ù†ØµØ¨Ø§Ø¨ Ø§Ù„Ø¬Ù†Ø¨ÙŠØŒ ÙŠØ´ÙŠØ± Ø§Ù†Ø®ÙØ§Ø¶ Ø£Ùˆ ØºÙŠØ§Ø¨ Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ù…ØµØ§Ø¨ Ø¥Ù„Ù‰ ____ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠÙ†ØªØ¬ Ø¹Ù† ____.
Ø£. ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø³ÙˆØ§Ø¦Ù„Ø› Ø¹Ø²Ù„ Ø§Ù„ØµÙˆØª
Ø¨. Ø§Ø­ØªØ¨Ø§Ø³ Ø§Ù„Ù‡ÙˆØ§Ø¡Ø› Ø§Ù†Ù‡ÙŠØ§Ø± Ø§Ù„Ø­ÙˆÙŠØµÙ„Ø§Øª Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©
Ø¬. Ø§Ù„ØªÙ„ÙŠÙØ› Ø§Ù†Ø®ÙØ§Ø¶ Ù…Ø±ÙˆÙ†Ø© Ø§Ù„Ø±Ø¦Ø©
Ø¯. Ù†Ù…Ùˆ Ø§Ù„ÙˆØ±Ù…Ø› Ø§Ù†Ø³Ø¯Ø§Ø¯ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©"""
    },
    {
        "role": "assistant",
        "content": """**Ø§Ù„ØªÙÙƒÙŠØ± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:**
1.  **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„**: ÙŠØ³Ø£Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø¯Ù„Ø§Ù„Ø© Ø§Ù†Ø®ÙØ§Ø¶ Ø£Ùˆ ØºÙŠØ§Ø¨ "Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ" (tactile fremitus) ÙÙŠ Ø­Ø§Ù„Ø© "Ø§Ù„Ø§Ù†ØµØ¨Ø§Ø¨ Ø§Ù„Ø¬Ù†Ø¨ÙŠ" (pleural effusion) ÙˆØ¹Ù† Ø§Ù„Ø¢Ù„ÙŠØ© Ø§Ù„Ù…Ø³Ø¨Ø¨Ø© Ù„Ø°Ù„Ùƒ. Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ Ù‡Ùˆ Ø§Ù„Ø§Ù‡ØªØ²Ø§Ø² Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ù‡ Ø¹Ù„Ù‰ Ø¬Ø¯Ø§Ø± Ø§Ù„ØµØ¯Ø± Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ÙƒÙ„Ø§Ù….
2.  **ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª**:
    * **Ø£. ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø³ÙˆØ§Ø¦Ù„Ø› Ø¹Ø²Ù„ Ø§Ù„ØµÙˆØª**: Ø§Ù„Ø§Ù†ØµØ¨Ø§Ø¨ Ø§Ù„Ø¬Ù†Ø¨ÙŠ Ù‡Ùˆ Ø¨Ø§Ù„ÙØ¹Ù„ ØªØ±Ø§ÙƒÙ… Ù„Ù„Ø³ÙˆØ§Ø¦Ù„ ÙÙŠ Ø§Ù„ØºØ´Ø§Ø¡ Ø§Ù„Ø¬Ù†Ø¨ÙŠ. Ù‡Ø°Ø§ Ø§Ù„Ø³Ø§Ø¦Ù„ ÙŠØ¹Ù…Ù„ ÙƒØ¹Ø§Ø²Ù„ØŒ Ù…Ù…Ø§ ÙŠÙ…Ù†Ø¹ Ø§Ù†ØªÙ‚Ø§Ù„ Ø§Ù‡ØªØ²Ø§Ø²Ø§Øª Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„Ø±Ø¦Ø© Ø¥Ù„Ù‰ Ø¬Ø¯Ø§Ø± Ø§Ù„ØµØ¯Ø±. Ù‡Ø°Ø§ ÙŠØªØ·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ù‹Ø§ Ù…Ø¹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ø© Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ.
    * **Ø¨. Ø§Ø­ØªØ¨Ø§Ø³ Ø§Ù„Ù‡ÙˆØ§Ø¡Ø› Ø§Ù†Ù‡ÙŠØ§Ø± Ø§Ù„Ø­ÙˆÙŠØµÙ„Ø§Øª Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©**: Ù‡Ø°Ø§ ÙŠØµÙ Ø­Ø§Ù„Ø© Ø§Ø³ØªØ±ÙˆØ§Ø­ Ø§Ù„ØµØ¯Ø± (pneumothorax) Ø£Ùˆ Ø§Ù†Ø®Ù…Ø§Øµ Ø§Ù„Ø±Ø¦Ø© (atelectasis)ØŒ ÙˆØ§Ù„ØªÙŠ Ù„Ù‡Ø§ Ù…ÙˆØ¬ÙˆØ¯Ø§Øª ÙÙŠØ²ÙŠØ§Ø¦ÙŠØ© Ù…Ø®ØªÙ„ÙØ©.
    * **Ø¬. Ø§Ù„ØªÙ„ÙŠÙØ› Ø§Ù†Ø®ÙØ§Ø¶ Ù…Ø±ÙˆÙ†Ø© Ø§Ù„Ø±Ø¦Ø©**: Ø§Ù„ØªÙ„ÙŠÙ Ø§Ù„Ø±Ø¦ÙˆÙŠ (Pulmonary fibrosis) ÙŠØ²ÙŠØ¯ Ù…Ù† ÙƒØ«Ø§ÙØ© Ø£Ù†Ø³Ø¬Ø© Ø§Ù„Ø±Ø¦Ø©ØŒ Ù…Ù…Ø§ Ù‚Ø¯ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠØŒ ÙˆÙ„ÙŠØ³ Ø§Ù†Ø®ÙØ§Ø¶Ù‡.
    * **Ø¯. Ù†Ù…Ùˆ Ø§Ù„ÙˆØ±Ù…Ø› Ø§Ù†Ø³Ø¯Ø§Ø¯ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©**: Ù‚Ø¯ ÙŠØ³Ø¨Ø¨ Ø§Ù„ÙˆØ±Ù… Ø§Ù†ØµØ¨Ø§Ø¨Ù‹Ø§ Ø¬Ù†Ø¨ÙŠÙ‹Ø§ØŒ Ù„ÙƒÙ† Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù„Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„Ø±Ø¬ÙØ§Ù† ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø© Ù‡Ùˆ Ø§Ù„Ø³Ø§Ø¦Ù„ Ù†ÙØ³Ù‡ Ø§Ù„Ø°ÙŠ ÙŠØ¹Ø²Ù„ Ø§Ù„ØµÙˆØª. Ø§Ù„Ø®ÙŠØ§Ø± "Ø£" ÙŠØµÙ Ø§Ù„Ø¢Ù„ÙŠØ© Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¦ÙŠØ© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„.
3.  **Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬**: Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© Ù‡Ùˆ Ø£Ù† ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø³ÙˆØ§Ø¦Ù„ Ù‡Ùˆ Ù…Ø§ ÙŠØ³Ø¨Ø¨ Ø¹Ø²Ù„ Ø§Ù„ØµÙˆØªØŒ Ù…Ù…Ø§ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ.

Final Answer: Ø£"""
    }
]


# --- Function to Generate Answers using Local Hugging Face Model ---
def generate_answer(question, pipeline):
    """
    Generates an answer using a local transformers pipeline.
    """
    # Construct the message list for the chat template
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT}
    ]
    messages.extend(FEW_SHOT_EXAMPLES)
    final_instruction = "Ø§Ù„Ø¢Ù†ØŒ Ø§ØªØ¨Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¨Ø¯Ù‚Ø©. Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„ØªÙÙƒÙŠØ± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© Ø«Ù… Ø§Ø®ØªØªÙ… Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ù€ 'Final Answer: ' Ù…ØªØ¨ÙˆØ¹Ù‹Ø§ Ø¨Ø§Ù„Ø­Ø±Ù Ø§Ù„ØµØ­ÙŠØ­ ÙÙ‚Ø·."
    messages.append({"role": "user", "content": f"{question}\n\n{final_instruction}"})

    try:
        # Apply the chat template to create the full prompt
        prompt = pipeline.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # Define terminators to stop generation cleanly
        terminators = [
            pipeline.tokenizer.eos_token_id,
            pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]

        # Generate text using the pipeline
        outputs = pipeline(
            prompt,
            max_new_tokens=1024,
            eos_token_id=terminators,
            do_sample=False, # Use greedy decoding for more deterministic results
        )

        # Extract the generated text after the prompt
        response_text = outputs[0]["generated_text"][len(prompt):]

        # --- Parsing Logic ---
        # 1. Primary method: Look for the Arabic format
        match = re.search(r"Final Answer:\s*([\u0621-\u064A])", response_text, re.IGNORECASE)
        if match:
            return match.group(1)

        # 2. Heuristic method: Look for Arabic conclusive phrases
        conclusive_phrases = [
            r"Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„ØµØ­ÙŠØ­ Ù‡Ùˆ\s*([\u0621-\u064A])", r"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© Ù‡ÙŠ\s*([\u0621-\u064A])",
            r"Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù‡Ùˆ Ø£Ù† Ø§Ù„Ø®ÙŠØ§Ø±\s*([\u0621-\u064A])", r"Ø§Ù„Ø®ÙŠØ§Ø±\s*([\u0621-\u064A])\s*Ù‡Ùˆ Ø§Ù„ØµØ­ÙŠØ­",
        ]
        for phrase in conclusive_phrases:
            match = re.search(phrase, response_text)
            if match:
                print(f"  -> Found answer using Arabic conclusive phrase heuristic.")
                return match.group(1)

        # 3. Fallback method: Look for English letters and translate them
        english_match = re.search(r"The correct answer is\s+([A-E])", response_text, re.IGNORECASE)
        if not english_match:
            english_match = re.search(r"Final Answer:\s*([A-E])", response_text, re.IGNORECASE)

        if english_match:
            english_letter = english_match.group(1).upper()
            translation_map = {'A': 'Ø£', 'B': 'Ø¨', 'C': 'Ø¬', 'D': 'Ø¯', 'E': 'Ù‡'}
            arabic_letter = translation_map.get(english_letter)
            if arabic_letter:
                print(f"  -> Found English letter '{english_letter}', translated to '{arabic_letter}'.")
                return arabic_letter

        # 4. Last resort: Look for any mentioned Arabic letter
        option_mentions = re.findall(r"Ø§Ù„Ø®ÙŠØ§Ø±\s*([\u0621-\u064A])", response_text)
        if option_mentions:
            last_option = option_mentions[-1]
            print(f"  -> Found answer using last-mentioned option heuristic: '{last_option}'")
            return last_option

        # If all parsing fails, return an empty string
        print(f"  -> Warning: Could not deduce answer from response: '{response_text}'. Recording as empty.")
        return ""

    except Exception as e:
        print(f"  -> An error occurred during model inference: {e}")
        return "INFERENCE_ERROR"


# --- Function to Evaluate MCQ Accuracy ---
def evaluate_mcq_accuracy(predictions, ground_truths):
    """Calculates and prints the accuracy of the model's predictions, normalizing Alif."""
    print("\n" + "="*50)
    print("ğŸš€ Starting Evaluation...")
    print("="*50)

    error_codes = ["INFERENCE_ERROR", ""]
    valid_indices = [i for i, p in enumerate(predictions) if p not in error_codes and pd.notna(p)]
    valid_predictions = [predictions[i] for i in valid_indices]
    valid_ground_truths = [ground_truths[i] for i in valid_indices]

    if not valid_predictions:
        print("No valid predictions to evaluate. Check for widespread inference or parsing errors.")
        return

    # Helper function to normalize different forms of Alif
    def normalize_alif(letter):
        # Replaces hamza forms (Ø£, Ø¥, Ø¢) with the plain Alif (Ø§)
        return str(letter).replace('Ø£', 'Ø§').replace('Ø¥', 'Ø§').replace('Ø¢', 'Ø§')

    # Normalize both predictions and ground truths before comparison
    normalized_predictions = [normalize_alif(p) for p in valid_predictions]
    normalized_ground_truths = [normalize_alif(g) for g in valid_ground_truths]

    # Calculate accuracy using the normalized lists
    accuracy = accuracy_score(normalized_ground_truths, normalized_predictions)
    correct_predictions = int(accuracy * len(normalized_predictions))
    total_valid_predictions = len(valid_predictions)
    total_questions = len(ground_truths)
    failed_or_empty = total_questions - total_valid_predictions

    print(f"Total Questions Attempted: {total_questions}")
    print(f"Final Unanswered / Error Count: {failed_or_empty}")
    print(f"Valid Predictions to Evaluate: {total_valid_predictions}")
    print("-" * 20)
    print(f"Correct Predictions: {correct_predictions} / {total_valid_predictions}")
    print(f"ğŸ“Š Accuracy (on valid responses): {accuracy * 100:.2f}%")
    print("="*50 + "\nâœ… Evaluation Complete.\n" + "="*50)


# --- Main Execution Logic ---
def main():
    """Main function to run the prediction and evaluation process."""
    # --- Login to Hugging Face and initialize the local model pipeline ---
    try:
        # Login to Hugging Face Hub to download the model
        hf_token = userdata.get('HF_TOKEN')
        login(token=hf_token)
        print("âœ… Successfully logged into Hugging Face Hub.")

        # Define the model ID
        model_id = "aaditya/Llama3-OpenBioLLM-8B"
        print(f"ğŸš€ Initializing local model pipeline: {model_id}")

        # Explicitly set device to cuda if available, otherwise cpu
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Device set to use {device}")

        # Load the tokenizer and set the chat template manually
        tokenizer = AutoTokenizer.from_pretrained(model_id)

        # This is the standard Llama 3 chat template
        llama3_template = (
            "{% if messages[0]['role'] == 'system' %}"
            "{{ bos_token + '<|start_header_id|>system<|end_header_id|>\n\n' + messages[0]['content'] + '<|eot_id|>' }}"
            "{% endif %}"
            "{% for message in messages %}"
            "{% if message['role'] == 'user' %}"
            "{{ bos_token + '<|start_header_id|>user<|end_header_id|>\n\n' + message['content'] + '<|eot_id|>' }}"
            "{% elif message['role'] == 'assistant' %}"
            "{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' + message['content'] + '<|eot_id|>' }}"
            "{% endif %}"
            "{% endfor %}"
            "{% if add_generation_prompt %}"
            "{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}"
            "{% endif %}"
        )
        tokenizer.chat_template = llama3_template

        # Create the transformers pipeline
        pipeline = transformers.pipeline(
            "text-generation",
            model=model_id,
            tokenizer=tokenizer, # Pass the updated tokenizer
            model_kwargs={"torch_dtype": torch.bfloat16},
            device=device,
        )
        print("âœ… Hugging Face pipeline initialized successfully.")
    except Exception as e:
        print(f"âŒ Failed to initialize the Hugging Face pipeline. Error: {e}")
        print("Please ensure you have set 'HF_TOKEN' in Colab secrets and that the Colab runtime has a GPU.")
        return

    # --- Load and Prepare Data ---
    # Load from output file if it exists to resume, otherwise start from input
    if os.path.exists(OUTPUT_CSV):
        print(f"âœ… Found existing predictions file: '{OUTPUT_CSV}'. Loading to resume.")
        try:
            df = pd.read_csv(OUTPUT_CSV, encoding='utf-8')
        except Exception as e:
            print(f"An error occurred while reading the existing output CSV: {e}")
            return
    else:
        print(f"No existing output file found. Loading from input: '{INPUT_CSV}'.")
        try:
            df = pd.read_csv(INPUT_CSV, encoding='utf-8')
        except FileNotFoundError:
            print(f"Error: The input file '{INPUT_CSV}' was not found. Please check the path.")
            return
        except Exception as e:
            print(f"An error occurred while reading the input CSV: {e}")
            return
        # Add the prediction column for the first run
        df[PREDICTED_COLUMN] = ""


    # --- Logic to Run Predictions (Full or Rerun) ---
    df.dropna(subset=[QUESTION_COLUMN, ANSWER_COLUMN], inplace=True)
    df.reset_index(drop=True, inplace=True)

    # Ensure the prediction column exists and fill NaNs with empty strings for processing
    if PREDICTED_COLUMN not in df.columns:
        df[PREDICTED_COLUMN] = ""
    df[PREDICTED_COLUMN] = df[PREDICTED_COLUMN].fillna("")

    # Identify questions that need a prediction
    error_codes_to_rerun = ["INFERENCE_ERROR", ""]
    indices_to_run = df[df[PREDICTED_COLUMN].isin(error_codes_to_rerun)].index

    if indices_to_run.empty:
        print("âœ… No questions found to run. All answers are present. Proceeding to evaluation.")
    else:
        print(f"Found {len(indices_to_run)} questions to process. Starting prediction process...")
        start_time = time.time()
        
        for i, index in enumerate(indices_to_run):
            question = df.loc[index, QUESTION_COLUMN]
            print(f"Processing question {i + 1}/{len(indices_to_run)} (Overall index: {index})...")
            
            predicted_answer = ""
            # Add retry logic
            for attempt in range(2): # Try up to 2 times
                print(f"  Attempt {attempt + 1}...")
                predicted_answer = generate_answer(question, pipeline)
                if predicted_answer and predicted_answer != "INFERENCE_ERROR":
                    break # Got a valid answer, no need to retry
                if attempt == 0: # Only print retry message on the first failure
                    print(f"  -> Attempt 1 failed. Retrying...")
            
            # Update the DataFrame with the new prediction
            df.loc[index, PREDICTED_COLUMN] = predicted_answer
            
            ground_truth_letter = str(df.loc[index, ANSWER_COLUMN]).strip()[0]
            print(f"  -> Ground Truth: {ground_truth_letter} | Final Predicted Letter: {predicted_answer}")
            
            # Save the updated DataFrame to the output file after each prediction
            # This makes the process resumable in case of interruption
            df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')

        end_time = time.time()
        total_duration = end_time - start_time
        minutes = int(total_duration // 60)
        seconds = int(total_duration % 60)
        print("\n" + "="*50)
        print(f"âœ… Prediction generation complete.")
        print(f"â±ï¸  Total time taken: {minutes} minutes and {seconds} seconds.")
        print(f"Successfully saved all predictions to '{OUTPUT_CSV}'.")
        print("="*50)


    # --- Final Evaluation ---
    # Convert columns to lists for the evaluation function
    predictions = df[PREDICTED_COLUMN].tolist()
    ground_truths = [str(ans).strip()[0] if pd.notna(ans) and str(ans).strip() else "INVALID_TRUTH" for ans in df[ANSWER_COLUMN].tolist()]
    
    evaluate_mcq_accuracy(predictions, ground_truths)


if __name__ == "__main__":
    main()
