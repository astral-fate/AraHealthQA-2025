import os
import re
import pandas as pd
import time
# Import the userdata module for Google Colab secrets
from google.colab import userdata
from sklearn.metrics import accuracy_score
# Import libraries for local Hugging Face model inference
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login

# --- File paths and column names (Updated for your data) ---
INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/t2t1/data/fill-in-the-blank-choices.csv'
OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/t2t1/data/predictions_fill-in-the-blank_OpenBioLLM_8B.csv'

# --- Column names (Set for your data) ---
QUESTION_COLUMN = 'Question - Arabic'
ANSWER_COLUMN = 'Answer - Arabic'
# --- NEW: Added a column name for our predictions to manage state ---
PREDICTED_COLUMN = 'Predicted_Answer'


# --- Chain of Thought & Few-Shot Prompting Configuration ---
# This system prompt is now in Arabic to guide the model more effectively.
SYSTEM_PROMPT = """ุฃูุช ุฎุจูุฑ ุทุจู ููุณุงุนุฏ ุงูุชุญุงูุงุช ุฏููู. ูููุชู ูู ุญู ุณุคุงู ูุชุนุฏุฏ ุงูุฎูุงุฑุงุช ุจุงููุบุฉ ุงูุนุฑุจูุฉ.
ุฃููุงูุ ุณุชููู ุจุนูููุฉ ุชูููุฑ ุฎุทูุฉ ุจุฎุทูุฉ. ูู ุจุชุญููู ุงูุณุคุงู ุงูุทุจูุ ูุชูููู ูู ุฎูุงุฑ (ุฃ, ุจ, ุฌ, ุฏ, ูู)ุ ูุดุฑุญ ุฃุณุจุงุจ ุงุฎุชูุงุฑู ููุฅุฌุงุจุฉ ุงูุตุญูุญุฉ.
ุซุงููุงูุ ุจุนุฏ ุดุฑุญ ุฃุณุจุงุจูุ ูุฌุจ ุนููู ุชูุฏูู ุงูุฅุฌุงุจุฉ ุงูููุงุฆูุฉ ูู ุณุทุฑ ุฌุฏูุฏ ุจุงูุชูุณูู ุงูุชุงูู:
Final Answer: [ุงูุญุฑู ุงูุนุฑุจู ููุฎูุงุฑ ุงูุตุญูุญ]

ูุฐู ุงูุนูููุฉ ุงูููููุฉ ูู ุฎุทูุชูู ุฅูุฒุงููุฉ. ูุฌุจ ุฃู ุชููู ุฅุฌุงุจุชู ุจุฃููููุง ุจุงููุบุฉ ุงูุนุฑุจูุฉ.
"""

FEW_SHOT_EXAMPLES = [
    {
        "role": "user",
        "content": """ุงููุฃ ุงููุฑุงุบุงุช ูู ุงูุฌููุฉ ุงูุชุงููุฉ:
ูู ุญุงูุฉ ุงูุงูุตุจุงุจ ุงูุฌูุจูุ ูุดูุฑ ุงูุฎูุงุถ ุฃู ุบูุงุจ ุงูุฑุฌูุงู ุงูููุณู ุนูู ุงูุฌุงูุจ ุงููุตุงุจ ุฅูู ____ุ ูุงูุฐู ููุชุฌ ุนู ____.
ุฃ. ุชุฑุงูู ุงูุณูุงุฆูุ ุนุฒู ุงูุตูุช
ุจ. ุงุญุชุจุงุณ ุงูููุงุกุ ุงูููุงุฑ ุงูุญููุตูุงุช ุงูููุงุฆูุฉ
ุฌ. ุงูุชูููุ ุงูุฎูุงุถ ูุฑููุฉ ุงูุฑุฆุฉ
ุฏ. ููู ุงููุฑูุ ุงูุณุฏุงุฏ ุงูุดุนุจ ุงูููุงุฆูุฉ"""
    },
    {
        "role": "assistant",
        "content": """**ุงูุชูููุฑ ุฎุทูุฉ ุจุฎุทูุฉ:**
1.  **ุชุญููู ุงูุณุคุงู**: ูุณุฃู ุงูุณุคุงู ุนู ุฏูุงูุฉ ุงูุฎูุงุถ ุฃู ุบูุงุจ "ุงูุฑุฌูุงู ุงูููุณู" (tactile fremitus) ูู ุญุงูุฉ "ุงูุงูุตุจุงุจ ุงูุฌูุจู" (pleural effusion) ูุนู ุงูุขููุฉ ุงููุณุจุจุฉ ูุฐูู. ุงูุฑุฌูุงู ุงูููุณู ูู ุงูุงูุชุฒุงุฒ ุงูุฐู ูููู ุงูุดุนูุฑ ุจู ุนูู ุฌุฏุงุฑ ุงูุตุฏุฑ ุฃุซูุงุก ุงูููุงู.
2.  **ุชูููู ุงูุฎูุงุฑุงุช**:
    * **ุฃ. ุชุฑุงูู ุงูุณูุงุฆูุ ุนุฒู ุงูุตูุช**: ุงูุงูุตุจุงุจ ุงูุฌูุจู ูู ุจุงููุนู ุชุฑุงูู ููุณูุงุฆู ูู ุงูุบุดุงุก ุงูุฌูุจู. ูุฐุง ุงูุณุงุฆู ูุนูู ูุนุงุฒูุ ููุง ูููุน ุงูุชูุงู ุงูุชุฒุงุฒุงุช ุงูุตูุช ูู ุงูุฑุฆุฉ ุฅูู ุฌุฏุงุฑ ุงูุตุฏุฑ. ูุฐุง ูุชุทุงุจู ุชูุงููุง ูุน ะฝะฐัะพะดุฉ ุงูุฎูุงุถ ุงูุฑุฌูุงู ุงูููุณู.
    * **ุจ. ุงุญุชุจุงุณ ุงูููุงุกุ ุงูููุงุฑ ุงูุญููุตูุงุช ุงูููุงุฆูุฉ**: ูุฐุง ูุตู ุญุงูุฉ ุงุณุชุฑูุงุญ ุงูุตุฏุฑ (pneumothorax) ุฃู ุงูุฎูุงุต ุงูุฑุฆุฉ (atelectasis)ุ ูุงูุชู ููุง ููุฌูุฏุงุช ููุฒูุงุฆูุฉ ูุฎุชููุฉ.
    * **ุฌ. ุงูุชูููุ ุงูุฎูุงุถ ูุฑููุฉ ุงูุฑุฆุฉ**: ุงูุชููู ุงูุฑุฆูู (Pulmonary fibrosis) ูุฒูุฏ ูู ูุซุงูุฉ ุฃูุณุฌุฉ ุงูุฑุฆุฉุ ููุง ูุฏ ูุคุฏู ุฅูู ุฒูุงุฏุฉ ุงูุฑุฌูุงู ุงูููุณูุ ูููุณ ุงูุฎูุงุถู.
    * **ุฏ. ููู ุงููุฑูุ ุงูุณุฏุงุฏ ุงูุดุนุจ ุงูููุงุฆูุฉ**: ูุฏ ูุณุจุจ ุงููุฑู ุงูุตุจุงุจูุง ุฌูุจููุงุ ููู ุงูุณุจุจ ุงููุจุงุดุฑ ูุงูุฎูุงุถ ุงูุฑุฌูุงู ูู ูุฐู ุงูุญุงูุฉ ูู ุงูุณุงุฆู ููุณู ุงูุฐู ูุนุฒู ุงูุตูุช. ุงูุฎูุงุฑ "ุฃ" ูุตู ุงูุขููุฉ ุงูููุฒูุงุฆูุฉ ุงููุจุงุดุฑุฉ ุจุดูู ุฃูุถู.
3.  **ุงูุงุณุชูุชุงุฌ**: ุงูุฎูุงุฑ ุงูุฃูุซุฑ ุฏูุฉ ูู ุฃู ุชุฑุงูู ุงูุณูุงุฆู ูู ูุง ูุณุจุจ ุนุฒู ุงูุตูุชุ ููุง ูุคุฏู ุฅูู ุงูุฎูุงุถ ุงูุฑุฌูุงู ุงูููุณู.

Final Answer: ุฃ"""
    }
]


# --- Function to Generate Answers using Local Hugging Face Model ---
def generate_answer(question, pipeline):
    """
    Generates an answer using a local transformers pipeline.
    """
    # Construct the message list for the chat template
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT}
    ]
    messages.extend(FEW_SHOT_EXAMPLES)
    final_instruction = "ุงูุขูุ ุงุชุจุน ุงูุชุนูููุงุช ุจุฏูุฉ. ุงุจุฏุฃ ุจุงูุชูููุฑ ุฎุทูุฉ ุจุฎุทูุฉ ุซู ุงุฎุชุชู ุฅุฌุงุจุชู ุจู 'Final Answer: ' ูุชุจูุนูุง ุจุงูุญุฑู ุงูุตุญูุญ ููุท."
    messages.append({"role": "user", "content": f"{question}\n\n{final_instruction}"})

    try:
        # Apply the chat template to create the full prompt
        prompt = pipeline.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # Define terminators to stop generation cleanly
        terminators = [
            pipeline.tokenizer.eos_token_id,
            pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]

        # Generate text using the pipeline
        outputs = pipeline(
            prompt,
            max_new_tokens=1024,
            eos_token_id=terminators,
            do_sample=False, # Use greedy decoding for more deterministic results
        )

        # Extract the generated text after the prompt
        response_text = outputs[0]["generated_text"][len(prompt):]

        # --- Parsing Logic ---
        # 1. Primary method: Look for the Arabic format
        match = re.search(r"Final Answer:\s*([\u0621-\u064A])", response_text, re.IGNORECASE)
        if match:
            return match.group(1)

        # 2. Heuristic method: Look for Arabic conclusive phrases
        conclusive_phrases = [
            r"ุงูุฎูุงุฑ ุงูุตุญูุญ ูู\s*([\u0621-\u064A])", r"ุงูุฅุฌุงุจุฉ ุงูุตุญูุญุฉ ูู\s*([\u0621-\u064A])",
            r"ุงูุงุณุชูุชุงุฌ ูู ุฃู ุงูุฎูุงุฑ\s*([\u0621-\u064A])", r"ุงูุฎูุงุฑ\s*([\u0621-\u064A])\s*ูู ุงูุตุญูุญ",
        ]
        for phrase in conclusive_phrases:
            match = re.search(phrase, response_text)
            if match:
                print(f"  -> Found answer using Arabic conclusive phrase heuristic.")
                return match.group(1)

        # 3. Fallback method: Look for English letters and translate them
        english_match = re.search(r"The correct answer is\s+([A-E])", response_text, re.IGNORECASE)
        if not english_match:
            english_match = re.search(r"Final Answer:\s*([A-E])", response_text, re.IGNORECASE)

        if english_match:
            english_letter = english_match.group(1).upper()
            translation_map = {'A': 'ุฃ', 'B': 'ุจ', 'C': 'ุฌ', 'D': 'ุฏ', 'E': 'ู'}
            arabic_letter = translation_map.get(english_letter)
            if arabic_letter:
                print(f"  -> Found English letter '{english_letter}', translated to '{arabic_letter}'.")
                return arabic_letter

        # 4. Last resort: Look for any mentioned Arabic letter
        option_mentions = re.findall(r"ุงูุฎูุงุฑ\s*([\u0621-\u064A])", response_text)
        if option_mentions:
            last_option = option_mentions[-1]
            print(f"  -> Found answer using last-mentioned option heuristic: '{last_option}'")
            return last_option

        # If all parsing fails, return an empty string
        print(f"  -> Warning: Could not deduce answer from response: '{response_text}'. Recording as empty.")
        return ""

    except Exception as e:
        print(f"  -> An error occurred during model inference: {e}")
        return "INFERENCE_ERROR"


# --- Function to Evaluate MCQ Accuracy ---
def evaluate_mcq_accuracy(predictions, ground_truths):
    """Calculates and prints the accuracy of the model's predictions, normalizing Alif."""
    print("\n" + "="*50)
    print("๐ Starting Evaluation...")
    print("="*50)

    error_codes = ["INFERENCE_ERROR", ""]
    valid_indices = [i for i, p in enumerate(predictions) if p not in error_codes and pd.notna(p)]
    valid_predictions = [predictions[i] for i in valid_indices]
    valid_ground_truths = [ground_truths[i] for i in valid_indices]

    if not valid_predictions:
        print("No valid predictions to evaluate. Check for widespread inference or parsing errors.")
        return

    # Helper function to normalize different forms of Alif
    def normalize_alif(letter):
        # Replaces hamza forms (ุฃ, ุฅ, ุข) with the plain Alif (ุง)
        return str(letter).replace('ุฃ', 'ุง').replace('ุฅ', 'ุง').replace('ุข', 'ุง')

    # Normalize both predictions and ground truths before comparison
    normalized_predictions = [normalize_alif(p) for p in valid_predictions]
    normalized_ground_truths = [normalize_alif(g) for g in valid_ground_truths]

    # Calculate accuracy using the normalized lists
    accuracy = accuracy_score(normalized_ground_truths, normalized_predictions)
    correct_predictions = int(accuracy * len(normalized_predictions))
    total_valid_predictions = len(valid_predictions)
    total_questions = len(ground_truths)
    failed_or_empty = total_questions - total_valid_predictions

    print(f"Total Questions Attempted: {total_questions}")
    print(f"Final Unanswered / Error Count: {failed_or_empty}")
    print(f"Valid Predictions to Evaluate: {total_valid_predictions}")
    print("-" * 20)
    print(f"Correct Predictions: {correct_predictions} / {total_valid_predictions}")
    print(f"๐ Accuracy (on valid responses): {accuracy * 100:.2f}%")
    print("="*50 + "\nโ Evaluation Complete.\n" + "="*50)


# --- Main Execution Logic ---
def main():
    """Main function to run the prediction and evaluation process."""
    # --- Login to Hugging Face and initialize the local model pipeline ---
    try:
        # Login to Hugging Face Hub to download the model
        hf_token = userdata.get('HF_TOKEN')
        login(token=hf_token)
        print("โ Successfully logged into Hugging Face Hub.")

        # Define the model ID
        model_id = "aaditya/Llama3-OpenBioLLM-8B"
        print(f"๐ Initializing local model pipeline: {model_id}")

        # Explicitly set device to cuda if available, otherwise cpu
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Device set to use {device}")

        # Load the tokenizer and set the chat template manually
        tokenizer = AutoTokenizer.from_pretrained(model_id)

        # This is the standard Llama 3 chat template
        llama3_template = (
            "{% if messages[0]['role'] == 'system' %}"
            "{{ bos_token + '<|start_header_id|>system<|end_header_id|>\n\n' + messages[0]['content'] + '<|eot_id|>' }}"
            "{% endif %}"
            "{% for message in messages %}"
            "{% if message['role'] == 'user' %}"
            "{{ bos_token + '<|start_header_id|>user<|end_header_id|>\n\n' + message['content'] + '<|eot_id|>' }}"
            "{% elif message['role'] == 'assistant' %}"
            "{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' + message['content'] + '<|eot_id|>' }}"
            "{% endif %}"
            "{% endfor %}"
            "{% if add_generation_prompt %}"
            "{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}"
            "{% endif %}"
        )
        tokenizer.chat_template = llama3_template

        # Create the transformers pipeline
        pipeline = transformers.pipeline(
            "text-generation",
            model=model_id,
            tokenizer=tokenizer, # Pass the updated tokenizer
            model_kwargs={"torch_dtype": torch.bfloat16},
            device=device,
        )
        print("โ Hugging Face pipeline initialized successfully.")
    except Exception as e:
        print(f"โ Failed to initialize the Hugging Face pipeline. Error: {e}")
        print("Please ensure you have set 'HF_TOKEN' in Colab secrets and that the Colab runtime has a GPU.")
        return

    # --- Load and Prepare Data ---
    # Load from output file if it exists to resume, otherwise start from input
    if os.path.exists(OUTPUT_CSV):
        print(f"โ Found existing predictions file: '{OUTPUT_CSV}'. Loading to resume.")
        try:
            df = pd.read_csv(OUTPUT_CSV, encoding='utf-8')
        except Exception as e:
            print(f"An error occurred while reading the existing output CSV: {e}")
            return
    else:
        print(f"No existing output file found. Loading from input: '{INPUT_CSV}'.")
        try:
            df = pd.read_csv(INPUT_CSV, encoding='utf-8')
        except FileNotFoundError:
            print(f"Error: The input file '{INPUT_CSV}' was not found. Please check the path.")
            return
        except Exception as e:
            print(f"An error occurred while reading the input CSV: {e}")
            return
        # Add the prediction column for the first run
        df[PREDICTED_COLUMN] = ""


    # --- Logic to Run Predictions (Full or Rerun) ---
    df.dropna(subset=[QUESTION_COLUMN, ANSWER_COLUMN], inplace=True)
    df.reset_index(drop=True, inplace=True)

    # Ensure the prediction column exists and fill NaNs with empty strings for processing
    if PREDICTED_COLUMN not in df.columns:
        df[PREDICTED_COLUMN] = ""
    df[PREDICTED_COLUMN] = df[PREDICTED_COLUMN].fillna("")

    # Identify questions that need a prediction
    error_codes_to_rerun = ["INFERENCE_ERROR", ""]
    indices_to_run = df[df[PREDICTED_COLUMN].isin(error_codes_to_rerun)].index

    if indices_to_run.empty:
        print("โ No questions found to run. All answers are present. Proceeding to evaluation.")
    else:
        print(f"Found {len(indices_to_run)} questions to process. Starting prediction process...")
        start_time = time.time()
        
        for i, index in enumerate(indices_to_run):
            question = df.loc[index, QUESTION_COLUMN]
            print(f"Processing question {i + 1}/{len(indices_to_run)} (Overall index: {index})...")
            
            predicted_answer = ""
            # Add retry logic
            for attempt in range(2): # Try up to 2 times
                print(f"  Attempt {attempt + 1}...")
                predicted_answer = generate_answer(question, pipeline)
                if predicted_answer and predicted_answer != "INFERENCE_ERROR":
                    break # Got a valid answer, no need to retry
                if attempt == 0: # Only print retry message on the first failure
                    print(f"  -> Attempt 1 failed. Retrying...")
            
            # Update the DataFrame with the new prediction
            df.loc[index, PREDICTED_COLUMN] = predicted_answer
            
            ground_truth_letter = str(df.loc[index, ANSWER_COLUMN]).strip()[0]
            print(f"  -> Ground Truth: {ground_truth_letter} | Final Predicted Letter: {predicted_answer}")
            
            # Save the updated DataFrame to the output file after each prediction
            # This makes the process resumable in case of interruption
            df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')

        end_time = time.time()
        total_duration = end_time - start_time
        minutes = int(total_duration // 60)
        seconds = int(total_duration % 60)
        print("\n" + "="*50)
        print(f"โ Prediction generation complete.")
        print(f"โฑ๏ธ  Total time taken: {minutes} minutes and {seconds} seconds.")
        print(f"Successfully saved all predictions to '{OUTPUT_CSV}'.")
        print("="*50)


    # --- Final Evaluation ---
    # Convert columns to lists for the evaluation function
    predictions = df[PREDICTED_COLUMN].tolist()
    ground_truths = [str(ans).strip()[0] if pd.notna(ans) and str(ans).strip() else "INVALID_TRUTH" for ans in df[ANSWER_COLUMN].tolist()]
    
    evaluate_mcq_accuracy(predictions, ground_truths)


if __name__ == "__main__":
    main()
