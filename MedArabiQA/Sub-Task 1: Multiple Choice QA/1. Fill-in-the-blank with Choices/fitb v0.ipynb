{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_hvLyqWPuVV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8U4Ezc1UfEJ"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Track 2: MedArabiQ 2025 (General Health)\n",
    "\n",
    "\n",
    "# Sub-Task 1: Multiple Choice QA\n",
    "\n",
    "\n",
    " # BiMediX2 : 25.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jW5H-nsoI9GN",
    "outputId": "d7bab182-d144-454d-9506-c6f2a7a25154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/content/drive/MyDrive/AraHealthQA/predictions_mcq.csv' not found. Starting prediction generation process...\n",
      "Processing question 1/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 2/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 3/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 4/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 5/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 6/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 7/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 8/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 9/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 10/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 11/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 12/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 13/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 14/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 15/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 16/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 17/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 18/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 19/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 20/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 21/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 22/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 23/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 24/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 25/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 26/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 27/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 28/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 29/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 30/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 31/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø§\n",
      "Processing question 32/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 33/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 34/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 35/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 36/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 37/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 38/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 39/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 40/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 41/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 42/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 43/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 44/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 45/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 46/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 47/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 48/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 49/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 50/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 51/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 52/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 53/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 54/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 55/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 56/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 57/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 58/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 59/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 60/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 61/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 62/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 63/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 64/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 65/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 66/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 67/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 68/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 69/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 70/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 71/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 72/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 73/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 74/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 75/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 76/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 77/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 78/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 79/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 80/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 81/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 82/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 83/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 84/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 85/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 86/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 87/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 88/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 89/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 90/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 91/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 92/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 93/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 94/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 95/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 96/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 97/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 98/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 99/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 100/100 with BiMediX2 (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "\n",
      "Successfully generated predictions and saved them to '/content/drive/MyDrive/AraHealthQA/predictions_mcq.csv'.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_b2514eb5-8bfe-4dc6-b8d4-55bdfa0fd93d\", \"predictions_mcq.csv\", 300)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸš€ Starting Evaluation...\n",
      "==================================================\n",
      "Correct Predictions: 25 / 100\n",
      "ðŸ“Š Accuracy: 25.00%\n",
      "==================================================\n",
      "âœ… Evaluation Complete.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Main script for Multiple Choice Question Answering with BiMediX2\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score # Import accuracy_score for evaluation\n",
    "\n",
    "# --- Local Server Configuration ---\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1/\",\n",
    "    api_key=\"DUMMY_KEY\",\n",
    ")\n",
    "\n",
    "# --- UPDATED: File paths and column names for the MCQ task ---\n",
    "INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/multiple-choice-questions.csv'\n",
    "OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/predictions_mcq.csv' # As requested\n",
    "QUESTION_COLUMN = 'Question'\n",
    "ANSWER_COLUMN = 'Answer'\n",
    "\n",
    "\n",
    "# --- Function to Generate Answers for MCQ Task ---\n",
    "def generate_answer(question):\n",
    "    \"\"\"\n",
    "    Sends an MCQ question to the local vLLM server, prompting the model\n",
    "    to return only the single correct letter.\n",
    "    \"\"\"\n",
    "    # --- NEW: System prompt for the MCQ task ---\n",
    "    system_prompt = \"\"\"You are a medical exam answering machine. Your only task is to answer the following multiple-choice medical question. Read the question and the provided options (Ø£, Ø¨, Ø¬, Ø¯, Ù‡). Your response must be ONLY the single Arabic letter corresponding to the correct answer. Do not provide any explanation, reasoning, or any other text. For example, if option 'Ø¨' is correct, your entire response must be 'Ø¨'.\"\"\"\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "              model=\"MBZUAI/BiMediX2-8B-hf\",\n",
    "              messages=[\n",
    "                  {\"role\":\"system\", \"content\": system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":question}\n",
    "              ],\n",
    "              temperature=0.0, # Set to 0 for maximum determinism in a classification task\n",
    "              max_tokens=5,    # A small value is sufficient for a single letter\n",
    "            )\n",
    "            response_text = completion.choices[0].message.content.strip()\n",
    "\n",
    "            # Clean the response to ensure it's just a single character\n",
    "            # This will find the first Arabic letter in the response\n",
    "            arabic_letters = re.findall(r'[\\u0621-\\u064A]', response_text)\n",
    "            if arabic_letters:\n",
    "                return arabic_letters[0]\n",
    "            else:\n",
    "                return \"\" # Return empty if no letter is found\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"  -> An error occurred: {e}. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                return f\"API Error after multiple retries: {e}\"\n",
    "    return f\"Failed to get a response after {max_retries} attempts.\"\n",
    "\n",
    "\n",
    "# --- NEW: Function to Evaluate MCQ Accuracy ---\n",
    "def evaluate_mcq_accuracy(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculates and prints the accuracy of the MCQ predictions.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸš€ Starting Evaluation...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Ensure both lists are of the same length\n",
    "    if len(predictions) != len(ground_truths):\n",
    "        print(\"Warning: Prediction and ground truth lists have different lengths. Evaluation might be inaccurate.\")\n",
    "        # Truncate to the shorter length for comparison\n",
    "        min_len = min(len(predictions), len(ground_truths))\n",
    "        predictions = predictions[:min_len]\n",
    "        ground_truths = ground_truths[:min_len]\n",
    "\n",
    "    accuracy = accuracy_score(ground_truths, predictions)\n",
    "    correct_predictions = sum(p == g for p, g in zip(predictions, ground_truths))\n",
    "    total_predictions = len(ground_truths)\n",
    "\n",
    "    print(f\"Correct Predictions: {correct_predictions} / {total_predictions}\")\n",
    "    print(f\"ðŸ“Š Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"=\"*50 + \"\\nâœ… Evaluation Complete.\\n\" + \"=\"*50)\n",
    "\n",
    "# --- Main Execution for MCQ Task ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for the Multiple Choice Question Answering task.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{INPUT_CSV}' was not found. Please upload it first.\")\n",
    "        return\n",
    "\n",
    "    if QUESTION_COLUMN not in df.columns or ANSWER_COLUMN not in df.columns:\n",
    "        print(f\"Error: Required columns ('{QUESTION_COLUMN}', '{ANSWER_COLUMN}') not in the CSV file.\")\n",
    "        return\n",
    "\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        print(f\"âœ… Found existing prediction file: '{OUTPUT_CSV}'.\")\n",
    "        print(\"Skipping generation and loading predictions from file for evaluation.\")\n",
    "        predictions_df = pd.read_csv(OUTPUT_CSV, header=None)\n",
    "        predictions = predictions_df[0].tolist()\n",
    "    else:\n",
    "        print(f\"'{OUTPUT_CSV}' not found. Starting prediction generation process...\")\n",
    "        predictions = []\n",
    "        total_questions = len(df)\n",
    "        for index, row in df.iterrows():\n",
    "            question = row[QUESTION_COLUMN]\n",
    "            print(f\"Processing question {index + 1}/{total_questions} with BiMediX2 (MCQ Mode)...\")\n",
    "            answer_letter = generate_answer(question)\n",
    "            predictions.append(answer_letter)\n",
    "            print(f\"  -> Generated Answer: {answer_letter}\")\n",
    "\n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "        predictions_df.to_csv(OUTPUT_CSV, header=False, index=False)\n",
    "        print(f\"\\nSuccessfully generated predictions and saved them to '{OUTPUT_CSV}'.\")\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            files.download(OUTPUT_CSV)\n",
    "        except ImportError:\n",
    "            print(f\"To download '{OUTPUT_CSV}', see the file browser on the left.\")\n",
    "\n",
    "    # --- UPDATED: Extracting the first character from the Answer column as the ground truth ---\n",
    "    # We assume the correct letter is the first character of the 'Answer' string (e.g., \"Ø¯. ...\")\n",
    "    ground_truths = [str(ans).strip()[0] for ans in df[ANSWER_COLUMN].tolist()]\n",
    "\n",
    "    evaluate_mcq_accuracy(predictions, ground_truths)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rZ8EZJdQ98Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLHAiHN4RIDN"
   },
   "source": [
    "\n",
    "\n",
    "# Track 2: MedArabiQ 2025 (General Health)\n",
    "\n",
    "\n",
    "# Sub-Task 1: Multiple Choice QA\n",
    "\n",
    "\n",
    "\n",
    "# deep seek - 61%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pwmsy2i8Q-AD",
    "outputId": "08582119-7458-4347-d4df-e0397b42219e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.19.0 requires torch==2.4.0, but you have torch 2.7.1 which is incompatible.\n",
      "vllm 0.6.1.post1 requires torch==2.4.0, but you have torch 2.7.1 which is incompatible.\n",
      "xformers 0.0.27.post2 requires torch==2.4.0, but you have torch 2.7.1 which is incompatible.\n",
      "vllm-flash-attn 2.6.1 requires torch==2.4.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m'/content/drive/MyDrive/AraHealthQA/predictions_mcq_deepseek.csv' not found. Starting prediction generation process...\n",
      "Processing question 1/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 2/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 3/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 4/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø§\n",
      "Processing question 5/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 6/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 7/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 8/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 9/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 10/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 11/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 12/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 13/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 14/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 15/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 16/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 17/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 18/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 19/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 20/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 21/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 22/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 23/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 24/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 25/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 26/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 27/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 28/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø§\n",
      "Processing question 29/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 30/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 31/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 32/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 33/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 34/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø§\n",
      "Processing question 35/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 36/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 37/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 38/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 39/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 40/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 41/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 42/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 43/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 44/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 45/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 46/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 47/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 48/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 49/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 50/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 51/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ù\n",
      "Processing question 52/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 53/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 54/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 55/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 56/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 57/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 58/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 59/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 60/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 61/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 62/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 63/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 64/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 65/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 66/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 67/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 68/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 69/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 70/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 71/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 72/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 73/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 74/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 75/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 76/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 77/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 78/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 79/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 80/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 81/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 82/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 83/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 84/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 85/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 86/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 87/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 88/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 89/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 90/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 91/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 92/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 93/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 94/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 95/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 96/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 97/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 98/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 99/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 100/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "\n",
      "Successfully generated predictions and saved them to '/content/drive/MyDrive/AraHealthQA/predictions_mcq_deepseek.csv'.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_94ff03ff-02a9-48dd-9446-11b5b8b37809\", \"predictions_mcq_deepseek.csv\", 300)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸš€ Starting Evaluation...\n",
      "==================================================\n",
      "Correct Predictions: 61 / 100\n",
      "ðŸ“Š Accuracy: 61.00%\n",
      "==================================================\n",
      "âœ… Evaluation Complete.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install all necessary libraries\n",
    "!pip install groq openai nltk rouge-score bert-score transformers sentencepiece -q\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import groq\n",
    "import time\n",
    "from getpass import getpass\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Groq API Configuration ---\n",
    "# Your Groq API key will be accessed securely from Colab's secrets\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
    "except (ImportError, KeyError):\n",
    "    print(\"Secret 'GROQ_API_KEY' not found. Please add it to the Colab secrets manager.\")\n",
    "    GROQ_API_KEY = getpass('Please enter your Groq API key: ')\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = groq.Client(api_key=GROQ_API_KEY)\n",
    "\n",
    "\n",
    "# --- File paths and column names for the MCQ task ---\n",
    "INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/multiple-choice-questions.csv'\n",
    "OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/predictions_mcq_deepseek.csv' # New file name for clarity\n",
    "QUESTION_COLUMN = 'Question'\n",
    "ANSWER_COLUMN = 'Answer'\n",
    "\n",
    "\n",
    "# --- Function to Generate Answers for MCQ Task with DeepSeek ---\n",
    "def generate_answer(question):\n",
    "    \"\"\"\n",
    "    Sends an MCQ question to the Groq API to be processed by DeepSeek,\n",
    "    and includes cleaning for the <think> block.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are a medical exam answering machine. Your only task is to answer the following multiple-choice medical question. Read the question and the provided options (Ø£, Ø¨, Ø¬, Ø¯, Ù‡). Your response must be ONLY the single Arabic letter corresponding to the correct answer. Do not provide any explanation, reasoning, or any other text. For example, if option 'Ø¨' is correct, your entire response must be 'Ø¨'.\"\"\"\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "              messages=[\n",
    "                  {\"role\":\"system\", \"content\": system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":question}\n",
    "              ],\n",
    "              # --- MODEL UPDATED to DeepSeek ---\n",
    "              model=\"deepseek-r1-distill-llama-70b\",\n",
    "              temperature=0.0, # Set to 0 for maximum determinism\n",
    "              max_tokens=1024, # Kept larger in case the <think> block is long\n",
    "            )\n",
    "            raw_response_text = chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "            # --- Re-introducing the cleaning process for DeepSeek's <think> blocks ---\n",
    "            # Step 1: Remove the entire <think>...</think> block.\n",
    "            text_without_think_block = re.sub(r'<think>.*?</think>', '', raw_response_text, flags=re.DOTALL)\n",
    "\n",
    "            # Step 2: Extract the first Arabic letter from the remaining text\n",
    "            arabic_letters = re.findall(r'[\\u0621-\\u064A]', text_without_think_block)\n",
    "            if arabic_letters:\n",
    "                return arabic_letters[0]\n",
    "            else:\n",
    "                return \"\" # Return empty if no letter is found\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"  -> An error occurred: {e}. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                return f\"API Error after multiple retries: {e}\"\n",
    "    return f\"Failed to get a response after {max_retries} attempts.\"\n",
    "\n",
    "\n",
    "# --- Function to Evaluate MCQ Accuracy ---\n",
    "def evaluate_mcq_accuracy(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculates and prints the accuracy of the MCQ predictions.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸš€ Starting Evaluation...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if len(predictions) != len(ground_truths):\n",
    "        print(\"Warning: Prediction and ground truth lists have different lengths. Evaluation might be inaccurate.\")\n",
    "        min_len = min(len(predictions), len(ground_truths))\n",
    "        predictions = predictions[:min_len]\n",
    "        ground_truths = ground_truths[:min_len]\n",
    "\n",
    "    accuracy = accuracy_score(ground_truths, predictions)\n",
    "    correct_predictions = sum(p == g for p, g in zip(predictions, ground_truths))\n",
    "    total_predictions = len(ground_truths)\n",
    "\n",
    "    print(f\"Correct Predictions: {correct_predictions} / {total_predictions}\")\n",
    "    print(f\"ðŸ“Š Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"=\"*50 + \"\\nâœ… Evaluation Complete.\\n\" + \"=\"*50)\n",
    "\n",
    "# --- Main Execution for MCQ Task ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for the Multiple Choice Question Answering task.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{INPUT_CSV}' was not found. Please upload it first.\")\n",
    "        return\n",
    "\n",
    "    if QUESTION_COLUMN not in df.columns or ANSWER_COLUMN not in df.columns:\n",
    "        print(f\"Error: Required columns ('{QUESTION_COLUMN}', '{ANSWER_COLUMN}') not in the CSV file.\")\n",
    "        return\n",
    "\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        print(f\"âœ… Found existing prediction file: '{OUTPUT_CSV}'.\")\n",
    "        print(\"Skipping generation and loading predictions from file for evaluation.\")\n",
    "        predictions_df = pd.read_csv(OUTPUT_CSV, header=None)\n",
    "        predictions = predictions_df[0].tolist()\n",
    "    else:\n",
    "        print(f\"'{OUTPUT_CSV}' not found. Starting prediction generation process...\")\n",
    "        predictions = []\n",
    "        total_questions = len(df)\n",
    "        for index, row in df.iterrows():\n",
    "            question = row[QUESTION_COLUMN]\n",
    "            print(f\"Processing question {index + 1}/{total_questions} with DeepSeek (MCQ Mode)...\")\n",
    "            answer_letter = generate_answer(question)\n",
    "            predictions.append(answer_letter)\n",
    "            print(f\"  -> Generated Answer: {answer_letter}\")\n",
    "\n",
    "            # Re-introducing delay for the Groq API rate limit\n",
    "            if index < total_questions - 1:\n",
    "                time.sleep(3)\n",
    "\n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "        predictions_df.to_csv(OUTPUT_CSV, header=False, index=False)\n",
    "        print(f\"\\nSuccessfully generated predictions and saved them to '{OUTPUT_CSV}'.\")\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            files.download(OUTPUT_CSV)\n",
    "        except ImportError:\n",
    "            print(f\"To download '{OUTPUT_CSV}', see the file browser on the left.\")\n",
    "\n",
    "    # Extracting the first character from the Answer column as the ground truth\n",
    "    ground_truths = [str(ans).strip()[0] for ans in df[ANSWER_COLUMN].tolist()]\n",
    "\n",
    "    evaluate_mcq_accuracy(predictions, ground_truths)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po1HJVsoUyj-"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Track 2: MedArabiQ 2025 (General Health)\n",
    "\n",
    "\n",
    "# Sub-Task 1: Multiple Choice QA\n",
    "\n",
    "\n",
    "\n",
    "# colosseum_355b_instruct_16k -> 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6Fl8rCVnU1JQ",
    "outputId": "ff1df876-104a-4466-aeb5-f3137eccfcc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.19.0 requires torch==2.4.0, but you have torch 2.7.1 which is incompatible.\n",
      "vllm 0.6.1.post1 requires torch==2.4.0, but you have torch 2.7.1 which is incompatible.\n",
      "xformers 0.0.27.post2 requires torch==2.4.0, but you have torch 2.7.1 which is incompatible.\n",
      "vllm-flash-attn 2.6.1 requires torch==2.4.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m'/content/drive/MyDrive/AraHealthQA/predictions_mcq_colosseum.csv' not found. Starting prediction generation process...\n",
      "Processing question 1/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 2/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 3/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 4/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 5/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 6/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 7/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 8/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 9/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 10/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 11/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 12/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 13/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 14/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 15/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 16/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 17/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 18/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 19/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 20/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 21/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 22/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 23/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 24/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 25/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 26/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 27/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 28/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 29/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 30/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 31/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 32/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 33/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 34/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 35/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 36/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 37/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 38/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 39/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 40/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 41/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 42/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 43/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 44/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 45/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 46/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 47/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 48/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 49/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 50/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 51/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 52/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 53/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 54/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 55/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 56/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 57/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 58/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 59/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 60/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 61/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 62/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 63/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 64/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 65/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 66/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 67/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 68/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 69/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 70/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 71/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 72/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 73/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 74/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 75/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 76/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 77/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 78/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 79/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 80/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 81/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 82/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 83/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 84/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 85/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 86/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 87/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 88/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 89/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 90/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 91/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 92/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 93/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 94/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 95/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 96/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 97/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 98/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 99/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 100/100 with Colosseum (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "\n",
      "Successfully generated predictions and saved them to '/content/drive/MyDrive/AraHealthQA/predictions_mcq_colosseum.csv'.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_ee8a67e4-97b8-48d9-a651-d51b0f170a80\", \"predictions_mcq_colosseum.csv\", 300)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸš€ Starting Evaluation...\n",
      "==================================================\n",
      "Correct Predictions: 50 / 100\n",
      "ðŸ“Š Accuracy: 50.00%\n",
      "==================================================\n",
      "âœ… Evaluation Complete.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install all necessary libraries\n",
    "!pip install openai nltk rouge-score bert-score transformers sentencepiece -q\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import OpenAI # Using the OpenAI library for the NVIDIA API\n",
    "import time\n",
    "from getpass import getpass\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- NVIDIA API Configuration ---\n",
    "# Your NVIDIA API key will be accessed securely from Colab's secrets\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    NVIDIA_API_KEY = userdata.get('NVIDIA_API_KEY')\n",
    "except (ImportError, KeyError):\n",
    "    print(\"Secret 'NVIDIA_API_KEY' not found. Please add it to the Colab secrets manager.\")\n",
    "    NVIDIA_API_KEY = getpass('Please enter your NVIDIA API key: ')\n",
    "\n",
    "# Initialize the OpenAI client to point to the NVIDIA API endpoint\n",
    "client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = NVIDIA_API_KEY\n",
    ")\n",
    "\n",
    "# --- File paths and column names for the MCQ task ---\n",
    "INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/multiple-choice-questions.csv'\n",
    "OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/predictions_mcq_colosseum.csv' # New file name for clarity\n",
    "QUESTION_COLUMN = 'Question'\n",
    "ANSWER_COLUMN = 'Answer'\n",
    "\n",
    "\n",
    "# --- Function to Generate Answers for MCQ Task with Colosseum ---\n",
    "def generate_answer(question):\n",
    "    \"\"\"\n",
    "    Sends an MCQ question to the NVIDIA API to be processed by Colosseum,\n",
    "    prompting the model to return only the single correct letter.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are a medical exam answering machine. Your only task is to answer the following multiple-choice medical question. Read the question and the provided options (Ø£, Ø¨, Ø¬, Ø¯, Ù‡). Your response must be ONLY the single Arabic letter corresponding to the correct answer. Do not provide any explanation, reasoning, or any other text. For example, if option 'Ø¨' is correct, your entire response must be 'Ø¨'.\"\"\"\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "              messages=[\n",
    "                  {\"role\":\"system\", \"content\": system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":question}\n",
    "              ],\n",
    "              # --- MODEL UPDATED to Colosseum ---\n",
    "              model=\"igenius/colosseum_355b_instruct_16k\",\n",
    "              temperature=0.0,\n",
    "              max_tokens=5,\n",
    "            )\n",
    "            response_text = completion.choices[0].message.content.strip()\n",
    "\n",
    "            # Clean the response to ensure it's just a single character\n",
    "            arabic_letters = re.findall(r'[\\u0621-\\u064A]', response_text)\n",
    "            if arabic_letters:\n",
    "                return arabic_letters[0]\n",
    "            else:\n",
    "                return \"\" # Return empty if no letter is found\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"  -> An error occurred: {e}. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                return f\"API Error after multiple retries: {e}\"\n",
    "    return f\"Failed to get a response after {max_retries} attempts.\"\n",
    "\n",
    "\n",
    "# --- Function to Evaluate MCQ Accuracy ---\n",
    "def evaluate_mcq_accuracy(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculates and prints the accuracy of the MCQ predictions.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸš€ Starting Evaluation...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if len(predictions) != len(ground_truths):\n",
    "        print(\"Warning: Prediction and ground truth lists have different lengths. Evaluation might be inaccurate.\")\n",
    "        min_len = min(len(predictions), len(ground_truths))\n",
    "        predictions = predictions[:min_len]\n",
    "        ground_truths = ground_truths[:min_len]\n",
    "\n",
    "    accuracy = accuracy_score(ground_truths, predictions)\n",
    "    correct_predictions = sum(p == g for p, g in zip(predictions, ground_truths))\n",
    "    total_predictions = len(ground_truths)\n",
    "\n",
    "    print(f\"Correct Predictions: {correct_predictions} / {total_predictions}\")\n",
    "    print(f\"ðŸ“Š Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"=\"*50 + \"\\nâœ… Evaluation Complete.\\n\" + \"=\"*50)\n",
    "\n",
    "# --- Main Execution for MCQ Task ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for the Multiple Choice Question Answering task.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{INPUT_CSV}' was not found. Please upload it first.\")\n",
    "        return\n",
    "\n",
    "    if QUESTION_COLUMN not in df.columns or ANSWER_COLUMN not in df.columns:\n",
    "        print(f\"Error: Required columns ('{QUESTION_COLUMN}', '{ANSWER_COLUMN}') not in the CSV file.\")\n",
    "        return\n",
    "\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        print(f\"âœ… Found existing prediction file: '{OUTPUT_CSV}'.\")\n",
    "        print(\"Skipping generation and loading predictions from file for evaluation.\")\n",
    "        predictions_df = pd.read_csv(OUTPUT_CSV, header=None)\n",
    "        predictions = predictions_df[0].tolist()\n",
    "    else:\n",
    "        print(f\"'{OUTPUT_CSV}' not found. Starting prediction generation process...\")\n",
    "        predictions = []\n",
    "        total_questions = len(df)\n",
    "        for index, row in df.iterrows():\n",
    "            question = row[QUESTION_COLUMN]\n",
    "            print(f\"Processing question {index + 1}/{total_questions} with Colosseum (MCQ Mode)...\")\n",
    "            answer_letter = generate_answer(question)\n",
    "            predictions.append(answer_letter)\n",
    "            print(f\"  -> Generated Answer: {answer_letter}\")\n",
    "\n",
    "            # Conservative delay for the NVIDIA API\n",
    "            if index < total_questions - 1:\n",
    "                time.sleep(1)\n",
    "\n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "        predictions_df.to_csv(OUTPUT_CSV, header=False, index=False)\n",
    "        print(f\"\\nSuccessfully generated predictions and saved them to '{OUTPUT_CSV}'.\")\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            files.download(OUTPUT_CSV)\n",
    "        except ImportError:\n",
    "            print(f\"To download '{OUTPUT_CSV}', see the file browser on the left.\")\n",
    "\n",
    "    # Extracting the first character from the Answer column as the ground truth\n",
    "    ground_truths = [str(ans).strip()[0] for ans in df[ANSWER_COLUMN].tolist()]\n",
    "\n",
    "    evaluate_mcq_accuracy(predictions, ground_truths)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pO8RmVHU1OA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba6LTMU4V4jf"
   },
   "source": [
    "\n",
    "\n",
    "# Track 2: MedArabiQ 2025 (General Health)\n",
    "\n",
    "\n",
    "# Sub-Task 1: Multiple Choice QA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  llama-3.3-70b-versatile -> 57%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ajuB3XkmV5Xk",
    "outputId": "6df3e8a1-761a-410b-9f6d-a2507e5a7adb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.19.0 requires torch==2.4.0, but you have torch 2.7.1 which is incompatible.\n",
      "vllm 0.6.1.post1 requires torch==2.4.0, but you have torch 2.7.1 which is incompatible.\n",
      "xformers 0.0.27.post2 requires torch==2.4.0, but you have torch 2.7.1 which is incompatible.\n",
      "vllm-flash-attn 2.6.1 requires torch==2.4.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m'/content/drive/MyDrive/AraHealthQA/predictions_mcq_llama3.3-70b.csv' not found. Starting prediction generation process...\n",
      "Processing question 1/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 2/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 3/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 4/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 5/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 6/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 7/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 8/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 9/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 10/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 11/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 12/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 13/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 14/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 15/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 16/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 17/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 18/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 19/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 20/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 21/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 22/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 23/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 24/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 25/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 26/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 27/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 28/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 29/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 30/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 31/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 32/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 33/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 34/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 35/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 36/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 37/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 38/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 39/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 40/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 41/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 42/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 43/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 44/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 45/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 46/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 47/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 48/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 49/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 50/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 51/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 52/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 53/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 54/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 55/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 56/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 57/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 58/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 59/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 60/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 61/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 62/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 63/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 64/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 65/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 66/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 67/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 68/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 69/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 70/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 71/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 72/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 73/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 74/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 75/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 76/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 77/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 78/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 79/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 80/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 81/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 82/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 83/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 84/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 85/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 86/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 87/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 88/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 89/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 90/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 91/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 92/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 93/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø£\n",
      "Processing question 94/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "Processing question 95/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ù‡\n",
      "Processing question 96/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 97/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¨\n",
      "Processing question 98/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 99/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¯\n",
      "Processing question 100/100 with Llama 3.3 70B (MCQ Mode)...\n",
      "  -> Generated Answer: Ø¬\n",
      "\n",
      "Successfully generated predictions and saved them to '/content/drive/MyDrive/AraHealthQA/predictions_mcq_llama3.3-70b.csv'.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_8ab50200-be99-4886-aab1-7e836a9cd138\", \"predictions_mcq_llama3.3-70b.csv\", 300)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸš€ Starting Evaluation...\n",
      "==================================================\n",
      "Correct Predictions: 57 / 100\n",
      "ðŸ“Š Accuracy: 57.00%\n",
      "==================================================\n",
      "âœ… Evaluation Complete.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install all necessary libraries\n",
    "!pip install groq openai nltk rouge-score bert-score transformers sentencepiece -q\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import groq # Import the Groq library\n",
    "import time\n",
    "from getpass import getpass\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Groq API Configuration ---\n",
    "# Your Groq API key will be accessed securely from Colab's secrets\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
    "except (ImportError, KeyError):\n",
    "    print(\"Secret 'GROQ_API_KEY' not found. Please add it to the Colab secrets manager.\")\n",
    "    GROQ_API_KEY = getpass('Please enter your Groq API key: ')\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = groq.Client(api_key=GROQ_API_KEY)\n",
    "\n",
    "# --- File paths and column names for the MCQ task ---\n",
    "INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/multiple-choice-questions.csv'\n",
    "OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/predictions_mcq_llama3.3-70b.csv' # New file name\n",
    "QUESTION_COLUMN = 'Question'\n",
    "ANSWER_COLUMN = 'Answer'\n",
    "\n",
    "\n",
    "# --- Function to Generate Answers for MCQ Task with Llama 3.3 ---\n",
    "def generate_answer(question):\n",
    "    \"\"\"\n",
    "    Sends an MCQ question to the Groq API to be processed by Llama 3.3,\n",
    "    prompting the model to return only the single correct letter.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are a medical exam answering machine. Your only task is to answer the following multiple-choice medical question. Read the question and the provided options (Ø£, Ø¨, Ø¬, Ø¯, Ù‡). Your response must be ONLY the single Arabic letter corresponding to the correct answer. Do not provide any explanation, reasoning, or any other text. For example, if option 'Ø¨' is correct, your entire response must be 'Ø¨'.\"\"\"\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "              messages=[\n",
    "                  {\"role\":\"system\", \"content\": system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":question}\n",
    "              ],\n",
    "              # --- MODEL UPDATED to Llama 3.3 70B as requested ---\n",
    "              model=\"llama-3.3-70b-versatile\",\n",
    "              temperature=0.0,\n",
    "              max_tokens=5,\n",
    "            )\n",
    "            response_text = chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "            # Clean the response to ensure it's just a single character\n",
    "            arabic_letters = re.findall(r'[\\u0621-\\u064A]', response_text)\n",
    "            if arabic_letters:\n",
    "                return arabic_letters[0]\n",
    "            else:\n",
    "                return \"\" # Return empty if no letter is found\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"  -> An error occurred: {e}. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                return f\"API Error after multiple retries: {e}\"\n",
    "    return f\"Failed to get a response after {max_retries} attempts.\"\n",
    "\n",
    "\n",
    "# --- Function to Evaluate MCQ Accuracy ---\n",
    "def evaluate_mcq_accuracy(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculates and prints the accuracy of the MCQ predictions.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸš€ Starting Evaluation...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if len(predictions) != len(ground_truths):\n",
    "        print(\"Warning: Prediction and ground truth lists have different lengths. Evaluation might be inaccurate.\")\n",
    "        min_len = min(len(predictions), len(ground_truths))\n",
    "        predictions = predictions[:min_len]\n",
    "        ground_truths = ground_truths[:min_len]\n",
    "\n",
    "    accuracy = accuracy_score(ground_truths, predictions)\n",
    "    correct_predictions = sum(p == g for p, g in zip(predictions, ground_truths))\n",
    "    total_predictions = len(ground_truths)\n",
    "\n",
    "    print(f\"Correct Predictions: {correct_predictions} / {total_predictions}\")\n",
    "    print(f\"ðŸ“Š Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"=\"*50 + \"\\nâœ… Evaluation Complete.\\n\" + \"=\"*50)\n",
    "\n",
    "# --- Main Execution for MCQ Task ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for the Multiple Choice Question Answering task.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{INPUT_CSV}' was not found. Please upload it first.\")\n",
    "        return\n",
    "\n",
    "    if QUESTION_COLUMN not in df.columns or ANSWER_COLUMN not in df.columns:\n",
    "        print(f\"Error: Required columns ('{QUESTION_COLUMN}', '{ANSWER_COLUMN}') not in the CSV file.\")\n",
    "        return\n",
    "\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        print(f\"âœ… Found existing prediction file: '{OUTPUT_CSV}'.\")\n",
    "        print(\"Skipping generation and loading predictions from file for evaluation.\")\n",
    "        predictions_df = pd.read_csv(OUTPUT_CSV, header=None)\n",
    "        predictions = predictions_df[0].tolist()\n",
    "    else:\n",
    "        print(f\"'{OUTPUT_CSV}' not found. Starting prediction generation process...\")\n",
    "        predictions = []\n",
    "        total_questions = len(df)\n",
    "        for index, row in df.iterrows():\n",
    "            question = row[QUESTION_COLUMN]\n",
    "            print(f\"Processing question {index + 1}/{total_questions} with Llama 3.3 70B (MCQ Mode)...\")\n",
    "            answer_letter = generate_answer(question)\n",
    "            predictions.append(answer_letter)\n",
    "            print(f\"  -> Generated Answer: {answer_letter}\")\n",
    "\n",
    "            # Delay for Groq API rate limit\n",
    "            if index < total_questions - 1:\n",
    "                time.sleep(3)\n",
    "\n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "        predictions_df.to_csv(OUTPUT_CSV, header=False, index=False)\n",
    "        print(f\"\\nSuccessfully generated predictions and saved them to '{OUTPUT_CSV}'.\")\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            files.download(OUTPUT_CSV)\n",
    "        except ImportError:\n",
    "            print(f\"To download '{OUTPUT_CSV}', see the file browser on the left.\")\n",
    "\n",
    "    # Extracting the first character from the Answer column as the ground truth\n",
    "    ground_truths = [str(ans).strip()[0] for ans in df[ANSWER_COLUMN].tolist()]\n",
    "\n",
    "    evaluate_mcq_accuracy(predictions, ground_truths)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnizK_SEV5ck"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4o554nXZhaO"
   },
   "source": [
    "\n",
    "\n",
    "# Track 2: MedArabiQ 2025 (General Health)\n",
    "\n",
    "\n",
    "# Sub-Task 1: Fill-in-the-blank with Choices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# llama-3.3-70b-versatile -> 69%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tE4LUlDqaNXV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qp4olnfAV5hP",
    "outputId": "e890cf4c-78bd-4d03-f4c2-52450a5c8875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices.csv' not found. Starting prediction generation process...\n",
      "Processing question 1/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
      "Processing question 2/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 3/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 4/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 5/100...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
      "Processing question 6/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
      "Processing question 7/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 8/100...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¬\n",
      "Processing question 9/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 10/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
      "Processing question 11/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 12/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
      "Processing question 13/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 14/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 15/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 16/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 17/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
      "Processing question 18/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 19/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 20/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 21/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¨\n",
      "Processing question 22/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
      "Processing question 23/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 24/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 25/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 26/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 27/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
      "Processing question 28/100...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¨\n",
      "Processing question 29/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
      "Processing question 30/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 31/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 32/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¬\n",
      "Processing question 33/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 34/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 35/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 36/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 37/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 38/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 39/100...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¨\n",
      "Processing question 40/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 41/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 42/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 43/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 44/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 45/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
      "Processing question 46/100...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
      "Processing question 47/100...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
      "Processing question 48/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 49/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
      "Processing question 50/100...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¬\n",
      "Processing question 51/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 52/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 53/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 54/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 55/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 56/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 57/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
      "Processing question 58/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
      "Processing question 59/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 60/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 61/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
      "Processing question 62/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 63/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
      "Processing question 64/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 65/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 66/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 67/100...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¨\n",
      "Processing question 68/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¬\n",
      "Processing question 69/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 70/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 71/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 72/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 73/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 74/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¯\n",
      "Processing question 75/100...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
      "Processing question 76/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 77/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 78/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 79/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¬\n",
      "Processing question 80/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 81/100...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
      "Processing question 82/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 83/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 84/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 85/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 86/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 87/100...\n",
      "  -> Ground Truth: Ø¥ | Model's Prediction: Ø¯\n",
      "Processing question 88/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¨\n",
      "Processing question 89/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 90/100...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
      "Processing question 91/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¬\n",
      "Processing question 92/100...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¯\n",
      "Processing question 93/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 94/100...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¨\n",
      "Processing question 95/100...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø¬\n",
      "Processing question 96/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
      "Processing question 97/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 98/100...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¬\n",
      "Processing question 99/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "Processing question 100/100...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø¨\n",
      "\n",
      "Successfully generated predictions and saved them to '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices.csv'.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_82b9ff1d-9565-4b16-99d4-e10e07fe4e73\", \"predictions_fitb_choices.csv\", 300)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸš€ Starting Evaluation...\n",
      "==================================================\n",
      "Total Questions Attempted: 100\n",
      "API Errors/Failed Attempts: 0\n",
      "Valid Predictions to Evaluate: 100\n",
      "--------------------\n",
      "Correct Predictions: 69 / 100\n",
      "ðŸ“Š Accuracy (on valid responses): 69.00%\n",
      "==================================================\n",
      "âœ… Evaluation Complete.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import groq\n",
    "import time\n",
    "from getpass import getpass\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Groq API Configuration ---\n",
    "# Your Groq API key will be accessed securely from Colab's secrets\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
    "except (ImportError, KeyError):\n",
    "    print(\"Secret 'GROQ_API_KEY' not found. Please add it to the Colab secrets manager.\")\n",
    "    GROQ_API_KEY = getpass('Please enter your Groq API key: ')\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = groq.Client(api_key=GROQ_API_KEY)\n",
    "\n",
    "\n",
    "# --- File paths and column names for the MCQ task ---\n",
    "# NOTE: The input CSV name suggests a \"fill-in-the-blank\" task, but the code logic\n",
    "# is designed for Multiple Choice. Ensure this is the correct file.\n",
    "INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/fill-in-the-blank-choices.csv'\n",
    "OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices.csv'\n",
    "\n",
    "# --- UPDATED: Correct column names as per your specification ---\n",
    "QUESTION_COLUMN = 'Question - Arabic'\n",
    "ANSWER_COLUMN = 'Answer - Arabic'\n",
    "\n",
    "\n",
    "# --- Function to Generate Answers for MCQ Task ---\n",
    "def generate_answer(question):\n",
    "    \"\"\"\n",
    "    Sends an MCQ question to the Groq API, prompting the model\n",
    "    to return only the single correct letter.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are a medical exam answering machine. Your only task is to answer the following multiple-choice medical question. Read the question and the provided options (Ø£, Ø¨, Ø¬, Ø¯, Ù‡). Your response must be ONLY the single Arabic letter corresponding to the correct answer. Do not provide any explanation, reasoning, or any other text. For example, if option 'Ø¨' is correct, your entire response must be 'Ø¨'.\"\"\"\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "              messages=[\n",
    "                  {\"role\":\"system\", \"content\": system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":question}\n",
    "              ],\n",
    "              model=\"llama3-70b-8192\", # Using a standard available model\n",
    "              temperature=0.0,\n",
    "              max_tokens=5,\n",
    "            )\n",
    "            response_text = chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "            # Clean the response to ensure it's just a single Arabic letter\n",
    "            arabic_letters = re.findall(r'[\\u0621-\\u064A]', response_text)\n",
    "            if arabic_letters:\n",
    "                return arabic_letters[0]\n",
    "            else:\n",
    "                print(f\"  -> Warning: Model returned an unexpected response: '{response_text}'. Recording as empty.\")\n",
    "                return \"\" # Return empty if no letter is found\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"  -> An error occurred: {e}. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"  -> API Error after multiple retries: {e}\")\n",
    "                return \"API_ERROR\"\n",
    "    return \"FAILED_ATTEMPTS\"\n",
    "\n",
    "\n",
    "# --- Function to Evaluate MCQ Accuracy ---\n",
    "def evaluate_mcq_accuracy(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculates and prints the accuracy of the MCQ predictions.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸš€ Starting Evaluation...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if len(predictions) != len(ground_truths):\n",
    "        print(\"Warning: Prediction and ground truth lists have different lengths. Evaluation might be inaccurate.\")\n",
    "        min_len = min(len(predictions), len(ground_truths))\n",
    "        predictions = predictions[:min_len]\n",
    "        ground_truths = ground_truths[:min_len]\n",
    "\n",
    "    # Filter out API errors before calculating accuracy\n",
    "    valid_indices = [i for i, p in enumerate(predictions) if p not in [\"API_ERROR\", \"FAILED_ATTEMPTS\"]]\n",
    "    valid_predictions = [predictions[i] for i in valid_indices]\n",
    "    valid_ground_truths = [ground_truths[i] for i in valid_indices]\n",
    "\n",
    "    if not valid_predictions:\n",
    "        print(\"No valid predictions to evaluate. Check for widespread API errors.\")\n",
    "        return\n",
    "\n",
    "    accuracy = accuracy_score(valid_ground_truths, valid_predictions)\n",
    "    correct_predictions = sum(p == g for p, g in zip(valid_predictions, valid_ground_truths))\n",
    "    total_valid_predictions = len(valid_predictions)\n",
    "    total_questions = len(ground_truths)\n",
    "    api_errors = total_questions - total_valid_predictions\n",
    "\n",
    "\n",
    "    print(f\"Total Questions Attempted: {total_questions}\")\n",
    "    print(f\"API Errors/Failed Attempts: {api_errors}\")\n",
    "    print(f\"Valid Predictions to Evaluate: {total_valid_predictions}\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Correct Predictions: {correct_predictions} / {total_valid_predictions}\")\n",
    "    print(f\"ðŸ“Š Accuracy (on valid responses): {accuracy * 100:.2f}%\")\n",
    "    print(\"=\"*50 + \"\\nâœ… Evaluation Complete.\\n\" + \"=\"*50)\n",
    "\n",
    "\n",
    "# --- Main Execution for MCQ Task ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for the Multiple Choice Question Answering task.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use encoding='utf-8' to handle Arabic characters properly\n",
    "        df = pd.read_csv(INPUT_CSV, encoding='utf-8')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{INPUT_CSV}' was not found. Please check the path.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    # Check if the corrected column names exist in the DataFrame\n",
    "    if QUESTION_COLUMN not in df.columns or ANSWER_COLUMN not in df.columns:\n",
    "        print(f\"Error: Required columns ('{QUESTION_COLUMN}', '{ANSWER_COLUMN}') not found in the CSV.\")\n",
    "        print(f\"Available columns are: {df.columns.tolist()}\")\n",
    "        return\n",
    "\n",
    "    # Drop rows where the question or answer is missing\n",
    "    df.dropna(subset=[QUESTION_COLUMN, ANSWER_COLUMN], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        print(f\"âœ… Found existing prediction file: '{OUTPUT_CSV}'.\")\n",
    "        print(\"Skipping generation and loading predictions from file for evaluation.\")\n",
    "        predictions_df = pd.read_csv(OUTPUT_CSV, header=None, encoding='utf-8')\n",
    "        predictions = predictions_df[0].astype(str).tolist() # Ensure predictions are read as strings\n",
    "    else:\n",
    "        print(f\"'{OUTPUT_CSV}' not found. Starting prediction generation process...\")\n",
    "        predictions = []\n",
    "        total_questions = len(df)\n",
    "        for index, row in df.iterrows():\n",
    "            question = row[QUESTION_COLUMN]\n",
    "            print(f\"Processing question {index + 1}/{total_questions}...\")\n",
    "            answer_letter = generate_answer(question)\n",
    "            predictions.append(answer_letter)\n",
    "            print(f\"  -> Ground Truth: {str(row[ANSWER_COLUMN]).strip()[0]} | Model's Prediction: {answer_letter}\")\n",
    "\n",
    "            # Delay for Groq API rate limit (30 RPM limit)\n",
    "            if index < total_questions - 1:\n",
    "                time.sleep(2.1) # Sleep for slightly over 2 seconds\n",
    "\n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "        predictions_df.to_csv(OUTPUT_CSV, header=False, index=False, encoding='utf-8')\n",
    "        print(f\"\\nSuccessfully generated predictions and saved them to '{OUTPUT_CSV}'.\")\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            files.download(OUTPUT_CSV)\n",
    "        except ImportError:\n",
    "            print(f\"To download '{OUTPUT_CSV}', see the file browser on the left.\")\n",
    "\n",
    "    # Extracting the first character from the Answer column as the ground truth\n",
    "    ground_truths = [str(ans).strip()[0] for ans in df[ANSWER_COLUMN].tolist()]\n",
    "\n",
    "    evaluate_mcq_accuracy(predictions, ground_truths)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JwGSfAcaW2F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRcBmcc5aW9c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v4YX7HBaXGe"
   },
   "source": [
    "\n",
    "# Track 2: MedArabiQ 2025 (General Health)\n",
    "\n",
    "\n",
    "# Sub-Task 1: Fill-in-the-blank with Choices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fill in blank deepseek-r1-distill-llama-70b -> 17%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdvDDWDfaZ0H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ePB6zbLHaZ94",
    "outputId": "498c13ef-0df1-4032-e1a1-fb1d3a0b41a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek.csv' not found. Starting prediction generation process with DeepSeek...\n",
      "Processing question 1/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 2/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø§\n",
      "Processing question 3/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Okay, so I've got this medical question here about pleural effusion. I'm a bit rusty on my respiratory system, but let me try to think this through.\n",
      "\n",
      "The question is about filling in the blanks in a sentence. It says that in cases of pleural effusion, the decrease or absence of tactile fremitus on the affected side indicates something, which is caused by something else.\n",
      "\n",
      "First, I need to remember what pleural effusion is. From what I recall, it's when there's an abnormal accumulation of fluid in the pleural space, which is the area between the lungs and the chest wall. This fluid buildup can be due to various reasons like infections, heart failure, or even cancer.\n",
      "\n",
      "Now,'. Recording as empty.\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: \n",
      "Processing question 4/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Okay, so I've got this medical question here, and I need to figure it out step by step. Let me read it again carefully.\n",
      "\n",
      "The question is about iron transport and storage in the body. It says there are three main categories of proteins involved. The first two are Transferrins and Transferrin receptors (TFR1). The third one is missing, and I need to choose the correct option from the given choices. Also, the second part of the question asks how many iron ions each transferrin molecule can bind.\n",
      "\n",
      "Alright, I remember that iron metabolism involves several proteins. Transferrin is the main protein that transports iron in the blood. The receptors, like TFR1, help cells take'. Recording as empty.\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: \n",
      "Processing question 5/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
      "Processing question 6/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: ÙŠ\n",
      "Processing question 7/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 8/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
      "Processing question 9/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: ÙŠ\n",
      "Processing question 10/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: ÙŠ\n",
      "Processing question 11/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø­\n",
      "Processing question 12/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Øª\n",
      "Processing question 13/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Okay, so I've got this question about Guillain-BarrÃ© Syndrome (GBS) and cerebrospinal fluid (CSF) findings. I'm a bit rusty on my neurology, but let me think this through.\n",
      "\n",
      "First, I remember that GBS is an autoimmune disorder where the body's immune system attacks the peripheral nerves. This leads to muscle weakness, sometimes paralysis, and other neurological symptoms. Now, the question is about what's found in the CSF in GBS patients.\n",
      "\n",
      "I recall that in GBS, the CSF analysis typically shows an elevated protein level without a significant increase in white blood cell count. This is often referred to as \"albuminocytological dissociation,\" where the protein is'. Recording as empty.\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: \n",
      "Processing question 14/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 15/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Øª\n",
      "Processing question 16/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Okay, so I've got this medical question here about ECG findings in myocarditis. Hmm, I'm a bit rusty on this, but let me think through it.\n",
      "\n",
      "The question is asking about the classic ECG results in myocarditis and what commonly accompanies them. The sentence has two blanks, so I need to figure out both parts.\n",
      "\n",
      "First, I remember that myocarditis is inflammation of the heart muscle, often due to viral infections. ECG changes can vary, but there are some classic signs. One common finding is the presence of ST-segment elevation. That's because the inflammation can cause the heart's cells to become irritated, leading to changes in the electrical activity, which shows up as ST elevation.\n",
      "\n",
      "Another thing I recall'. Recording as empty.\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: \n",
      "Processing question 17/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 18/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 19/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Okay, so I've got this medical question here about Systemic Lupus Erythematosus (SLE), and I need to fill in the blanks. Let me think through this step by step.\n",
      "\n",
      "First, the question is in Arabic, but I can translate it to understand better. It says: \"In systemic lupus erythematosus, the immune system produces antibodies known as _____, which target nuclear components. The disease can be diagnosed using tests to detect antibodies against _____ using immunological methods.\"\n",
      "\n",
      "So, the first blank is about the type of antibodies produced, and the second is about what those antibodies target.\n",
      "\n",
      "I remember that in SLE, the immune system mistakenly attacks the body's own tissues, particularly the nucleus of'. Recording as empty.\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: \n",
      "Processing question 20/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Øª\n",
      "Processing question 21/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
      "Processing question 22/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 23/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Okay, so I've got this medical question here about peripheral neuropathy caused by vitamin B6 toxicity. Hmm, I need to figure out the correct answer. Let me think through this step by step.\n",
      "\n",
      "First, I remember that vitamin B6, or pyridoxine, is a B complex vitamin. It's important for brain function and helping the body make the hormones serotonin and norepinephrine. But too much of it can be a problem. I've heard that excessive intake can lead to toxicity, which affects the nervous system.\n",
      "\n",
      "Peripheral neuropathy is a common side effect of vitamin B6 toxicity. So, what does that neuropathy present like? I think peripheral neuropathy can cause various symptoms depending on which nerves are'. Recording as empty.\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: \n",
      "Processing question 24/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 25/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
      "Processing question 26/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 27/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 28/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
      "Processing question 29/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¨\n",
      "Processing question 30/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Okay, so I've got this question about thalassemia syndromes. I'm a bit rusty on my hematology, but let me think through this.\n",
      "\n",
      "The question is asking about the types of globin chain deficiencies that cause thalassemia. The sentence says it's a group of genetic disorders resulting from a deficiency in the synthesis of globin chains, and the blanks are for the types of chains involved.\n",
      "\n",
      "I remember that globin chains are a key part of hemoglobin. There are different types, like alpha, beta, gamma, and delta. Thalassemia is classified based on which chains are affected.\n",
      "\n",
      "The main types I recall are alpha-thalassemia and beta-thalas'. Recording as empty.\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: \n",
      "Processing question 31/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
      "Processing question 32/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 33/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Øª\n",
      "Processing question 34/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 35/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Øª\n",
      "Processing question 36/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ù…\n",
      "Processing question 37/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: ÙŠ\n",
      "Processing question 38/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
      "Processing question 39/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
      "Processing question 40/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 41/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: ÙŠ\n",
      "Processing question 42/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 43/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 44/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 45/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 46/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ù…\n",
      "Processing question 47/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
      "Processing question 48/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ù\n",
      "Processing question 49/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Øª\n",
      "Processing question 50/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Øª\n",
      "Processing question 51/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
      "Processing question 52/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 53/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 54/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 55/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: ÙŠ\n",
      "Processing question 56/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 57/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 58/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 59/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 60/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Okay, so I've got this medical question here about the D-Dimer test. I'm a bit rusty on this, but I'll try to think it through. The question is about filling in the blank in a sentence. The sentence says that a positive D-Dimer test indicates a possible blood clot, and a negative test means what?\n",
      "\n",
      "First, I remember that D-Dimer is a blood test used to help diagnose or rule out thrombotic disorders. Thrombotic disorders include things like deep vein thrombosis (DVT) or pulmonary embolism (PE). The D-Dimer test measures the levels of a protein fragment called D-Dimer, which is produced when a blood clot dissolves. So, if'. Recording as empty.\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: \n",
      "Processing question 61/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 62/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 63/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Okay, so I've got this medical question here about the corneal layer dissolution. Hmm, I remember that the cornea is the clear part at the front of the eye, right? And when it gets infected, it can lead to something called corneal ulcers or maybe keratitis. \n",
      "\n",
      "The question is asking about the pathogens responsible for this condition. The options are pairs of bacteria. Let me think about the common causes of corneal infections. I remember that Staphylococcus and Streptococcus are pretty common in eye infections. They're both Gram-positive cocci. \n",
      "\n",
      "Wait, but I also recall that Pseudomonas is a big player in corneal ulcers, especially in contact lens'. Recording as empty.\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: \n",
      "Processing question 64/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Okay, so I have this medical question to answer, and I need to figure it out step by step. The question is about Erb-Duchenne palsy, which I remember is a type of brachial plexus injury. The question is asking which nerve roots are involved in this condition.\n",
      "\n",
      "First, I should recall what Erb-Duchenne palsy entails. From what I remember, it's the most common type of obstetric brachial plexus injury. It usually happens during childbirth when the baby's shoulder gets stuck during delivery, causing the nerves in the brachial plexus to stretch or tear.\n",
      "\n",
      "The brachial plexus is a network of nerves that originates in the spinal cord'. Recording as empty.\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: \n",
      "Processing question 65/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø¥\n",
      "Processing question 66/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Okay, so I've got this medical question here about ketoacidosis in diabetes. Hmm, I remember that ketoacidosis is a serious complication, especially in type 1 diabetes. It happens when the body doesn't have enough insulin, so it starts breaking down fat for energy instead of glucose. \n",
      "\n",
      "Wait, when the body breaks down fat, it produces ketones. Oh right, ketones are acidic, so if they build up, the blood becomes too acidic, leading to ketoacidosis. The question is asking which substance accumulates to cause this.\n",
      "\n",
      "Looking at the options: \n",
      "\n",
      "A is lactic acid. I know lactic acid can build up during intense exercise or in certain conditions, causing lactic acidosis, but'. Recording as empty.\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: \n",
      "Processing question 67/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Øº\n",
      "Processing question 68/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 69/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
      "Processing question 70/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
      "Processing question 71/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø¯\n",
      "Processing question 72/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
      "Processing question 73/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
      "Processing question 74/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: ÙŠ\n",
      "Processing question 75/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
      "Processing question 76/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ù\n",
      "Processing question 77/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 78/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 79/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 80/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 81/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø§\n",
      "Processing question 82/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 83/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø®\n",
      "Processing question 84/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Okay, so I have this medical question to answer, and I need to figure it out step by step. The question is about confirming the diagnosis of an insulinoma, which I remember is a type of tumor that produces insulin. The sentence to fill in is about the blood glucose and insulin levels during a fasting episode.\n",
      "\n",
      "First, I need to recall the normal and abnormal ranges for blood glucose and insulin. I know that normal fasting blood glucose is usually between 70 to 100 mg/dL. When someone is diagnosed with an insulinoma, they have episodes of hypoglycemia because the tumor secretes insulin inappropriately, even when blood glucose is low.\n",
      "\n",
      "So during a hypoglycemic episode caused by insulinoma'. Recording as empty.\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: \n",
      "Processing question 85/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 86/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ù\n",
      "Processing question 87/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Alright, so I've got this medical question here about the treatment for corneal ulcers. The question is in Arabic, and I need to figure out the correct answer. Let me break it down step by step.\n",
      "\n",
      "The question is asking about the topical treatment for corneal ulcers, specifically what medication is used. The options are erythromycin, penicillin, ciprofloxacin, and tetracycline.\n",
      "\n",
      "First, I remember that corneal ulcers can be caused by different types of infectionsâ€”bacterial, viral, or fungal. The treatment depends on the cause. Since the question is about topical treatment, I'm focusing on eye drops or ointments.\n",
      "\n",
      "Erythromycin'. Recording as empty.\n",
      "  -> Ground Truth: Ø¥ | Model's Prediction: \n",
      "Processing question 88/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
      "Processing question 89/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ø£\n",
      "Processing question 90/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ù\n",
      "Processing question 91/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: Ù\n",
      "Processing question 92/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: Ø£\n",
      "Processing question 93/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ù…\n",
      "Processing question 94/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¬ | Model's Prediction: ÙŠ\n",
      "Processing question 95/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Warning: No Arabic letter found after cleaning. Cleaned response: '<think>\n",
      "Okay, so I have this medical question to answer, and I need to figure it out step by step. The question is about the diagnosis of exudative pleural effusion using Light's criteria, specifically focusing on the LDH levels in pleural fluid compared to serum.\n",
      "\n",
      "First, I remember that Light's criteria are used to differentiate between exudative and transudative pleural effusions. Exudative effusions are typically caused by inflammatory or malignant processes, while transudative are due to fluid overload or other non-inflammatory conditions.\n",
      "\n",
      "The criteria mention that for a fluid to be considered exudative, it must meet at least one of the following: the pleural fluid protein to serum protein ratio is greater than 0'. Recording as empty.\n",
      "  -> Ground Truth: Ø¯ | Model's Prediction: \n",
      "Processing question 96/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø£\n",
      "Processing question 97/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ù\n",
      "Processing question 98/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø£ | Model's Prediction: Ø§\n",
      "Processing question 99/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: Ø£\n",
      "Processing question 100/100 with DeepSeek (MCQ Mode)...\n",
      "  -> Ground Truth: Ø¨ | Model's Prediction: ÙŠ\n",
      "\n",
      "Successfully generated predictions and saved them to '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek.csv'.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_00578c14-5cc4-46e0-a42e-609ee44ec442\", \"predictions_fitb_choices_deepseek.csv\", 300)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸš€ Starting Evaluation...\n",
      "==================================================\n",
      "Total Questions Attempted: 100\n",
      "API Errors/Failed Attempts: 0\n",
      "Valid Predictions to Evaluate: 100\n",
      "--------------------\n",
      "Correct Predictions: 17 / 100\n",
      "ðŸ“Š Accuracy (on valid responses): 17.00%\n",
      "==================================================\n",
      "âœ… Evaluation Complete.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import groq\n",
    "import time\n",
    "from getpass import getpass\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Groq API Configuration ---\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
    "except (ImportError, KeyError):\n",
    "    print(\"Secret 'GROQ_API_KEY' not found. Please add it to the Colab secrets manager.\")\n",
    "    GROQ_API_KEY = getpass('Please enter your Groq API key: ')\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = groq.Client(api_key=GROQ_API_KEY)\n",
    "\n",
    "\n",
    "# --- File paths and column names for the MCQ task ---\n",
    "INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/fill-in-the-blank-choices.csv'\n",
    "OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/predictions_fitb_choices_deepseek.csv'\n",
    "\n",
    "# Column names from your CSV\n",
    "QUESTION_COLUMN = 'Question - Arabic'\n",
    "ANSWER_COLUMN = 'Answer - Arabic'\n",
    "\n",
    "\n",
    "# --- Function to Generate Answers for MCQ Task using DeepSeek ---\n",
    "def generate_answer(question):\n",
    "    \"\"\"\n",
    "    Sends an MCQ question to the Groq API, prompting the DeepSeek model\n",
    "    to return only the single correct letter. Includes cleaning for <think> blocks.\n",
    "    \"\"\"\n",
    "    # --- UPDATED: Added an explicit instruction to not use <think> tags ---\n",
    "    system_prompt = \"\"\"You are a medical exam answering machine. Your only task is to answer the following multiple-choice medical question. Read the question and the provided options (Ø£, Ø¨, Ø¬, Ø¯, Ù‡). Your response must be ONLY the single Arabic letter corresponding to the correct answer. Do not provide any explanation, reasoning, or any other text. Do not use <think> tags. For example, if option 'Ø¨' is correct, your entire response must be 'Ø¨'.\"\"\"\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_delay = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "              messages=[\n",
    "                  {\"role\":\"system\", \"content\": system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":question}\n",
    "              ],\n",
    "              model=\"deepseek-r1-distill-llama-70b\",\n",
    "              temperature=0.0,\n",
    "              # --- UPDATED: Increased max_tokens significantly to prevent cutoff ---\n",
    "              max_tokens=150,\n",
    "            )\n",
    "            raw_response_text = chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "            # The cleaning logic is now a robust failsafe.\n",
    "            # It will work correctly now that the model has enough tokens to provide a complete response.\n",
    "            cleaned_text = re.sub(r'<think>.*?</think>', '', raw_response_text, flags=re.DOTALL).strip()\n",
    "            arabic_letters = re.findall(r'[\\u0621-\\u064A]', cleaned_text)\n",
    "\n",
    "            if arabic_letters:\n",
    "                return arabic_letters[0]\n",
    "            else:\n",
    "                print(f\"  -> Warning: No Arabic letter found after cleaning. Cleaned response: '{cleaned_text}'. Recording as empty.\")\n",
    "                return \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"  -> An error occurred: {e}. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"  -> API Error after multiple retries: {e}\")\n",
    "                return \"API_ERROR\"\n",
    "    return \"FAILED_ATTEMPTS\"\n",
    "\n",
    "\n",
    "# --- Function to Evaluate MCQ Accuracy (Unchanged) ---\n",
    "def evaluate_mcq_accuracy(predictions, ground_truths):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸš€ Starting Evaluation...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if len(predictions) != len(ground_truths):\n",
    "        print(\"Warning: Prediction and ground truth lists have different lengths. Evaluation might be inaccurate.\")\n",
    "        min_len = min(len(predictions), len(ground_truths))\n",
    "        predictions = predictions[:min_len]\n",
    "        ground_truths = ground_truths[:min_len]\n",
    "\n",
    "    valid_indices = [i for i, p in enumerate(predictions) if p not in [\"API_ERROR\", \"FAILED_ATTEMPTS\"]]\n",
    "    valid_predictions = [predictions[i] for i in valid_indices]\n",
    "    valid_ground_truths = [ground_truths[i] for i in valid_indices]\n",
    "\n",
    "    if not valid_predictions:\n",
    "        print(\"No valid predictions to evaluate. Check for widespread API errors.\")\n",
    "        return\n",
    "\n",
    "    accuracy = accuracy_score(valid_ground_truths, valid_predictions)\n",
    "    correct_predictions = sum(p == g for p, g in zip(valid_predictions, valid_ground_truths))\n",
    "    total_valid_predictions = len(valid_predictions)\n",
    "    total_questions = len(ground_truths)\n",
    "    api_errors = total_questions - total_valid_predictions\n",
    "\n",
    "    print(f\"Total Questions Attempted: {total_questions}\")\n",
    "    print(f\"API Errors/Failed Attempts: {api_errors}\")\n",
    "    print(f\"Valid Predictions to Evaluate: {total_valid_predictions}\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Correct Predictions: {correct_predictions} / {total_valid_predictions}\")\n",
    "    print(f\"ðŸ“Š Accuracy (on valid responses): {accuracy * 100:.2f}%\")\n",
    "    print(\"=\"*50 + \"\\nâœ… Evaluation Complete.\\n\" + \"=\"*50)\n",
    "\n",
    "\n",
    "# --- Main Execution for MCQ Task (Unchanged) ---\n",
    "def main():\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV, encoding='utf-8')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{INPUT_CSV}' was not found. Please check the path.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    if QUESTION_COLUMN not in df.columns or ANSWER_COLUMN not in df.columns:\n",
    "        print(f\"Error: Required columns ('{QUESTION_COLUMN}', '{ANSWER_COLUMN}') not found in the CSV.\")\n",
    "        print(f\"Available columns are: {df.columns.tolist()}\")\n",
    "        return\n",
    "\n",
    "    df.dropna(subset=[QUESTION_COLUMN, ANSWER_COLUMN], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        print(f\"âœ… Found existing prediction file: '{OUTPUT_CSV}'.\")\n",
    "        print(\"Skipping generation and loading predictions from file for evaluation.\")\n",
    "        predictions_df = pd.read_csv(OUTPUT_CSV, header=None, encoding='utf-8')\n",
    "        predictions = predictions_df[0].astype(str).tolist()\n",
    "    else:\n",
    "        print(f\"'{OUTPUT_CSV}' not found. Starting prediction generation process with DeepSeek...\")\n",
    "        predictions = []\n",
    "        total_questions = len(df)\n",
    "        for index, row in df.iterrows():\n",
    "            question = row[QUESTION_COLUMN]\n",
    "            print(f\"Processing question {index + 1}/{total_questions} with DeepSeek (MCQ Mode)...\")\n",
    "            answer_letter = generate_answer(question)\n",
    "            predictions.append(answer_letter)\n",
    "            print(f\"  -> Ground Truth: {str(row[ANSWER_COLUMN]).strip()[0]} | Model's Prediction: {answer_letter}\")\n",
    "\n",
    "            if index < total_questions - 1:\n",
    "                time.sleep(2.1)\n",
    "\n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "        predictions_df.to_csv(OUTPUT_CSV, header=False, index=False, encoding='utf-8')\n",
    "        print(f\"\\nSuccessfully generated predictions and saved them to '{OUTPUT_CSV}'.\")\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            files.download(OUTPUT_CSV)\n",
    "        except ImportError:\n",
    "            print(f\"To download '{OUTPUT_CSV}', see the file browser on the left.\")\n",
    "\n",
    "    ground_truths = [str(ans).strip()[0] for ans in df[ANSWER_COLUMN].tolist()]\n",
    "    evaluate_mcq_accuracy(predictions, ground_truths)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWKfKpOfUbYW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asBIJnsEUbcG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMXd_qZiUbgd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
