# --- Step 1: Install all necessary libraries ---
# This needs to be run once to install the required packages for the model.
!pip install -q -U transformers bitsandbytes accelerate Pillow

# --- Step 2: Import libraries ---
import os
import re
import time
import pandas as pd
import torch
from transformers import pipeline

# --- Step 3: Initialize the MedGemma Model ---
# This will download the model (over 50GB) the first time it's run.
# It requires a GPU runtime in Google Colab.
print("Initializing the MedGemma pipeline...")
try:
    pipe = pipeline(
        "image-text-to-text",
        model="google/medgemma-27b-it",
        torch_dtype=torch.bfloat16,
        device_map="auto", # Automatically use the available GPU
        # The following can help reduce memory usage if you encounter CUDA errors
        # load_in_4bit=True,
    )
    print("‚úÖ Pipeline initialized successfully.")
except Exception as e:
    print(f"‚ùå Failed to initialize pipeline. Ensure you are using a GPU runtime.")
    print(f"Error: {e}")
    # Stop execution if the model can't be loaded
    raise

# --- Step 4: File Paths and Column Names ---
INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/multiple-choice-questions.csv'
# Updated output file name for the MedGemma model
OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/mcq/predictions_medgemma_mcq.csv'
QUESTION_COLUMN = 'Question'

# --- Step 5: Prepare Few-Shot Examples for MedGemma ---
# The message format is specific to this model.
FEW_SHOT_MESSAGES = [
    {
        "role": "system",
        "content": [{"type": "text", "text": "You are a helpful medical assistant. Your task is to answer multiple-choice medical questions. Provide a step-by-step reasoning process and then state the final answer clearly."}]
    },
    {
        "role": "user",
        "content": [{"type": "text", "text": """ÿßŸÖŸÑÿ£ ÿßŸÑŸÅÿ±ÿßÿ∫ÿßÿ™ ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© ÿßŸÑÿ™ÿßŸÑŸäÿ©:
ŸÅŸä ÿ≠ÿßŸÑÿ© ÿßŸÑÿßŸÜÿµÿ®ÿßÿ® ÿßŸÑÿ¨ŸÜÿ®Ÿäÿå Ÿäÿ¥Ÿäÿ± ÿßŸÜÿÆŸÅÿßÿ∂ ÿ£Ÿà ÿ∫Ÿäÿßÿ® ÿßŸÑÿ±ÿ¨ŸÅÿßŸÜ ÿßŸÑŸÑŸÖÿ≥Ÿä ÿπŸÑŸâ ÿßŸÑÿ¨ÿßŸÜÿ® ÿßŸÑŸÖÿµÿßÿ® ÿ•ŸÑŸâ ____ÿå ŸàÿßŸÑÿ∞Ÿä ŸäŸÜÿ™ÿ¨ ÿπŸÜ ____.
ÿ£. ÿ™ÿ±ÿßŸÉŸÖ ÿßŸÑÿ≥Ÿàÿßÿ¶ŸÑÿõ ÿπÿ≤ŸÑ ÿßŸÑÿµŸàÿ™
ÿ®. ÿßÿ≠ÿ™ÿ®ÿßÿ≥ ÿßŸÑŸáŸàÿßÿ°ÿõ ÿßŸÜŸáŸäÿßÿ± ÿßŸÑÿ≠ŸàŸäÿµŸÑÿßÿ™ ÿßŸÑŸáŸàÿßÿ¶Ÿäÿ©
ÿ¨. ÿßŸÑÿ™ŸÑŸäŸÅÿõ ÿßŸÜÿÆŸÅÿßÿ∂ ŸÖÿ±ŸàŸÜÿ© ÿßŸÑÿ±ÿ¶ÿ©
ÿØ. ŸÜŸÖŸà ÿßŸÑŸàÿ±ŸÖÿõ ÿßŸÜÿ≥ÿØÿßÿØ ÿßŸÑÿ¥ÿπÿ® ÿßŸÑŸáŸàÿßÿ¶Ÿäÿ©"""}]
    },
    {
        "role": "assistant",
        "content": [{"type": "text", "text": """**ÿßŸÑÿ™ŸÅŸÉŸäÿ± ÿÆÿ∑Ÿàÿ© ÿ®ÿÆÿ∑Ÿàÿ©:**
1.  **ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ≥ÿ§ÿßŸÑ**: Ÿäÿ≥ÿ£ŸÑ ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿπŸÜ ÿØŸÑÿßŸÑÿ© ÿßŸÜÿÆŸÅÿßÿ∂ ÿ£Ÿà ÿ∫Ÿäÿßÿ® "ÿßŸÑÿ±ÿ¨ŸÅÿßŸÜ ÿßŸÑŸÑŸÖÿ≥Ÿä" (tactile fremitus) ŸÅŸä ÿ≠ÿßŸÑÿ© "ÿßŸÑÿßŸÜÿµÿ®ÿßÿ® ÿßŸÑÿ¨ŸÜÿ®Ÿä" (pleural effusion) ŸàÿπŸÜ ÿßŸÑÿ¢ŸÑŸäÿ© ÿßŸÑŸÖÿ≥ÿ®ÿ®ÿ© ŸÑÿ∞ŸÑŸÉ. ÿßŸÑÿ±ÿ¨ŸÅÿßŸÜ ÿßŸÑŸÑŸÖÿ≥Ÿä ŸáŸà ÿßŸÑÿßŸáÿ™ÿ≤ÿßÿ≤ ÿßŸÑÿ∞Ÿä ŸäŸÖŸÉŸÜ ÿßŸÑÿ¥ÿπŸàÿ± ÿ®Ÿá ÿπŸÑŸâ ÿ¨ÿØÿßÿ± ÿßŸÑÿµÿØÿ± ÿ£ÿ´ŸÜÿßÿ° ÿßŸÑŸÉŸÑÿßŸÖ.
2.  **ÿ™ŸÇŸäŸäŸÖ ÿßŸÑÿÆŸäÿßÿ±ÿßÿ™**:
    * **ÿ£. ÿ™ÿ±ÿßŸÉŸÖ ÿßŸÑÿ≥Ÿàÿßÿ¶ŸÑÿõ ÿπÿ≤ŸÑ ÿßŸÑÿµŸàÿ™**: ÿßŸÑÿßŸÜÿµÿ®ÿßÿ® ÿßŸÑÿ¨ŸÜÿ®Ÿä ŸáŸà ÿ®ÿßŸÑŸÅÿπŸÑ ÿ™ÿ±ÿßŸÉŸÖ ŸÑŸÑÿ≥Ÿàÿßÿ¶ŸÑ ŸÅŸä ÿßŸÑÿ∫ÿ¥ÿßÿ° ÿßŸÑÿ¨ŸÜÿ®Ÿä. Ÿáÿ∞ÿß ÿßŸÑÿ≥ÿßÿ¶ŸÑ ŸäÿπŸÖŸÑ ŸÉÿπÿßÿ≤ŸÑÿå ŸÖŸÖÿß ŸäŸÖŸÜÿπ ÿßŸÜÿ™ŸÇÿßŸÑ ÿßŸáÿ™ÿ≤ÿßÿ≤ÿßÿ™ ÿßŸÑÿµŸàÿ™ ŸÖŸÜ ÿßŸÑÿ±ÿ¶ÿ© ÿ•ŸÑŸâ ÿ¨ÿØÿßÿ± ÿßŸÑÿµÿØÿ±. Ÿáÿ∞ÿß Ÿäÿ™ÿ∑ÿßÿ®ŸÇ ÿ™ŸÖÿßŸÖŸãÿß ŸÖÿπ –Ω–∞—Ö–æ–¥ÿ© ÿßŸÜÿÆŸÅÿßÿ∂ ÿßŸÑÿ±ÿ¨ŸÅÿßŸÜ ÿßŸÑŸÑŸÖÿ≥Ÿä.
    * **ÿ®. ÿßÿ≠ÿ™ÿ®ÿßÿ≥ ÿßŸÑŸáŸàÿßÿ°ÿõ ÿßŸÜŸáŸäÿßÿ± ÿßŸÑÿ≠ŸàŸäÿµŸÑÿßÿ™ ÿßŸÑŸáŸàÿßÿ¶Ÿäÿ©**: Ÿáÿ∞ÿß ŸäÿµŸÅ ÿ≠ÿßŸÑÿ© ÿßÿ≥ÿ™ÿ±Ÿàÿßÿ≠ ÿßŸÑÿµÿØÿ± (pneumothorax) ÿ£Ÿà ÿßŸÜÿÆŸÖÿßÿµ ÿßŸÑÿ±ÿ¶ÿ© (atelectasis)ÿå ŸàÿßŸÑÿ™Ÿä ŸÑŸáÿß ŸÖŸàÿ¨ŸàÿØÿßÿ™ ŸÅŸäÿ≤Ÿäÿßÿ¶Ÿäÿ© ŸÖÿÆÿ™ŸÑŸÅÿ©.
    * **ÿ¨. ÿßŸÑÿ™ŸÑŸäŸÅÿõ ÿßŸÜÿÆŸÅÿßÿ∂ ŸÖÿ±ŸàŸÜÿ© ÿßŸÑÿ±ÿ¶ÿ©**: ÿßŸÑÿ™ŸÑŸäŸÅ ÿßŸÑÿ±ÿ¶ŸàŸä (Pulmonary fibrosis) Ÿäÿ≤ŸäÿØ ŸÖŸÜ ŸÉÿ´ÿßŸÅÿ© ÿ£ŸÜÿ≥ÿ¨ÿ© ÿßŸÑÿ±ÿ¶ÿ©ÿå ŸÖŸÖÿß ŸÇÿØ Ÿäÿ§ÿØŸä ÿ•ŸÑŸâ ÿ≤ŸäÿßÿØÿ© ÿßŸÑÿ±ÿ¨ŸÅÿßŸÜ ÿßŸÑŸÑŸÖÿ≥Ÿäÿå ŸàŸÑŸäÿ≥ ÿßŸÜÿÆŸÅÿßÿ∂Ÿá.
    * **ÿØ. ŸÜŸÖŸà ÿßŸÑŸàÿ±ŸÖÿõ ÿßŸÜÿ≥ÿØÿßÿØ ÿßŸÑÿ¥ÿπÿ® ÿßŸÑŸáŸàÿßÿ¶Ÿäÿ©**: ŸÇÿØ Ÿäÿ≥ÿ®ÿ® ÿßŸÑŸàÿ±ŸÖ ÿßŸÜÿµÿ®ÿßÿ®Ÿãÿß ÿ¨ŸÜÿ®ŸäŸãÿßÿå ŸÑŸÉŸÜ ÿßŸÑÿ≥ÿ®ÿ® ÿßŸÑŸÖÿ®ÿßÿ¥ÿ± ŸÑÿßŸÜÿÆŸÅÿßÿ∂ ÿßŸÑÿ±ÿ¨ŸÅÿßŸÜ ŸÅŸä Ÿáÿ∞Ÿá ÿßŸÑÿ≠ÿßŸÑÿ© ŸáŸà ÿßŸÑÿ≥ÿßÿ¶ŸÑ ŸÜŸÅÿ≥Ÿá ÿßŸÑÿ∞Ÿä Ÿäÿπÿ≤ŸÑ ÿßŸÑÿµŸàÿ™. ÿßŸÑÿÆŸäÿßÿ± "ÿ£" ŸäÿµŸÅ ÿßŸÑÿ¢ŸÑŸäÿ© ÿßŸÑŸÅŸäÿ≤Ÿäÿßÿ¶Ÿäÿ© ÿßŸÑŸÖÿ®ÿßÿ¥ÿ±ÿ© ÿ®ÿ¥ŸÉŸÑ ÿ£ŸÅÿ∂ŸÑ.
3.  **ÿßŸÑÿßÿ≥ÿ™ŸÜÿ™ÿßÿ¨**: ÿßŸÑÿÆŸäÿßÿ± ÿßŸÑÿ£ŸÉÿ´ÿ± ÿØŸÇÿ© ŸáŸà ÿ£ŸÜ ÿ™ÿ±ÿßŸÉŸÖ ÿßŸÑÿ≥Ÿàÿßÿ¶ŸÑ ŸáŸà ŸÖÿß Ÿäÿ≥ÿ®ÿ® ÿπÿ≤ŸÑ ÿßŸÑÿµŸàÿ™ÿå ŸÖŸÖÿß Ÿäÿ§ÿØŸä ÿ•ŸÑŸâ ÿßŸÜÿÆŸÅÿßÿ∂ ÿßŸÑÿ±ÿ¨ŸÅÿßŸÜ ÿßŸÑŸÑŸÖÿ≥Ÿä.

Final Answer: ÿ£"""}]
    }
]

def extract_and_normalize_answer(full_text):
    """
    Parses the full text from the model to find, normalize, and translate the final answer letter.
    """
    found_letter = None
    explicit_pattern = r"(?:Final Answer|ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑŸÜŸáÿßÿ¶Ÿäÿ©|ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ© ŸáŸä|ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ©|ÿßŸÑÿÆŸÑÿßÿµÿ©)\s*[:Ôºö]?\s*\**\s*([A-Ea-eÿ£-Ÿä])"
    match = re.search(explicit_pattern, full_text, re.IGNORECASE | re.MULTILINE)
    if match:
        found_letter = match.group(1)

    if not found_letter:
        line_pattern = r"^\s*\**([A-Ea-eÿ£-Ÿä])\.\s*.+"
        lines = full_text.splitlines()
        for line in reversed(lines):
            match = re.match(line_pattern, line.strip())
            if match:
                found_letter = match.group(1)
                break

    if not found_letter:
        lines = full_text.splitlines()
        for line in reversed(lines[-3:]):
            cleaned_line = line.strip().replace('*', '')
            if len(cleaned_line) == 1 and re.match(r"^[A-Ea-eÿ£-Ÿä]$", cleaned_line):
                found_letter = cleaned_line
                break

    if not found_letter:
        return "N/A"

    found_letter = found_letter.upper()
    translation_map = {'A': 'ÿ£', 'B': 'ÿ®', 'C': 'ÿ¨', 'D': 'ÿØ', 'E': 'Ÿá'}
    if found_letter in translation_map:
        found_letter = translation_map[found_letter]

    if found_letter in ['ÿß', 'ÿ•', 'ÿ¢', 'ÿ£']:
        found_letter = 'ÿ£'

    if found_letter in ['ÿ£', 'ÿ®', 'ÿ¨', 'ÿØ', 'Ÿá']:
        return found_letter
    else:
        return "Parse Error"

def get_full_reasoning(user_prompt):
    """
    Uses the local MedGemma pipeline to get the model's reasoning.
    """
    # Combine the few-shot examples with the current question
    messages = FEW_SHOT_MESSAGES + [
        {"role": "user", "content": [{"type": "text", "text": user_prompt}]}
    ]

    try:
        # Generate the response using the pipeline
        output = pipe(messages, max_new_tokens=1024)
        # Extract the content from the last message in the generated text
        generated_content = output[0]["generated_text"][-1]["content"]
        print(generated_content) # Print the full response for real-time viewing
        return generated_content
    except Exception as e:
        error_message = f"An error occurred during model inference: {e}"
        print(f"\n  -> {error_message}")
        return error_message

def main():
    """
    Main function to process the CSV file with the MedGemma model.
    """
    output_dir = os.path.dirname(OUTPUT_CSV)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"Created output directory: {output_dir}")

    try:
        df = pd.read_csv(INPUT_CSV)
        if QUESTION_COLUMN not in df.columns:
            print(f"Error: CSV must have a '{QUESTION_COLUMN}' column.")
            return
    except FileNotFoundError:
        print(f"Error: '{INPUT_CSV}' not found. Ensure Drive is mounted.")
        return

    print("="*50)
    print(f"üöÄ Starting prediction for {len(df)} questions from '{INPUT_CSV}' using MedGemma...")
    print("="*50)

    start_time = time.time()
    full_reasoning_list = []
    final_answer_list = []

    for index, row in df.iterrows():
        question = row[QUESTION_COLUMN]
        print(f"Processing question {index + 1}/{len(df)}: '{str(question)[:50]}...'")

        full_reasoning = get_full_reasoning(question)
        full_reasoning_list.append(full_reasoning)

        final_answer = extract_and_normalize_answer(full_reasoning)
        final_answer_list.append(final_answer)
        print(f"  -> Extracted and Normalized Answer: {final_answer}\n")

    results_df = pd.DataFrame({
        'Question': df[QUESTION_COLUMN],
        'Full_Model_Reasoning': full_reasoning_list,
        'Final_Answer_Letter': final_answer_list
    })

    results_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')

    end_time = time.time()
    total_time = end_time - start_time

    print("\n" + "="*50)
    print(f"‚úÖ All predictions complete.")
    print(f"üíæ Results saved to '{OUTPUT_CSV}'.")
    print(f"‚è±Ô∏è Total time taken: {total_time:.2f} seconds")
    print("="*50)

if __name__ == "__main__":
    main()
