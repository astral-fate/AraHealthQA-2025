import os
import re
import time
import pandas as pd
from openai import OpenAI
from getpass import getpass

# --- API Configuration for Google Colab ---
try:
    from google.colab import userdata
    api_key = userdata.get('NVIDIA_API_KEY')
    if not api_key:
        raise ValueError("Secret 'NVIDIA_API_KEY' not found or is empty.")
except (ImportError, ValueError, KeyError):
    print("Could not find 'NVIDIA_API_KEY' in Colab secrets. Please enter it manually.")
    api_key = getpass("NVIDIA API Key: ")

client = OpenAI(
  base_url = "https://integrate.api.nvidia.com/v1",
  api_key = api_key
)

# --- File Paths and Column Names ---
INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/multiple-choice-questions.csv'
# Updated output file name to reflect new functionality
OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/mcq/predictions_mixtral_mcq_with_accuracy.csv'
QUESTION_COLUMN = 'Question'
ANSWER_COLUMN = 'Answer' # Ground truth column

# --- Few-Shot Examples to guide the model's output format ---
FEW_SHOT_EXAMPLES = [
    {
        "role": "user",
        "content": """Ø§Ù…Ù„Ø£ Ø§Ù„ÙØ±Ø§ØºØ§Øª ÙÙŠ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©:
ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ù†ØµØ¨Ø§Ø¨ Ø§Ù„Ø¬Ù†Ø¨ÙŠØŒ ÙŠØ´ÙŠØ± Ø§Ù†Ø®ÙØ§Ø¶ Ø£Ùˆ ØºÙŠØ§Ø¨ Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ù…ØµØ§Ø¨ Ø¥Ù„Ù‰ ____ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠÙ†ØªØ¬ Ø¹Ù† ____.
Ø£. ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø³ÙˆØ§Ø¦Ù„Ø› Ø¹Ø²Ù„ Ø§Ù„ØµÙˆØª
Ø¨. Ø§Ø­ØªØ¨Ø§Ø³ Ø§Ù„Ù‡ÙˆØ§Ø¡Ø› Ø§Ù†Ù‡ÙŠØ§Ø± Ø§Ù„Ø­ÙˆÙŠØµÙ„Ø§Øª Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©
Ø¬. Ø§Ù„ØªÙ„ÙŠÙØ› Ø§Ù†Ø®ÙØ§Ø¶ Ù…Ø±ÙˆÙ†Ø© Ø§Ù„Ø±Ø¦Ø©
Ø¯. Ù†Ù…Ùˆ Ø§Ù„ÙˆØ±Ù…Ø› Ø§Ù†Ø³Ø¯Ø§Ø¯ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©"""
    },
    {
        "role": "assistant",
        "content": """**Ø§Ù„ØªÙÙƒÙŠØ± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:**
1.  **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„**: ÙŠØ³Ø£Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø¯Ù„Ø§Ù„Ø© Ø§Ù†Ø®ÙØ§Ø¶ Ø£Ùˆ ØºÙŠØ§Ø¨ "Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ" (tactile fremitus) ÙÙŠ Ø­Ø§Ù„Ø© "Ø§Ù„Ø§Ù†ØµØ¨Ø§Ø¨ Ø§Ù„Ø¬Ù†Ø¨ÙŠ" (pleural effusion) ÙˆØ¹Ù† Ø§Ù„Ø¢Ù„ÙŠØ© Ø§Ù„Ù…Ø³Ø¨Ø¨Ø© Ù„Ø°Ù„Ùƒ. Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ Ù‡Ùˆ Ø§Ù„Ø§Ù‡ØªØ²Ø§Ø² Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ù‡ Ø¹Ù„Ù‰ Ø¬Ø¯Ø§Ø± Ø§Ù„ØµØ¯Ø± Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ÙƒÙ„Ø§Ù….
2.  **ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª**:
    * **Ø£. ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø³ÙˆØ§Ø¦Ù„Ø› Ø¹Ø²Ù„ Ø§Ù„ØµÙˆØª**: Ø§Ù„Ø§Ù†ØµØ¨Ø§Ø¨ Ø§Ù„Ø¬Ù†Ø¨ÙŠ Ù‡Ùˆ Ø¨Ø§Ù„ÙØ¹Ù„ ØªØ±Ø§ÙƒÙ… Ù„Ù„Ø³ÙˆØ§Ø¦Ù„ ÙÙŠ Ø§Ù„ØºØ´Ø§Ø¡ Ø§Ù„Ø¬Ù†Ø¨ÙŠ. Ù‡Ø°Ø§ Ø§Ù„Ø³Ø§Ø¦Ù„ ÙŠØ¹Ù…Ù„ ÙƒØ¹Ø§Ø²Ù„ØŒ Ù…Ù…Ø§ ÙŠÙ…Ù†Ø¹ Ø§Ù†ØªÙ‚Ø§Ù„ Ø§Ù‡ØªØ²Ø§Ø²Ø§Øª Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„Ø±Ø¦Ø© Ø¥Ù„Ù‰ Ø¬Ø¯Ø§Ø± Ø§Ù„ØµØ¯Ø±. Ù‡Ø°Ø§ ÙŠØªØ·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ù‹Ø§ Ù…Ø¹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ø© Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ.
    * **Ø¨. Ø§Ø­ØªØ¨Ø§Ø³ Ø§Ù„Ù‡ÙˆØ§Ø¡Ø› Ø§Ù†Ù‡ÙŠØ§Ø± Ø§Ù„Ø­ÙˆÙŠØµÙ„Ø§Øª Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©**: Ù‡Ø°Ø§ ÙŠØµÙ Ø­Ø§Ù„Ø© Ø§Ø³ØªØ±ÙˆØ§Ø­ Ø§Ù„ØµØ¯Ø± (pneumothorax) Ø£Ùˆ Ø§Ù†Ø®Ù…Ø§Øµ Ø§Ù„Ø±Ø¦Ø© (atelectasis)ØŒ ÙˆØ§Ù„ØªÙŠ Ù„Ù‡Ø§ Ù…ÙˆØ¬ÙˆØ¯Ø§Øª ÙÙŠØ²ÙŠØ§Ø¦ÙŠØ© Ù…Ø®ØªÙ„ÙØ©.
    * **Ø¬. Ø§Ù„ØªÙ„ÙŠÙØ› Ø§Ù†Ø®ÙØ§Ø¶ Ù…Ø±ÙˆÙ†Ø© Ø§Ù„Ø±Ø¦Ø©**: Ø§Ù„ØªÙ„ÙŠÙ Ø§Ù„Ø±Ø¦ÙˆÙŠ (Pulmonary fibrosis) ÙŠØ²ÙŠØ¯ Ù…Ù† ÙƒØ«Ø§ÙØ© Ø£Ù†Ø³Ø¬Ø© Ø§Ù„Ø±Ø¦Ø©ØŒ Ù…Ù…Ø§ Ù‚Ø¯ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠØŒ ÙˆÙ„ÙŠØ³ Ø§Ù†Ø®ÙØ§Ø¶Ù‡.
    * **Ø¯. Ù†Ù…Ùˆ Ø§Ù„ÙˆØ±Ù…Ø› Ø§Ù†Ø³Ø¯Ø§Ø¯ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©**: Ù‚Ø¯ ÙŠØ³Ø¨Ø¨ Ø§Ù„ÙˆØ±Ù… Ø§Ù†ØµØ¨Ø§Ø¨Ù‹Ø§ Ø¬Ù†Ø¨ÙŠÙ‹Ø§ØŒ Ù„ÙƒÙ† Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù„Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„Ø±Ø¬ÙØ§Ù† ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø© Ù‡Ùˆ Ø§Ù„Ø³Ø§Ø¦Ù„ Ù†ÙØ³Ù‡ Ø§Ù„Ø°ÙŠ ÙŠØ¹Ø²Ù„ Ø§Ù„ØµÙˆØª. Ø§Ù„Ø®ÙŠØ§Ø± "Ø£" ÙŠØµÙ Ø§Ù„Ø¢Ù„ÙŠØ© Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¦ÙŠØ© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„.
3.  **Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬**: Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© Ù‡Ùˆ Ø£Ù† ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø³ÙˆØ§Ø¦Ù„ Ù‡Ùˆ Ù…Ø§ ÙŠØ³Ø¨Ø¨ Ø¹Ø²Ù„ Ø§Ù„ØµÙˆØªØŒ Ù…Ù…Ø§ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ.

Final Answer: Ø£"""
    }
]

def extract_and_normalize_answer(full_text):
    """
    Parses the full text from the model to find, normalize, and translate the final answer letter.
    This function is designed to be robust and handle various output formats by searching in stages.
    """
    found_letter = None

    # Stage 1: Look for explicit answer declarations (highest priority)
    explicit_pattern = r"(?:Final Answer|Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©|Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© Ù‡ÙŠ|Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©|Ø§Ù„Ø®Ù„Ø§ØµØ©)\s*[:ï¼š]?\s*\**\s*([A-Ea-eØ£-ÙŠ])"
    match = re.search(explicit_pattern, full_text, re.IGNORECASE | re.MULTILINE)
    if match:
        found_letter = match.group(1)

    # Stage 2: If no explicit declaration, look for lines starting with a letter and a dot
    if not found_letter:
        line_pattern = r"^\s*\**([A-Ea-eØ£-ÙŠ])\.\s*.+"
        lines = full_text.splitlines()
        for line in reversed(lines):
            match = re.match(line_pattern, line.strip())
            if match:
                found_letter = match.group(1)
                break

    # Stage 3: Last resort - check for a single letter on a line
    if not found_letter:
        lines = full_text.splitlines()
        for line in reversed(lines[-3:]):
            cleaned_line = line.strip().replace('*', '')
            if len(cleaned_line) == 1 and re.match(r"^[A-Ea-eØ£-ÙŠ]$", cleaned_line):
                found_letter = cleaned_line
                break

    if not found_letter:
        return "N/A"

    found_letter = found_letter.upper()

    translation_map = {'A': 'Ø£', 'B': 'Ø¨', 'C': 'Ø¬', 'D': 'Ø¯', 'E': 'Ù‡'}
    if found_letter in translation_map:
        found_letter = translation_map[found_letter]

    if found_letter in ['Ø§', 'Ø¥', 'Ø¢', 'Ø£']:
        found_letter = 'Ø£'

    if found_letter in ['Ø£', 'Ø¨', 'Ø¬', 'Ø¯', 'Ù‡']:
        return found_letter
    else:
        return "Parse Error"

def extract_ground_truth_letter(answer_text):
    """
    Extracts the first Arabic letter from the ground truth answer text.
    Handles formats like 'Ø£. text' or 'Ø£ text'.
    """
    if not isinstance(answer_text, str):
        return "N/A"
    match = re.match(r"^\s*([Ø£-ÙŠ])", answer_text.strip())
    if match:
        letter = match.group(1)
        if letter in ['Ø§', 'Ø¥', 'Ø¢', 'Ø£']:
            return 'Ø£'
        return letter
    return "N/A"

def get_full_reasoning(user_prompt):
    """
    Makes a single API call to the Mixtral model and returns the entire text.
    """
    messages = FEW_SHOT_EXAMPLES + [{"role": "user", "content": user_prompt}]
    full_response = ""
    max_retries = 3
    retry_delay = 5

    for attempt in range(max_retries):
        try:
            completion = client.chat.completions.create(
              model="mistralai/mixtral-8x22b-instruct-v0.1",
              messages=messages,
              temperature=0.5,
              top_p=1,
              max_tokens=1024,
              stream=True
            )
            print("  -> ğŸ¤– Streaming response...", end="")
            for chunk in completion:
                if chunk.choices[0].delta.content is not None:
                    print(chunk.choices[0].delta.content, end="", flush=True)
                    full_response += chunk.choices[0].delta.content
            print("\n")
            return full_response
        except Exception as e:
            print(f"\n  -> An error occurred (Attempt {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                print(f"  -> Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
            else:
                return f"Failed after {max_retries} attempts: {e}"
    return "Failed to get a response."

def main():
    """
    Main function to read questions, get predictions, calculate accuracy,
    and save everything, with the ability to resume from a previous run.
    """
    output_dir = os.path.dirname(OUTPUT_CSV)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"Created output directory: {output_dir}")

    try:
        full_df = pd.read_csv(INPUT_CSV)
        if QUESTION_COLUMN not in full_df.columns or ANSWER_COLUMN not in full_df.columns:
            print(f"Error: Input CSV must have '{QUESTION_COLUMN}' and '{ANSWER_COLUMN}' columns.")
            return
    except FileNotFoundError:
        print(f"Error: Input CSV '{INPUT_CSV}' not found. Ensure Drive is mounted and the path is correct.")
        return

    # --- Resumability Logic ---
    existing_results_df = pd.DataFrame()
    if os.path.exists(OUTPUT_CSV):
        print(f"ğŸ“„ Found existing results file: '{OUTPUT_CSV}'. Loading previous work.")
        existing_results_df = pd.read_csv(OUTPUT_CSV)
        
        # Get list of questions already processed
        processed_questions = existing_results_df[QUESTION_COLUMN].tolist()
        print(f"  -> Found {len(processed_questions)} previously processed questions.")
        
        # Filter the main dataframe to get only the questions that need processing
        df_to_process = full_df[~full_df[QUESTION_COLUMN].isin(processed_questions)].copy()
        
        if len(df_to_process) == 0:
            print("âœ… All questions have already been processed. Nothing to do.")
            # Optional: Recalculate accuracy on existing file and exit
            accuracy = (existing_results_df['Final_Answer_Letter'] == existing_results_df['Ground_Truth_Letter']).sum() / len(existing_results_df) * 100
            print(f"ğŸ“Š Final accuracy from existing file: {accuracy:.2f}%")
            return
            
        print(f"  -> Resuming with {len(df_to_process)} remaining questions.")
    else:
        print("ğŸ“„ No existing results file found. Starting from scratch.")
        df_to_process = full_df.copy()

    print("="*50)
    print(f"ğŸš€ Starting prediction for {len(df_to_process)} questions...")
    print("="*50)

    start_time = time.time()
    new_results_list = []

    for index, row in df_to_process.iterrows():
        question = row[QUESTION_COLUMN]
        ground_truth_text = row[ANSWER_COLUMN]
        
        # Using original index from full_df for progress tracking
        original_index = full_df.index[full_df[QUESTION_COLUMN] == question].tolist()[0]
        print(f"Processing question {original_index + 1}/{len(full_df)}: '{str(question)[:50]}...'")

        full_reasoning = get_full_reasoning(question)
        predicted_answer = extract_and_normalize_answer(full_reasoning)
        ground_truth_letter = extract_ground_truth_letter(ground_truth_text)
        
        print(f"  -> Ground Truth: {ground_truth_letter} | Predicted: {predicted_answer}")
        
        new_results_list.append({
            'Question': question,
            'Full_Model_Reasoning': full_reasoning,
            'Ground_Truth_Letter': ground_truth_letter,
            'Final_Answer_Letter': predicted_answer
        })

    # Create a DataFrame for the newly processed results
    new_results_df = pd.DataFrame(new_results_list)

    # Combine the existing results with the new ones
    final_df = pd.concat([existing_results_df, new_results_df], ignore_index=True)

    # Save the combined DataFrame
    final_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')

    # --- Final Calculation and Summary ---
    correct_predictions = (final_df['Final_Answer_Letter'] == final_df['Ground_Truth_Letter']).sum()
    total_questions = len(final_df)
    accuracy = (correct_predictions / total_questions) * 100 if total_questions > 0 else 0
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print("\n" + "="*50)
    print(f"âœ… Processing complete.")
    if not new_results_df.empty:
      print(f"â±ï¸ Time for this session: {total_time:.2f} seconds")
    print(f"ğŸ“Š Final Accuracy: {accuracy:.2f}% ({correct_predictions}/{total_questions} correct)")
    print(f"ğŸ’¾ All results saved to '{OUTPUT_CSV}'.")
    print("="*50)

if __name__ == "__main__":
    main()
