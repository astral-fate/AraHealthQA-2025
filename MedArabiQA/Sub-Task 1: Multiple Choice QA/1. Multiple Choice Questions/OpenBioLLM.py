import os
import re
import pandas as pd
import time
from getpass import getpass
from sklearn.metrics import accuracy_score
import gc

# --- Import necessary libraries for local inference ---
try:
    from transformers import pipeline, BitsAndBytesConfig
    import torch
except ImportError:
    print("Please install necessary libraries: pip install transformers torch accelerate bitsandbytes")
    exit()


# --- File paths and column names ---
INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/t2t1/data/fill-in-the-blank-choices.csv'
# Changed output file name for the new model
OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/t2t1/final_result/predictions_fitb_choices_OpenBioLLM.csv'

# --- Column names ---
QUESTION_COLUMN = 'Question - Arabic'
ANSWER_COLUMN = 'Answer - Arabic'


# --- Chain of Thought & Few-Shot Prompting Configuration ---
# Using the specific system prompt for OpenBioLLM
SYSTEM_PROMPT = """You are an expert and experienced from the healthcare and biomedical domain with extensive medical knowledge and practical experience. Your name is OpenBioLLM, and you were developed by Saama AI Labs. who's willing to help answer the user's query with explanation. In your explanation, leverage your deep medical expertise such as relevant anatomical structures, physiological processes, diagnostic criteria, treatment guidelines, or other pertinent medical concepts. Use precise medical terminology while still aiming to make the explanation clear and accessible to a general audience."""

FEW_SHOT_EXAMPLES = [
     {
        "role": "user",
        "content": """Ø§Ù…Ù„Ø£ Ø§Ù„ÙØ±Ø§ØºØ§Øª ÙÙŠ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©:
ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ù†ØµØ¨Ø§Ø¨ Ø§Ù„Ø¬Ù†Ø¨ÙŠØŒ ÙŠØ´ÙŠØ± Ø§Ù†Ø®ÙØ§Ø¶ Ø£Ùˆ ØºÙŠØ§Ø¨ Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ù…ØµØ§Ø¨ Ø¥Ù„Ù‰ ____ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠÙ†ØªØ¬ Ø¹Ù† ____.
Ø£. ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø³ÙˆØ§Ø¦Ù„Ø› Ø¹Ø²Ù„ Ø§Ù„ØµÙˆØª
Ø¨. Ø§Ø­ØªØ¨Ø§Ø³ Ø§Ù„Ù‡ÙˆØ§Ø¡Ø› Ø§Ù†Ù‡ÙŠØ§Ø± Ø§Ù„Ø­ÙˆÙŠØµÙ„Ø§Øª Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©
Ø¬. Ø§Ù„ØªÙ„ÙŠÙØ› Ø§Ù†Ø®ÙØ§Ø¶ Ù…Ø±ÙˆÙ†Ø© Ø§Ù„Ø±Ø¦Ø©
Ø¯. Ù†Ù…Ùˆ Ø§Ù„ÙˆØ±Ù…Ø› Ø§Ù†Ø³Ø¯Ø§Ø¯ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©"""
    },
    {
        "role": "assistant",
        "content": """<thinking>
        1.  **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„**: ÙŠØ³Ø£Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø¯Ù„Ø§Ù„Ø© Ø§Ù†Ø®ÙØ§Ø¶ Ø£Ùˆ ØºÙŠØ§Ø¨ "Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ" (tactile fremitus) ÙÙŠ Ø­Ø§Ù„Ø© "Ø§Ù„Ø§Ù†ØµØ¨Ø§Ø¨ Ø§Ù„Ø¬Ù†Ø¨ÙŠ" (pleural effusion) ÙˆØ¹Ù† Ø§Ù„Ø¢Ù„ÙŠØ© Ø§Ù„Ù…Ø³Ø¨Ø¨Ø© Ù„Ø°Ù„Ùƒ. Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ Ù‡Ùˆ Ø§Ù„Ø§Ù‡ØªØ²Ø§Ø² Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ù‡ Ø¹Ù„Ù‰ Ø¬Ø¯Ø§Ø± Ø§Ù„ØµØ¯Ø± Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ÙƒÙ„Ø§Ù….
        2.  **ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª**:
            * **Ø£. ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø³ÙˆØ§Ø¦Ù„Ø› Ø¹Ø²Ù„ Ø§Ù„ØµÙˆØª**: Ø§Ù„Ø§Ù†ØµØ¨Ø§Ø¨ Ø§Ù„Ø¬Ù†Ø¨ÙŠ Ù‡Ùˆ Ø¨Ø§Ù„ÙØ¹Ù„ ØªØ±Ø§ÙƒÙ… Ù„Ù„Ø³ÙˆØ§Ø¦Ù„ ÙÙŠ Ø§Ù„ØºØ´Ø§Ø¡ Ø§Ù„Ø¬Ù†Ø¨ÙŠ. Ù‡Ø°Ø§ Ø§Ù„Ø³Ø§Ø¦Ù„ ÙŠØ¹Ù…Ù„ ÙƒØ¹Ø§Ø²Ù„ØŒ Ù…Ù…Ø§ ÙŠÙ…Ù†Ø¹ Ø§Ù†ØªÙ‚Ø§Ù„ Ø§Ù‡ØªØ²Ø§Ø²Ø§Øª Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„Ø±Ø¦Ø© Ø¥Ù„Ù‰ Ø¬Ø¯Ø§Ø± Ø§Ù„ØµØ¯Ø±. Ù‡Ø°Ø§ ÙŠØªØ·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ù‹Ø§ Ù…Ø¹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ø© Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ.
            * **Ø¨. Ø§Ø­ØªØ¨Ø§Ø³ Ø§Ù„Ù‡ÙˆØ§Ø¡Ø› Ø§Ù†Ù‡ÙŠØ§Ø± Ø§Ù„Ø­ÙˆÙŠØµÙ„Ø§Øª Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©**: Ù‡Ø°Ø§ ÙŠØµÙ Ø­Ø§Ù„Ø© Ø§Ø³ØªØ±ÙˆØ§Ø­ Ø§Ù„ØµØ¯Ø± (pneumothorax) Ø£Ùˆ Ø§Ù†Ø®Ù…Ø§Øµ Ø§Ù„Ø±Ø¦Ø© (atelectasis)ØŒ ÙˆØ§Ù„ØªÙŠ Ù„Ù‡Ø§ Ù…ÙˆØ¬ÙˆØ¯Ø§Øª ÙÙŠØ²ÙŠØ§Ø¦ÙŠØ© Ù…Ø®ØªÙ„ÙØ©.
            * **Ø¬. Ø§Ù„ØªÙ„ÙŠÙØ› Ø§Ù†Ø®ÙØ§Ø¶ Ù…Ø±ÙˆÙ†Ø© Ø§Ù„Ø±Ø¦Ø©**: Ø§Ù„ØªÙ„ÙŠÙ Ø§Ù„Ø±Ø¦ÙˆÙŠ (Pulmonary fibrosis) ÙŠØ²ÙŠØ¯ Ù…Ù† ÙƒØ«Ø§ÙØ© Ø£Ù†Ø³Ø¬Ø© Ø§Ù„Ø±Ø¦Ø©ØŒ Ù…Ù…Ø§ Ù‚Ø¯ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠØŒ ÙˆÙ„ÙŠØ³ Ø§Ù†Ø®ÙØ§Ø¶Ù‡.
            * **Ø¯. Ù†Ù…Ùˆ Ø§Ù„ÙˆØ±Ù…Ø› Ø§Ù†Ø³Ø¯Ø§Ø¯ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©**: Ù‚Ø¯ ÙŠØ³Ø¨Ø¨ Ø§Ù„ÙˆØ±Ù… Ø§Ù†ØµØ¨Ø§Ø¨Ù‹Ø§ Ø¬Ù†Ø¨ÙŠÙ‹Ø§ØŒ Ù„ÙƒÙ† Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù„Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„Ø±Ø¬ÙØ§Ù† ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø© Ù‡Ùˆ Ø§Ù„Ø³Ø§Ø¦Ù„ Ù†ÙØ³Ù‡ Ø§Ù„Ø°ÙŠ ÙŠØ¹Ø²Ù„ Ø§Ù„ØµÙˆØª. Ø§Ù„Ø®ÙŠØ§Ø± "Ø£" ÙŠØµÙ Ø§Ù„Ø¢Ù„ÙŠØ© Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¦ÙŠØ© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„.
        3.  **Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬**: Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© Ù‡Ùˆ Ø£Ù† ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø³ÙˆØ§Ø¦Ù„ Ù‡Ùˆ Ù…Ø§ ÙŠØ³Ø¨Ø¨ Ø¹Ø²Ù„ Ø§Ù„ØµÙˆØªØŒ Ù…Ù…Ø§ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ.
        </thinking>
        Final Answer: Ø£"""
    }
]


# --- Function to Generate Answers ---
def generate_answer(question, pipe):
    """
    Generates an answer using the OpenBioLLM pipeline.
    Uses the apply_chat_template method and includes intelligent parsing.
    """
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT}
    ]
    # The FEW_SHOT_EXAMPLES are not used here as we are now using a different prompting strategy
    # If the model struggles, we can add them back. For now, let's follow the user's snippet structure.
    messages.append({"role": "user", "content": question})

    try:
        # Apply the chat template to create the prompt string
        prompt = pipe.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # Define terminators for the model's generation
        terminators = [
            pipe.tokenizer.eos_token_id,
            pipe.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]
        
        # Generate the text
        outputs = pipe(
            prompt,
            max_new_tokens=1024, # Increased for detailed reasoning
            eos_token_id=terminators,
            do_sample=False, # Use False for deterministic, exam-like answers
        )
        
        # Extract only the newly generated text
        response_text = outputs[0]["generated_text"][len(prompt):].strip()
        
        # --- Intelligent Parsing Logic ---
        # Method 1: Check for the standard 'Final Answer:' format first.
        match = re.search(r"Final Answer:\s*([\u0621-\u064A])", response_text, re.IGNORECASE)
        if match:
            return match.group(1)

        # Method 2: If standard format fails, parse the reasoning text.
        print(f"  -> 'Final Answer' format not found. Attempting to parse reasoning...")
        
        # Heuristic 2.1: Look for explicit conclusive phrases.
        conclusive_phrases = [
            r"Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„ØµØ­ÙŠØ­ Ù‡Ùˆ\s*([\u0621-\u064A])",
            r"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© Ù‡ÙŠ\s*([\u0621-\u064A])",
            r"Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù‡Ùˆ Ø£Ù† Ø§Ù„Ø®ÙŠØ§Ø±\s*([\u0621-\u064A])",
            r"Ø§Ù„Ø®ÙŠØ§Ø±\s*([\u0621-\u064A])\s*Ù‡Ùˆ Ø§Ù„ØµØ­ÙŠØ­",
        ]
        for phrase in conclusive_phrases:
            match = re.search(phrase, response_text)
            if match:
                print(f"  -> Found answer using conclusive phrase heuristic.")
                return match.group(1)

        # Heuristic 2.2: Assume the last mentioned option is the intended answer.
        option_mentions = re.findall(r"Ø§Ù„Ø®ÙŠØ§Ø±\s*([\u0621-\u064A])", response_text)
        if option_mentions:
            last_option = option_mentions[-1]
            print(f"  -> Found answer using last-mentioned option heuristic: '{last_option}'")
            return last_option
        
        print(f"  -> Warning: Could not deduce answer from response: '{response_text}'. Recording as empty.")
        return ""
            
    except Exception as e:
        print(f"  -> An error occurred during model inference: {e}")
        return "INFERENCE_ERROR"


# --- Function to Evaluate MCQ Accuracy (with Normalization) ---
def evaluate_mcq_accuracy(predictions, ground_truths):
    """
    Calculates and prints the accuracy of the MCQ predictions.
    Normalizes different forms of the Arabic letter Alif before comparison.
    """
    print("\n" + "="*50)
    print("ğŸš€ Starting Evaluation...")
    print("="*50)
    
    error_codes = ["INFERENCE_ERROR", ""]
    valid_indices = [i for i, p in enumerate(predictions) if p not in error_codes]
    valid_predictions = [predictions[i] for i in valid_indices]
    valid_ground_truths = [ground_truths[i] for i in valid_indices]

    if not valid_predictions:
        print("No valid predictions to evaluate.")
        return

    # --- Normalization Logic ---
    def normalize_alif(letter):
        """Replaces all variants of Alif (Ø£, Ø¥, Ø¢) with a plain Alif (Ø§)."""
        return letter.replace('Ø£', 'Ø§').replace('Ø¥', 'Ø§').replace('Ø¢', 'Ø§')

    # Apply normalization to both predictions and ground truths
    normalized_predictions = [normalize_alif(p) for p in valid_predictions]
    normalized_ground_truths = [normalize_alif(g) for g in valid_ground_truths]
    # --- End of Normalization Logic ---

    accuracy = accuracy_score(normalized_ground_truths, normalized_predictions)
    correct_predictions = sum(p == g for p, g in zip(normalized_ground_truths, normalized_predictions))
    
    total_valid_predictions = len(valid_predictions)
    total_questions = len(ground_truths)
    failed_or_empty = total_questions - total_valid_predictions

    print(f"Total Questions Attempted: {total_questions}")
    print(f"Final Unanswered / Error Count: {failed_or_empty}")
    print(f"Valid Predictions to Evaluate: {total_valid_predictions}")
    print("-" * 20)
    print(f"Correct Predictions: {correct_predictions} / {total_valid_predictions}")
    print(f"ğŸ“Š Accuracy (on valid responses): {accuracy * 100:.2f}%")
    print("="*50 + "\nâœ… Evaluation Complete.\n" + "="*50)


# --- Main Execution ---
def main():
    try:
        df = pd.read_csv(INPUT_CSV, encoding='utf-8')
    except FileNotFoundError:
        print(f"Error: The file '{INPUT_CSV}' was not found. Please check the path.")
        return
    except Exception as e:
        print(f"An error occurred while reading the CSV: {e}")
        return

    df.dropna(subset=[QUESTION_COLUMN, ANSWER_COLUMN], inplace=True)
    df.reset_index(drop=True, inplace=True)
    
    if os.path.exists(OUTPUT_CSV):
        print(f"Output file '{OUTPUT_CSV}' already exists. Please remove it or rename it to run a new generation.")
        return
        
    # --- Model Loading ---
    print("="*50)
    print("ğŸš€ Initializing local model: aaditya/OpenBioLLM-Llama3-8B")
    print("This may take a few minutes...")
    print("="*50)
    
    if not torch.cuda.is_available():
        print("âŒ CUDA is not available. A GPU is required for this model.")
        return
    
    try:
        print("Clearing GPU cache before model loading...")
        gc.collect()
        torch.cuda.empty_cache()
        
        # Proactively use 4-bit quantization for memory safety
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        model_id = "aaditya/OpenBioLLM-Llama3-8B"
        pipe = pipeline(
            "text-generation",
            model=model_id,
            model_kwargs={"quantization_config": quantization_config}
        )
    except Exception as e:
        print(f"âŒ Failed to load the model. Error: {e}")
        return

    print("âœ… Model loaded successfully.")
    
    # --- Prediction Generation ---
    predictions = []
    total_questions = len(df)
    start_time = time.time()

    for index, row in df.iterrows():
        question = row[QUESTION_COLUMN]
        print(f"Processing question {index + 1}/{total_questions}...")
        answer_letter = generate_answer(question, pipe)
        predictions.append(answer_letter)
        
        ground_truth_letter = str(row[ANSWER_COLUMN]).strip()[0] if str(row[ANSWER_COLUMN]).strip() else "N/A"
        print(f"  -> Ground Truth: {ground_truth_letter} | Model's Predicted Letter: {answer_letter}")

    end_time = time.time()
    total_duration = end_time - start_time
    minutes = int(total_duration // 60)
    seconds = int(total_duration % 60)
    print("\n" + "="*50)
    print(f"âœ… Prediction generation complete.")
    print(f"â±ï¸  Total time taken: {minutes} minutes and {seconds} seconds.")
    print("="*50)

    predictions_df = pd.DataFrame(predictions)
    predictions_df.to_csv(OUTPUT_CSV, header=False, index=False, encoding='utf-8')
    print(f"\nSuccessfully saved predictions to '{OUTPUT_CSV}'.")

    # --- Final Evaluation ---
    ground_truths = [str(ans).strip()[0] if str(ans).strip() else "INVALID_TRUTH" for ans in df[ANSWER_COLUMN].tolist()]
    evaluate_mcq_accuracy(predictions, ground_truths)


if __name__ == "__main__":
    main()
