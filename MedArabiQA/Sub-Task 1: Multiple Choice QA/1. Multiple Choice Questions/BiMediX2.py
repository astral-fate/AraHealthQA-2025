import os
import re
import pandas as pd
import time
from openai import OpenAI
from sklearn.metrics import accuracy_score
import gc

# --- Configuration ---
# Set the base URL for your local vLLM server
VLLM_BASE_URL = "http://localhost:8000/v1/"
# The model name as served by vLLM
MODEL_NAME = "MBZUAI/BiMediX2-8B-hf"
# A dummy API key is required by the OpenAI client, but not used by vLLM.
API_KEY = "DUMMY_KEY"

# --- File paths and column names ---
INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/t2t1/data/fill-in-the-blank-choices.csv'
# Updated output file to reflect the new model
OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/t2t1/final_result/predictions_fitb_choices_BiMediX2.csv'

# --- Column names ---
QUESTION_COLUMN = 'Question - Arabic'
ANSWER_COLUMN = 'Answer - Arabic'

# --- Chain of Thought & Few-Shot Prompting Configuration ---
# This prompt structure guides the model to provide a step-by-step reasoning before the final answer.
SYSTEM_PROMPT = """You are an expert medical professional and a meticulous exam assistant. Your task is to solve a multiple-choice question in Arabic.
First, you will engage in a step-by-step thinking process in a <thinking> block. Analyze the medical question, evaluate each option (Ø£, Ø¨, Ø¬, Ø¯, Ù‡), and explain your reasoning for choosing the correct answer.
Second, after your reasoning, you MUST provide the final answer on a new line in the format:
Final Answer: [The single Arabic letter of the correct option]

This two-step process is mandatory. Your entire response must be in Arabic.
"""

FEW_SHOT_EXAMPLES = [
    {
        "role": "user",
        "content": """Ø§Ù…Ù„Ø£ Ø§Ù„ÙØ±Ø§ØºØ§Øª ÙÙŠ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©:
ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ù†ØµØ¨Ø§Ø¨ Ø§Ù„Ø¬Ù†Ø¨ÙŠØŒ ÙŠØ´ÙŠØ± Ø§Ù†Ø®ÙØ§Ø¶ Ø£Ùˆ ØºÙŠØ§Ø¨ Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ù…ØµØ§Ø¨ Ø¥Ù„Ù‰ ____ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠÙ†ØªØ¬ Ø¹Ù† ____.
Ø£. ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø³ÙˆØ§Ø¦Ù„Ø› Ø¹Ø²Ù„ Ø§Ù„ØµÙˆØª
Ø¨. Ø§Ø­ØªØ¨Ø§Ø³ Ø§Ù„Ù‡ÙˆØ§Ø¡Ø› Ø§Ù†Ù‡ÙŠØ§Ø± Ø§Ù„Ø­ÙˆÙŠØµÙ„Ø§Øª Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©
Ø¬. Ø§Ù„ØªÙ„ÙŠÙØ› Ø§Ù†Ø®ÙØ§Ø¶ Ù…Ø±ÙˆÙ†Ø© Ø§Ù„Ø±Ø¦Ø©
Ø¯. Ù†Ù…Ùˆ Ø§Ù„ÙˆØ±Ù…Ø› Ø§Ù†Ø³Ø¯Ø§Ø¯ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©"""
    },
    {
        "role": "assistant",
        "content": """<thinking>
        1.  **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„**: ÙŠØ³Ø£Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø¯Ù„Ø§Ù„Ø© Ø§Ù†Ø®ÙØ§Ø¶ Ø£Ùˆ ØºÙŠØ§Ø¨ "Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ" (tactile fremitus) ÙÙŠ Ø­Ø§Ù„Ø© "Ø§Ù„Ø§Ù†ØµØ¨Ø§Ø¨ Ø§Ù„Ø¬Ù†Ø¨ÙŠ" (pleural effusion) ÙˆØ¹Ù† Ø§Ù„Ø¢Ù„ÙŠØ© Ø§Ù„Ù…Ø³Ø¨Ø¨Ø© Ù„Ø°Ù„Ùƒ. Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ Ù‡Ùˆ Ø§Ù„Ø§Ù‡ØªØ²Ø§Ø² Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ù‡ Ø¹Ù„Ù‰ Ø¬Ø¯Ø§Ø± Ø§Ù„ØµØ¯Ø± Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ÙƒÙ„Ø§Ù….
        2.  **ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª**:
            * **Ø£. ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø³ÙˆØ§Ø¦Ù„Ø› Ø¹Ø²Ù„ Ø§Ù„ØµÙˆØª**: Ø§Ù„Ø§Ù†ØµØ¨Ø§Ø¨ Ø§Ù„Ø¬Ù†Ø¨ÙŠ Ù‡Ùˆ Ø¨Ø§Ù„ÙØ¹Ù„ ØªØ±Ø§ÙƒÙ… Ù„Ù„Ø³ÙˆØ§Ø¦Ù„ ÙÙŠ Ø§Ù„ØºØ´Ø§Ø¡ Ø§Ù„Ø¬Ù†Ø¨ÙŠ. Ù‡Ø°Ø§ Ø§Ù„Ø³Ø§Ø¦Ù„ ÙŠØ¹Ù…Ù„ ÙƒØ¹Ø§Ø²Ù„ØŒ Ù…Ù…Ø§ ÙŠÙ…Ù†Ø¹ Ø§Ù†ØªÙ‚Ø§Ù„ Ø§Ù‡ØªØ²Ø§Ø²Ø§Øª Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„Ø±Ø¦Ø© Ø¥Ù„Ù‰ Ø¬Ø¯Ø§Ø± Ø§Ù„ØµØ¯Ø±. Ù‡Ø°Ø§ ÙŠØªØ·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ù‹Ø§ Ù…Ø¹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ø© Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ.
            * **Ø¨. Ø§Ø­ØªØ¨Ø§Ø³ Ø§Ù„Ù‡ÙˆØ§Ø¡Ø› Ø§Ù†Ù‡ÙŠØ§Ø± Ø§Ù„Ø­ÙˆÙŠØµÙ„Ø§Øª Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©**: Ù‡Ø°Ø§ ÙŠØµÙ Ø­Ø§Ù„Ø© Ø§Ø³ØªØ±ÙˆØ§Ø­ Ø§Ù„ØµØ¯Ø± (pneumothorax) Ø£Ùˆ Ø§Ù†Ø®Ù…Ø§Øµ Ø§Ù„Ø±Ø¦Ø© (atelectasis)ØŒ ÙˆØ§Ù„ØªÙŠ Ù„Ù‡Ø§ Ù…ÙˆØ¬ÙˆØ¯Ø§Øª ÙÙŠØ²ÙŠØ§Ø¦ÙŠØ© Ù…Ø®ØªÙ„ÙØ©.
            * **Ø¬. Ø§Ù„ØªÙ„ÙŠÙØ› Ø§Ù†Ø®ÙØ§Ø¶ Ù…Ø±ÙˆÙ†Ø© Ø§Ù„Ø±Ø¦Ø©**: Ø§Ù„ØªÙ„ÙŠÙ Ø§Ù„Ø±Ø¦ÙˆÙŠ (Pulmonary fibrosis) ÙŠØ²ÙŠØ¯ Ù…Ù† ÙƒØ«Ø§ÙØ© Ø£Ù†Ø³Ø¬Ø© Ø§Ù„Ø±Ø¦Ø©ØŒ Ù…Ù…Ø§ Ù‚Ø¯ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠØŒ ÙˆÙ„ÙŠØ³ Ø§Ù†Ø®ÙØ§Ø¶Ù‡.
            * **Ø¯. Ù†Ù…Ùˆ Ø§Ù„ÙˆØ±Ù…Ø› Ø§Ù†Ø³Ø¯Ø§Ø¯ Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ©**: Ù‚Ø¯ ÙŠØ³Ø¨Ø¨ Ø§Ù„ÙˆØ±Ù… Ø§Ù†ØµØ¨Ø§Ø¨Ù‹Ø§ Ø¬Ù†Ø¨ÙŠÙ‹Ø§ØŒ Ù„ÙƒÙ† Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù„Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„Ø±Ø¬ÙØ§Ù† ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø© Ù‡Ùˆ Ø§Ù„Ø³Ø§Ø¦Ù„ Ù†ÙØ³Ù‡ Ø§Ù„Ø°ÙŠ ÙŠØ¹Ø²Ù„ Ø§Ù„ØµÙˆØª. Ø§Ù„Ø®ÙŠØ§Ø± "Ø£" ÙŠØµÙ Ø§Ù„Ø¢Ù„ÙŠØ© Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¦ÙŠØ© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„.
        3.  **Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬**: Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© Ù‡Ùˆ Ø£Ù† ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø³ÙˆØ§Ø¦Ù„ Ù‡Ùˆ Ù…Ø§ ÙŠØ³Ø¨Ø¨ Ø¹Ø²Ù„ Ø§Ù„ØµÙˆØªØŒ Ù…Ù…Ø§ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø§Ù†Ø®ÙØ§Ø¶ Ø§Ù„Ø±Ø¬ÙØ§Ù† Ø§Ù„Ù„Ù…Ø³ÙŠ.
        </thinking>
        Final Answer: Ø£"""
    }
]

# --- Function to Generate Answers via vLLM API ---
def generate_answer(question, client):
    """
    Generates an answer by calling the vLLM OpenAI-compatible API.
    Includes advanced parsing to deduce the answer from the model's reasoning if the standard format is missing.
    """
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT}
    ]
    messages.extend(FEW_SHOT_EXAMPLES)
    # Add a final reminder to ensure the model follows the instructions.
    final_instruction = "Ø§Ù„Ø¢Ù†ØŒ Ø§ØªØ¨Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¨Ø¯Ù‚Ø©. Ø§Ø¨Ø¯Ø£ Ø¨ÙƒØªÙ„Ø© <thinking> Ø«Ù… Ø§Ø®ØªØªÙ… Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ù€ 'Final Answer: ' Ù…ØªØ¨ÙˆØ¹Ù‹Ø§ Ø¨Ø§Ù„Ø­Ø±Ù Ø§Ù„ØµØ­ÙŠØ­ ÙÙ‚Ø·."
    prompt_with_reminder = f"{question}\n\n{final_instruction}"
    # The user's question is text-only, so we don't need the image part of the payload.
    messages.append({"role": "user", "content": prompt_with_reminder})

    try:
        # Make the API call to the local vLLM server
        completion = client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            max_tokens=1536,  # Adjusted for reasoning and answer
            temperature=0.1,
            top_p=0.9,
            stop=["<|eot_id|>"] # Add stop token if needed for the model
        )
        response_text = completion.choices[0].message.content.strip()

        # Method 1: Check for the standard 'Final Answer:' format first.
        match = re.search(r"Final Answer:\s*([\u0621-\u064A])", response_text, re.IGNORECASE)
        if match:
            return match.group(1)

        # Method 2: If standard format fails, parse the reasoning text.
        print(f"  -> 'Final Answer' format not found. Attempting to parse reasoning...")

        # Heuristic 2.1: Look for explicit conclusive phrases.
        conclusive_phrases = [
            r"Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„ØµØ­ÙŠØ­ Ù‡Ùˆ\s*([\u0621-\u064A])",
            r"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© Ù‡ÙŠ\s*([\u0621-\u064A])",
            r"Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù‡Ùˆ Ø£Ù† Ø§Ù„Ø®ÙŠØ§Ø±\s*([\u0621-\u064A])",
            r"Ø§Ù„Ø®ÙŠØ§Ø±\s*([\u0621-\u064A])\s*Ù‡Ùˆ Ø§Ù„ØµØ­ÙŠØ­",
        ]
        for phrase in conclusive_phrases:
            match = re.search(phrase, response_text)
            if match:
                print(f"  -> Found answer using conclusive phrase heuristic.")
                return match.group(1)

        # Heuristic 2.2: Assume the last mentioned option is the intended answer.
        option_mentions = re.findall(r"Ø§Ù„Ø®ÙŠØ§Ø±\s*([\u0621-\u064A])", response_text)
        if option_mentions:
            last_option = option_mentions[-1]
            print(f"  -> Found answer using last-mentioned option heuristic: '{last_option}'")
            return last_option

        # If all parsing fails, return empty.
        print(f"  -> Warning: Could not deduce answer from response: '{response_text}'. Recording as empty.")
        return ""

    except Exception as e:
        print(f"  -> An error occurred during API call: {e}")
        return "INFERENCE_ERROR"


# --- Function to Evaluate MCQ Accuracy ---
def evaluate_mcq_accuracy(predictions, ground_truths):
    """Calculates and prints the accuracy of the predictions."""
    print("\n" + "="*50)
    print("ğŸš€ Starting Evaluation...")
    print("="*50)

    error_codes = ["INFERENCE_ERROR", ""]
    valid_indices = [i for i, p in enumerate(predictions) if p not in error_codes]
    valid_predictions = [predictions[i] for i in valid_indices]
    valid_ground_truths = [ground_truths[i] for i in valid_indices]

    if not valid_predictions:
        print("No valid predictions to evaluate. Check for widespread inference or parsing errors.")
        return

    accuracy = accuracy_score(valid_ground_truths, valid_predictions)
    correct_predictions = int(accuracy * len(valid_predictions))
    total_valid_predictions = len(valid_predictions)
    total_questions = len(ground_truths)
    failed_or_empty = total_questions - total_valid_predictions

    print(f"Total Questions Attempted: {total_questions}")
    print(f"Final Unanswered / Error Count: {failed_or_empty}")
    print(f"Valid Predictions to Evaluate: {total_valid_predictions}")
    print("-" * 20)
    print(f"Correct Predictions: {correct_predictions} / {total_valid_predictions}")
    print(f"ğŸ“Š Accuracy (on valid responses): {accuracy * 100:.2f}%")
    print("="*50 + "\nâœ… Evaluation Complete.\n" + "="*50)


# --- Main Execution Logic ---
def main():
    """Main function to run the prediction and evaluation process."""
    try:
        df = pd.read_csv(INPUT_CSV, encoding='utf-8')
    except FileNotFoundError:
        print(f"Error: The file '{INPUT_CSV}' was not found. Please check the path.")
        return
    except Exception as e:
        print(f"An error occurred while reading the CSV: {e}")
        return

    df.dropna(subset=[QUESTION_COLUMN, ANSWER_COLUMN], inplace=True)
    df.reset_index(drop=True, inplace=True)

    # Initialize the OpenAI client to connect to the local vLLM server.
    # The client is initialized once and reused for all API calls.
    try:
        client = OpenAI(base_url=VLLM_BASE_URL, api_key=API_KEY)
        print("âœ… OpenAI client initialized. Ready to connect to vLLM server.")
    except Exception as e:
        print(f"âŒ Failed to initialize OpenAI client. Error: {e}")
        print("Please ensure the vLLM server is running and accessible.")
        return


    if os.path.exists(OUTPUT_CSV):
        print(f"âœ… Found existing prediction file: '{OUTPUT_CSV}'.")
        predictions_df = pd.read_csv(OUTPUT_CSV, header=None, encoding='utf-8', na_filter=False)
        predictions = predictions_df[0].astype(str).tolist()

        error_codes_to_rerun = ["INFERENCE_ERROR", ""]
        failed_indices = [i for i, p in enumerate(predictions) if p in error_codes_to_rerun]

        if not failed_indices:
            print("âœ… No failed questions found to rerun. Proceeding directly to evaluation.")
        else:
            print(f"âš ï¸ Found {len(failed_indices)} failed questions. Starting rerun process...")
            for index in failed_indices:
                question = df.loc[index, QUESTION_COLUMN]
                print(f"Rerunning question {index + 1}/{len(df)}...")
                new_answer = generate_answer(question, client)
                predictions[index] = new_answer

                ground_truth_letter = str(df.loc[index, ANSWER_COLUMN]).strip()[0]
                print(f"  -> Ground Truth: {ground_truth_letter} | New Predicted Letter: {new_answer}")

            print("\nâœ… Rerun complete. Saving updated results...")
            updated_predictions_df = pd.DataFrame(predictions)
            updated_predictions_df.to_csv(OUTPUT_CSV, header=False, index=False, encoding='utf-8')
            print(f"Successfully saved updated predictions to '{OUTPUT_CSV}'.")

    else:
        # This is the logic for a full run from scratch.
        print(f"'{OUTPUT_CSV}' not found. Starting a full prediction run...")
        predictions = []
        total_questions = len(df)
        start_time = time.time()

        for index, row in df.iterrows():
            question = row[QUESTION_COLUMN]
            print(f"Processing question {index + 1}/{total_questions}...")
            answer_letter = generate_answer(question, client)
            predictions.append(answer_letter)

            ground_truth_letter = str(row[ANSWER_COLUMN]).strip()[0] if str(row[ANSWER_COLUMN]).strip() else "N/A"
            print(f"  -> Ground Truth: {ground_truth_letter} | Model's Predicted Letter: {answer_letter}")

        end_time = time.time()
        total_duration = end_time - start_time
        minutes = int(total_duration // 60)
        seconds = int(total_duration % 60)
        print("\n" + "="*50)
        print(f"âœ… Prediction generation complete.")
        print(f"â±ï¸  Total time taken: {minutes} minutes and {seconds} seconds.")
        print("="*50)

        predictions_df = pd.DataFrame(predictions)
        predictions_df.to_csv(OUTPUT_CSV, header=False, index=False, encoding='utf-8')
        print(f"\nSuccessfully saved predictions to '{OUTPUT_CSV}'.")

    # --- Final Evaluation ---
    ground_truths = [str(ans).strip()[0] if str(ans).strip() else "INVALID_TRUTH" for ans in df[ANSWER_COLUMN].tolist()]
    evaluate_mcq_accuracy(predictions, ground_truths)


if __name__ == "__main__":
    # Clean up memory before starting, although less critical without local model loading.
    gc.collect()
    main()
