import os
import re
import pandas as pd
import time
from getpass import getpass
from sklearn.metrics import accuracy_score
import gc

# --- Import necessary libraries for local inference ---
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    import torch
except ImportError:
    print("Please install necessary libraries: pip install transformers torch accelerate bitsandbytes")
    exit()


# --- File paths and column names ---
INPUT_CSV = '/content/drive/MyDrive/AraHealthQA/t2t1/data/fill-in-the-blank-choices.csv'
# Changed output file name for the new model
OUTPUT_CSV = '/content/drive/MyDrive/AraHealthQA/t2t1/final_result/predictions_fitb_choices_BioMistral.csv' 

# --- Column names ---
QUESTION_COLUMN = 'Question - Arabic'
ANSWER_COLUMN = 'Answer - Arabic'


# --- Chain of Thought & Few-Shot Prompting Configuration ---
# Reverting to a general-purpose medical prompt for the new model
SYSTEM_PROMPT = """You are an expert medical professional and a meticulous exam assistant. Your task is to solve a multiple-choice question in Arabic.
First, you will engage in a step-by-step thinking process in a <thinking> block. Analyze the medical question, evaluate each option (ุฃ, ุจ, ุฌ, ุฏ, ู), and explain your reasoning for choosing the correct answer.
Second, after your reasoning, you MUST provide the final answer on a new line in the format:
Final Answer: [The single Arabic letter of the correct option]

This two-step process is mandatory. Your entire response must be in Arabic.
"""

FEW_SHOT_EXAMPLES = [
    {
        "role": "user",
        "content": """ุงููุฃ ุงููุฑุงุบุงุช ูู ุงูุฌููุฉ ุงูุชุงููุฉ:
ูู ุญุงูุฉ ุงูุงูุตุจุงุจ ุงูุฌูุจูุ ูุดูุฑ ุงูุฎูุงุถ ุฃู ุบูุงุจ ุงูุฑุฌูุงู ุงูููุณู ุนูู ุงูุฌุงูุจ ุงููุตุงุจ ุฅูู ____ุ ูุงูุฐู ููุชุฌ ุนู ____.
ุฃ. ุชุฑุงูู ุงูุณูุงุฆูุ ุนุฒู ุงูุตูุช
ุจ. ุงุญุชุจุงุณ ุงูููุงุกุ ุงูููุงุฑ ุงูุญููุตูุงุช ุงูููุงุฆูุฉ
ุฌ. ุงูุชูููุ ุงูุฎูุงุถ ูุฑููุฉ ุงูุฑุฆุฉ
ุฏ. ููู ุงููุฑูุ ุงูุณุฏุงุฏ ุงูุดุนุจ ุงูููุงุฆูุฉ"""
    },
    {
        "role": "assistant",
        "content": """<thinking>
        1.  **ุชุญููู ุงูุณุคุงู**: ูุณุฃู ุงูุณุคุงู ุนู ุฏูุงูุฉ ุงูุฎูุงุถ ุฃู ุบูุงุจ "ุงูุฑุฌูุงู ุงูููุณู" (tactile fremitus) ูู ุญุงูุฉ "ุงูุงูุตุจุงุจ ุงูุฌูุจู" (pleural effusion) ูุนู ุงูุขููุฉ ุงููุณุจุจุฉ ูุฐูู. ุงูุฑุฌูุงู ุงูููุณู ูู ุงูุงูุชุฒุงุฒ ุงูุฐู ูููู ุงูุดุนูุฑ ุจู ุนูู ุฌุฏุงุฑ ุงูุตุฏุฑ ุฃุซูุงุก ุงูููุงู.
        2.  **ุชูููู ุงูุฎูุงุฑุงุช**:
            * **ุฃ. ุชุฑุงูู ุงูุณูุงุฆูุ ุนุฒู ุงูุตูุช**: ุงูุงูุตุจุงุจ ุงูุฌูุจู ูู ุจุงููุนู ุชุฑุงูู ููุณูุงุฆู ูู ุงูุบุดุงุก ุงูุฌูุจู. ูุฐุง ุงูุณุงุฆู ูุนูู ูุนุงุฒูุ ููุง ูููุน ุงูุชูุงู ุงูุชุฒุงุฒุงุช ุงูุตูุช ูู ุงูุฑุฆุฉ ุฅูู ุฌุฏุงุฑ ุงูุตุฏุฑ. ูุฐุง ูุชุทุงุจู ุชูุงููุง ูุน ะฝะฐัะพะดุฉ ุงูุฎูุงุถ ุงูุฑุฌูุงู ุงูููุณู.
            * **ุจ. ุงุญุชุจุงุณ ุงูููุงุกุ ุงูููุงุฑ ุงูุญููุตูุงุช ุงูููุงุฆูุฉ**: ูุฐุง ูุตู ุญุงูุฉ ุงุณุชุฑูุงุญ ุงูุตุฏุฑ (pneumothorax) ุฃู ุงูุฎูุงุต ุงูุฑุฆุฉ (atelectasis)ุ ูุงูุชู ููุง ููุฌูุฏุงุช ููุฒูุงุฆูุฉ ูุฎุชููุฉ.
            * **ุฌ. ุงูุชูููุ ุงูุฎูุงุถ ูุฑููุฉ ุงูุฑุฆุฉ**: ุงูุชููู ุงูุฑุฆูู (Pulmonary fibrosis) ูุฒูุฏ ูู ูุซุงูุฉ ุฃูุณุฌุฉ ุงูุฑุฆุฉุ ููุง ูุฏ ูุคุฏู ุฅูู ุฒูุงุฏุฉ ุงูุฑุฌูุงู ุงูููุณูุ ูููุณ ุงูุฎูุงุถู.
            * **ุฏ. ููู ุงููุฑูุ ุงูุณุฏุงุฏ ุงูุดุนุจ ุงูููุงุฆูุฉ**: ูุฏ ูุณุจุจ ุงููุฑู ุงูุตุจุงุจูุง ุฌูุจููุงุ ููู ุงูุณุจุจ ุงููุจุงุดุฑ ูุงูุฎูุงุถ ุงูุฑุฌูุงู ูู ูุฐู ุงูุญุงูุฉ ูู ุงูุณุงุฆู ููุณู ุงูุฐู ูุนุฒู ุงูุตูุช. ุงูุฎูุงุฑ "ุฃ" ูุตู ุงูุขููุฉ ุงูููุฒูุงุฆูุฉ ุงููุจุงุดุฑุฉ ุจุดูู ุฃูุถู.
        3.  **ุงูุงุณุชูุชุงุฌ**: ุงูุฎูุงุฑ ุงูุฃูุซุฑ ุฏูุฉ ูู ุฃู ุชุฑุงูู ุงูุณูุงุฆู ูู ูุง ูุณุจุจ ุนุฒู ุงูุตูุชุ ููุง ูุคุฏู ุฅูู ุงูุฎูุงุถ ุงูุฑุฌูุงู ุงูููุณู.
        </thinking>
        Final Answer: ุฃ"""
    }
]


# --- Function to Generate Answers (MODIFIED for Manual Inference) ---
def generate_answer(question, model, tokenizer):
    """
    Generates an answer using a manually handled model and tokenizer.
    """
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT}
    ]
    messages.extend(FEW_SHOT_EXAMPLES)
    messages.append({"role": "user", "content": question})

    try:
        # Manually apply the chat template and tokenize the input
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(model.device)
        
        # Generate output token IDs
        outputs = model.generate(
            **inputs,
            max_new_tokens=1024,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id # Set pad token for open-ended generation
        )
        
        # Decode only the newly generated tokens
        input_length = inputs.input_ids.shape[1]
        new_tokens = outputs[0, input_length:]
        response_text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
        
        # --- Intelligent Parsing Logic ---
        # Method 1: Check for the standard 'Final Answer:' format.
        match = re.search(r"Final Answer:\s*([\u0621-\u064A])", response_text, re.IGNORECASE)
        if match:
            return match.group(1)

        # Method 2: If standard format fails, parse the reasoning text.
        print(f"  -> 'Final Answer' format not found. Attempting to parse reasoning...")
        
        conclusive_phrases = [
            r"ุงูุฎูุงุฑ ุงูุตุญูุญ ูู\s*([\u0621-\u064A])", r"ุงูุฅุฌุงุจุฉ ุงูุตุญูุญุฉ ูู\s*([\u0621-\u064A])",
            r"ุงูุงุณุชูุชุงุฌ ูู ุฃู ุงูุฎูุงุฑ\s*([\u0621-\u064A])", r"ุงูุฎูุงุฑ\s*([\u0621-\u064A])\s*ูู ุงูุตุญูุญ",
        ]
        for phrase in conclusive_phrases:
            match = re.search(phrase, response_text)
            if match:
                print(f"  -> Found answer using conclusive phrase heuristic.")
                return match.group(1)

        option_mentions = re.findall(r"ุงูุฎูุงุฑ\s*([\u0621-\u064A])", response_text)
        if option_mentions:
            last_option = option_mentions[-1]
            print(f"  -> Found answer using last-mentioned option heuristic: '{last_option}'")
            return last_option
        
        print(f"  -> Warning: Could not deduce answer from response: '{response_text}'. Recording as empty.")
        return ""
            
    except Exception as e:
        print(f"  -> An error occurred during model inference: {e}")
        return "INFERENCE_ERROR"


# --- Function to Evaluate MCQ Accuracy (with Normalization) ---
def evaluate_mcq_accuracy(predictions, ground_truths):
    """
    Calculates and prints the accuracy, normalizing different forms of Alif.
    """
    print("\n" + "="*50)
    print("๐ Starting Evaluation...")
    print("="*50)
    
    error_codes = ["INFERENCE_ERROR", ""]
    valid_indices = [i for i, p in enumerate(predictions) if p not in error_codes]
    valid_predictions = [predictions[i] for i in valid_indices]
    valid_ground_truths = [ground_truths[i] for i in valid_indices]

    if not valid_predictions:
        print("No valid predictions to evaluate.")
        return

    def normalize_alif(letter):
        return letter.replace('ุฃ', 'ุง').replace('ุฅ', 'ุง').replace('ุข', 'ุง')

    normalized_predictions = [normalize_alif(p) for p in valid_predictions]
    normalized_ground_truths = [normalize_alif(g) for g in valid_ground_truths]

    accuracy = accuracy_score(normalized_ground_truths, normalized_predictions)
    correct_predictions = sum(p == g for p, g in zip(normalized_ground_truths, normalized_predictions))
    
    total_valid_predictions = len(valid_predictions)
    total_questions = len(ground_truths)
    failed_or_empty = total_questions - total_valid_predictions

    print(f"Total Questions Attempted: {total_questions}")
    print(f"Final Unanswered / Error Count: {failed_or_empty}")
    print(f"Valid Predictions to Evaluate: {total_valid_predictions}")
    print("-" * 20)
    print(f"Correct Predictions: {correct_predictions} / {total_valid_predictions}")
    print(f"๐ Accuracy (on valid responses): {accuracy * 100:.2f}%")
    print("="*50 + "\nโ Evaluation Complete.\n" + "="*50)


# --- Main Execution ---
def main():
    try:
        df = pd.read_csv(INPUT_CSV, encoding='utf-8')
    except FileNotFoundError:
        print(f"Error: The file '{INPUT_CSV}' was not found. Please check the path.")
        return
    except Exception as e:
        print(f"An error occurred while reading the CSV: {e}")
        return

    df.dropna(subset=[QUESTION_COLUMN, ANSWER_COLUMN], inplace=True)
    df.reset_index(drop=True, inplace=True)
    
    if os.path.exists(OUTPUT_CSV):
        print(f"Output file '{OUTPUT_CSV}' already exists. Please remove or rename it to run a new generation.")
        return
        
    # --- Model Loading ---
    print("="*50)
    print("๐ Initializing local model: BioMistral/BioMistral-7B")
    print("This may take a few minutes...")
    print("="*50)
    
    if not torch.cuda.is_available():
        print("โ CUDA is not available. A GPU is required for this model.")
        return
    
    try:
        print("Clearing GPU cache before model loading...")
        gc.collect()
        torch.cuda.empty_cache()
        
        model_id = "BioMistral/BioMistral-7B"
        
        # Use 4-bit quantization for memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        # Load tokenizer and model separately
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            quantization_config=quantization_config,
        )

    except Exception as e:
        print(f"โ Failed to load the model. Error: {e}")
        return

    print("โ Model loaded successfully.")
    
    # --- Prediction Generation ---
    predictions = []
    total_questions = len(df)
    start_time = time.time()

    for index, row in df.iterrows():
        question = row[QUESTION_COLUMN]
        print(f"Processing question {index + 1}/{total_questions}...")
        # Pass both model and tokenizer to the generation function
        answer_letter = generate_answer(question, model, tokenizer)
        predictions.append(answer_letter)
        
        ground_truth_letter = str(row[ANSWER_COLUMN]).strip()[0] if str(row[ANSWER_COLUMN]).strip() else "N/A"
        print(f"  -> Ground Truth: {ground_truth_letter} | Model's Predicted Letter: {answer_letter}")

    end_time = time.time()
    total_duration = end_time - start_time
    minutes = int(total_duration // 60)
    seconds = int(total_duration % 60)
    print("\n" + "="*50)
    print(f"โ Prediction generation complete.")
    print(f"โฑ๏ธ  Total time taken: {minutes} minutes and {seconds} seconds.")
    print("="*50)

    predictions_df = pd.DataFrame(predictions)
    predictions_df.to_csv(OUTPUT_CSV, header=False, index=False, encoding='utf-8')
    print(f"\nSuccessfully saved predictions to '{OUTPUT_CSV}'.")

    # --- Final Evaluation ---
    ground_truths = [str(ans).strip()[0] if str(ans).strip() else "INVALID_TRUTH" for ans in df[ANSWER_COLUMN].tolist()]
    evaluate_mcq_accuracy(predictions, ground_truths)


if __name__ == "__main__":
    main()
